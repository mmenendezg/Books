{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import (\n",
    "    datasets,\n",
    "    layers,\n",
    "    Sequential,\n",
    "    callbacks,\n",
    "    losses, \n",
    "    optimizers,\n",
    "    metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "def create_model():\n",
    "    return Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TensorBoard with Keras Model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-11 16:32:50.003399: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-03-11 16:32:50.003519: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-03-11 16:32:50.155278: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  10/1875 [..............................] - ETA: 10s - loss: 1.8370 - accuracy: 0.4281 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-11 16:32:50.314582: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.2184 - accuracy: 0.9345"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-11 16:32:59.875619: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2181 - accuracy: 0.9346 - val_loss: 0.1095 - val_accuracy: 0.9656\n",
      "Epoch 2/20\n",
      "1875/1875 [==============================] - 10s 6ms/step - loss: 0.0966 - accuracy: 0.9704 - val_loss: 0.0762 - val_accuracy: 0.9760\n",
      "Epoch 3/20\n",
      "1875/1875 [==============================] - 10s 6ms/step - loss: 0.0670 - accuracy: 0.9787 - val_loss: 0.0789 - val_accuracy: 0.9753\n",
      "Epoch 4/20\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0517 - accuracy: 0.9836 - val_loss: 0.0680 - val_accuracy: 0.9799\n",
      "Epoch 5/20\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0411 - accuracy: 0.9868 - val_loss: 0.0621 - val_accuracy: 0.9811\n",
      "Epoch 6/20\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0322 - accuracy: 0.9895 - val_loss: 0.0630 - val_accuracy: 0.9808\n",
      "Epoch 7/20\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0285 - accuracy: 0.9904 - val_loss: 0.0649 - val_accuracy: 0.9818\n",
      "Epoch 8/20\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0245 - accuracy: 0.9916 - val_loss: 0.0693 - val_accuracy: 0.9797\n",
      "Epoch 9/20\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0205 - accuracy: 0.9931 - val_loss: 0.0818 - val_accuracy: 0.9798\n",
      "Epoch 10/20\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0196 - accuracy: 0.9934 - val_loss: 0.0821 - val_accuracy: 0.9779\n",
      "Epoch 11/20\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0159 - accuracy: 0.9944 - val_loss: 0.0739 - val_accuracy: 0.9823\n",
      "Epoch 12/20\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0158 - accuracy: 0.9944 - val_loss: 0.1000 - val_accuracy: 0.9765\n",
      "Epoch 13/20\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0151 - accuracy: 0.9948 - val_loss: 0.0896 - val_accuracy: 0.9799\n",
      "Epoch 14/20\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0157 - accuracy: 0.9947 - val_loss: 0.0830 - val_accuracy: 0.9802\n",
      "Epoch 15/20\n",
      "1875/1875 [==============================] - 10s 6ms/step - loss: 0.0116 - accuracy: 0.9962 - val_loss: 0.0892 - val_accuracy: 0.9815\n",
      "Epoch 16/20\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0143 - accuracy: 0.9955 - val_loss: 0.0950 - val_accuracy: 0.9806\n",
      "Epoch 17/20\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0128 - accuracy: 0.9958 - val_loss: 0.0852 - val_accuracy: 0.9831\n",
      "Epoch 18/20\n",
      "1875/1875 [==============================] - 10s 6ms/step - loss: 0.0123 - accuracy: 0.9958 - val_loss: 0.0776 - val_accuracy: 0.9841\n",
      "Epoch 19/20\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0100 - accuracy: 0.9966 - val_loss: 0.0851 - val_accuracy: 0.9826\n",
      "Epoch 20/20\n",
      "1875/1875 [==============================] - 10s 6ms/step - loss: 0.0091 - accuracy: 0.9968 - val_loss: 0.0789 - val_accuracy: 0.9840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16a5db2e0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "log_dir = 'logs/fit/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "tensorboard_cb = callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit(x = x_train,\n",
    "          y = y_train,\n",
    "          validation_data = [x_test, y_test],\n",
    "          epochs=20,\n",
    "          callbacks = [tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Tensorboard with other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(60000).batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the objects to compile the model\n",
    "loss_function = losses.SparseCategoricalCrossentropy()\n",
    "optimizer = optimizers.Adam()\n",
    "\n",
    "train_loss = metrics.Mean('train_loss', dtype=tf.float32)\n",
    "train_accuracy = metrics.SparseCategoricalAccuracy('train_accuracy')\n",
    "test_loss = metrics.Mean('test_loss', dtype=tf.float32)\n",
    "test_accuracy = metrics.SparseCategoricalAccuracy('test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training and test functions\n",
    "\n",
    "def train_step(model, optimizer, x_train, y_train):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(x_train, training=True)\n",
    "        loss = loss_function(y_train, predictions)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(y_train, predictions)\n",
    "\n",
    "def test_step(model, x_test, y_test):\n",
    "    predictions = model(x_test)\n",
    "    loss = loss_function(y_test, predictions)\n",
    "    \n",
    "    test_loss(loss)\n",
    "    test_accuracy(y_test, predictions\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1786699742078781, Accuracy: 94.57725524902344, Test Loss: 0.09580734372138977, Test Accuracy: 96.99000549316406\n",
      "Epoch 2, Loss: 0.08433691412210464, Accuracy: 97.45833587646484, Test Loss: 0.08396423608064651, Test Accuracy: 97.36000061035156\n",
      "Epoch 3, Loss: 0.05963600426912308, Accuracy: 98.18500518798828, Test Loss: 0.06449270993471146, Test Accuracy: 98.01000213623047\n",
      "Epoch 4, Loss: 0.04526938498020172, Accuracy: 98.58333587646484, Test Loss: 0.060837168246507645, Test Accuracy: 98.20000457763672\n",
      "Epoch 5, Loss: 0.03566374257206917, Accuracy: 98.89500427246094, Test Loss: 0.055467940866947174, Test Accuracy: 98.22000122070312\n",
      "Epoch 6, Loss: 0.02797708474099636, Accuracy: 99.11333465576172, Test Loss: 0.06324158608913422, Test Accuracy: 98.01000213623047\n",
      "Epoch 7, Loss: 0.02371121384203434, Accuracy: 99.22333526611328, Test Loss: 0.060003191232681274, Test Accuracy: 98.22000122070312\n",
      "Epoch 8, Loss: 0.019709572196006775, Accuracy: 99.33333587646484, Test Loss: 0.05620067939162254, Test Accuracy: 98.39000701904297\n",
      "Epoch 9, Loss: 0.0184515081346035, Accuracy: 99.40833282470703, Test Loss: 0.06193049997091293, Test Accuracy: 98.20000457763672\n",
      "Epoch 10, Loss: 0.01468654628843069, Accuracy: 99.55833435058594, Test Loss: 0.060497984290122986, Test Accuracy: 98.30000305175781\n",
      "Epoch 11, Loss: 0.012960623949766159, Accuracy: 99.59166717529297, Test Loss: 0.06364715844392776, Test Accuracy: 98.31000518798828\n",
      "Epoch 12, Loss: 0.012210054323077202, Accuracy: 99.6050033569336, Test Loss: 0.0696183368563652, Test Accuracy: 98.19000244140625\n",
      "Epoch 13, Loss: 0.01097039319574833, Accuracy: 99.62666320800781, Test Loss: 0.06993446499109268, Test Accuracy: 98.22000122070312\n",
      "Epoch 14, Loss: 0.011474860832095146, Accuracy: 99.58499908447266, Test Loss: 0.0640064924955368, Test Accuracy: 98.39000701904297\n",
      "Epoch 15, Loss: 0.008912459015846252, Accuracy: 99.70000457763672, Test Loss: 0.070890411734581, Test Accuracy: 98.29000091552734\n",
      "Epoch 16, Loss: 0.009220343083143234, Accuracy: 99.6866683959961, Test Loss: 0.06349322199821472, Test Accuracy: 98.45000457763672\n",
      "Epoch 17, Loss: 0.01017169002443552, Accuracy: 99.66333770751953, Test Loss: 0.0657549500465393, Test Accuracy: 98.44000244140625\n",
      "Epoch 18, Loss: 0.007775360252708197, Accuracy: 99.74333953857422, Test Loss: 0.06920286267995834, Test Accuracy: 98.44000244140625\n",
      "Epoch 19, Loss: 0.00621818657964468, Accuracy: 99.80500030517578, Test Loss: 0.0729314461350441, Test Accuracy: 98.46000671386719\n",
      "Epoch 20, Loss: 0.009711138904094696, Accuracy: 99.66500091552734, Test Loss: 0.0790899470448494, Test Accuracy: 98.2800064086914\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for (x_train, y_train) in train_dataset:\n",
    "        train_step(model, optimizer, x_train, y_train)\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', train_loss.result(), step=epoch)\n",
    "    \n",
    "    for (x_test, y_test) in test_dataset:\n",
    "        test_step(model, x_test, y_test)\n",
    "    with test_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', test_loss.result(), step=epoch)\n",
    "    \n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "    print(template.format(epoch+1,\n",
    "                          train_loss.result(),\n",
    "                          train_accuracy.result() * 100,\n",
    "                          test_loss.result(),\n",
    "                          test_accuracy.result() * 100))\n",
    "    \n",
    "    # Reset metrics every epoch\n",
    "    for metric in [train_loss, train_accuracy, test_loss, test_accuracy]:\n",
    "        metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1b2ab899e8a04099\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1b2ab899e8a04099\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/gradient_tape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6ba1b7cf6e8418139948d4f5b35e26443252c5696a8a4a7bc6c27d254786872"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('books-zhEizXov-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
