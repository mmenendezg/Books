{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: Introduction to Artificial Neural Networks with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Why is it generally preferable to use a Logistic Regression classifier rather than a classical Perceptron (i.e., a single layer of threshold logic units trained using the Perceptron training algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The perceptron will only be able to work properly if the data is linearly separable, in contrast, the logistic regression classifier will be able to perform better when the data is not linearly separable. A good way to tweak the perceptron to be equivalent to the logistic regression classifier is to use the sigmoid function for binary classification, or softmax for multiclass problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Why was the logistic activation function a key ingredient in training the first MLPs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The perceptron uses the heaviside activation function that is flat, and therefore does not have derivative for the optimizer to work with. On the other side, the logistic activation function has derivative in all its domain, and it is possible to use an optimizer to tweak the weights of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Name three popular activation functions. Can you draw them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Some of the most popular activation functions are the following:\n",
    "\n",
    "> - Rectified Linear Unit (ReLU) $ReLU(z) = max(0, z)$\n",
    "> - Sigmoid function $\\sigma(z) = \\frac{1}{1 + e^{-z} }$\n",
    "> - Hyperbolic tangent $tanh(z) = \\frac{e^{ z} - e^{-z} }{ e^{z} + e^{-z} }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function.\n",
    "\n",
    "#### - What is the shape of the input matrix $X$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The shape of the input matrix $X$ is $m\\times10$, where $m$ is the size of the batch of records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - What are the shapes of the hidden layer’s weight vector $W_{h}$ and its bias vector $b_h$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The shape of the vector $W_h$ is $10\\times50$, and the shape of the vector $b_h$ is $50\\times1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - What are the shapes of the output layer’s weight vector and its bias vector $b_o$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The shape of the vector $W_o$ is $50\\times3$. On the other hand, the output layer do not have bias vector associated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - What is the shape of the network’s output matrix $Y$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The shape of the output matrix $Y$ is $m\\times3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Write the equation that computes the network’s output matrix $Y$ as a function of $X$, $W_h$, $b_h$, $W_o$, and $b_o$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$Y = ReLU(ReLU(W_hX + b_h)W_o + b_o)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the output layer, and which activation function should you use? What about for getting your network to predict housing prices, as in Chapter 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If we want to classify the email into spam or ham we need one neuron in the output layer, and the activation function would be the logistic (sigmoid) function. For the MNIST scenario, we would need 10 neurons in the output layer, and the activation function would be softmax. Finally for the housing prices problem, it would be necessary only one output neuron and the activation function may be ReLU to make sure only positive values will be predicted, since a house cannot have negative price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. What is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Backpropagation is the technique used to update the weights of a neural network while training. This works by using the chain rule: it determines the ouput error, then calculates the gradient of the cost function and then applies gradient descent to all the weights. Reverse-mode autodiff is an efficient way to calculate the gradients, and it is part of the backpropagation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Can you list all the hyperparameters you can tweak in a basic MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Some of the most important hyperparameters are the activation function, the number of neurons, the number of hidden layers. In the case of overfitting, we can decrease the number of hidden layers or the number of neurons per layer to avoid it. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d7b166ebd68e38de58fc1f6af134f0f2ff66e0d51839b30fdba22526be05163"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
