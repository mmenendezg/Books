{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: Introduction to Artificial Neural Networks with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Train a deep MLP on the MNIST dataset (you can load it using keras.datasets.mnist.load_data(). See if you can get over 98% precision. Try searching for the optimal learning rate by using the approach presented in this chapter (i.e., by growing the learning rate exponentially, plotting the error, and finding the point where the error shoots up). Try adding all the bells and whistlesâ€”save checkpoints, use early stopping, and plot learning curves using TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import (\n",
    "    Sequential,\n",
    "    layers,\n",
    "    callbacks,\n",
    "    datasets,\n",
    "    activations,\n",
    "    optimizers,\n",
    "    losses,\n",
    "    metrics,\n",
    ")\n",
    "import keras_tuner as kt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first download the dataset and split it into training, validation and test sets. The train set returned by `tf.keras.datasets.mnist.load_data()` has 60k images. We will use 50k for training and 10k for validation. At the same time we will normalize the values of the pixels to make sure all the possible values lay between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(dataset, percentage=0.7, target=False):\n",
    "    \"\"\"This function normalizes the dataset and divides the dataset \n",
    "    into training and validation datasets\n",
    "\n",
    "    Args:\n",
    "        dataset (array): Dataset of images to train the model   \n",
    "        percentage (float, optional): The percentage of the dataset that will be used to train. Defaults to 0.7.\n",
    "        target (boolean): True if the dataset contains the targets of the model. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        train_dataset (array): dataset that will be used to train the model\n",
    "        valid_dataset (array): dataset that will be used to validate the results of the model\n",
    "    \"\"\"\n",
    "    \n",
    "    max_value = np.max(dataset)\n",
    "    threshold = round(len(dataset) * percentage)\n",
    "    train_dataset = dataset[:threshold]/max_value if not target else dataset[:threshold]\n",
    "    valid_dataset = dataset[threshold:]/max_value if not target else dataset[threshold:]\n",
    "    return train_dataset, valid_dataset\n",
    "\n",
    "\n",
    "def get_mnist():\n",
    "    \"\"\"This function downloads the mnist dataset from Keras,\n",
    "    and splits the data into training, validation and testing sets.\n",
    "\n",
    "    Returns:\n",
    "        train_set : array containing the input and target values for the training\n",
    "        valid_set : array containing the input and target values for the validation in training\n",
    "        test_set : array containing the input and target values for the testing of the model\n",
    "    \"\"\"\n",
    "    (x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
    "\n",
    "    x_train, x_valid = preprocessing(x_train)\n",
    "    y_train, y_valid = preprocessing(y_train, target=True)\n",
    "\n",
    "    train_set = [x_train, y_train]\n",
    "    valid_set = [x_valid, y_valid]\n",
    "    test_set = [x_test, y_test]\n",
    "    \n",
    "    return train_set, valid_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_set, valid_set, test_set, neurons=100, epochs=25, lrate=1e-4):\n",
    "    x_train, y_train = train_set\n",
    "    x_valid, y_valid = valid_set\n",
    "    x_test, y_test = test_set\n",
    "    \n",
    "\n",
    "    model = Sequential(\n",
    "        [\n",
    "            layers.Flatten(input_shape=[28, 28]),\n",
    "            layers.Dense(neurons, activation=activations.relu),\n",
    "            layers.Dense(neurons, activation=activations.relu),\n",
    "            layers.Dense(neurons, activation=activations.relu),\n",
    "            layers.Dense(10, activation=activations.softmax),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Compilation\n",
    "    optimizer = optimizers.SGD(learning_rate=lrate)\n",
    "    model.compile(\n",
    "        loss=[losses.sparse_categorical_crossentropy],\n",
    "        metrics=[\"accuracy\"],\n",
    "        optimizer=optimizer,\n",
    "    )\n",
    "\n",
    "    # Define Callbacks\n",
    "    logdir = os.path.join(\"..\", \"logs\", \"chapter_10\")\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "    tensorboard_cb = callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "    early_stop_cb = callbacks.EarlyStopping(patience=5)\n",
    "\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_data=[x_valid, y_valid],\n",
    "        callbacks=[tensorboard_cb, early_stop_cb],\n",
    "        batch_size=32,\n",
    "        epochs=epochs,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    evaluation = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f'The accuracy of the model is {evaluation[1]:.4f}')\n",
    "    print(f'Learning rate: {lrate}')\n",
    "    print(f'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    \n",
    "    hp_units = hp.Int('units', min_value=50, max_value=300, step=50)\n",
    "    \n",
    "    model = Sequential(\n",
    "        [\n",
    "            layers.Flatten(input_shape=[28, 28]),\n",
    "            layers.Dense(hp_units, activation=activations.relu),\n",
    "            layers.Dense(hp_units, activation=activations.relu),\n",
    "            layers.Dense(hp_units, activation=activations.relu),\n",
    "            layers.Dense(10, activation=activations.softmax),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[8e-3, 9e-3, 1e-2, 2e-2, 3e-2, 4e-2])\n",
    "    \n",
    "    # Compilation\n",
    "    optimizer = optimizers.SGD(learning_rate=hp_learning_rate)\n",
    "    model.compile(\n",
    "        loss=[losses.sparse_categorical_crossentropy],\n",
    "        metrics=[\"accuracy\"],\n",
    "        optimizer=optimizer,\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set, test_set = get_mnist()\n",
    "x_train, y_train = train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project tuner_kt/homl_chapter_10/oracle.json\n",
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n",
      "INFO:tensorflow:Reloading Tuner from tuner_kt/homl_chapter_10/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    model_builder,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=10,\n",
    "    factor=3,\n",
    "    directory='tuner_kt',\n",
    "    project_name='homl_chapter_10'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 23 Complete [00h 00m 25s]\n",
      "val_accuracy: 0.9258333444595337\n",
      "\n",
      "Best val_accuracy So Far: 0.9688094854354858\n",
      "Total elapsed time: 00h 03m 46s\n",
      "\n",
      "Search: Running Trial #24\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "50                |300               |units\n",
      "0.008             |0.04              |learning_rate\n",
      "4                 |10                |tuner/epochs\n",
      "0                 |4                 |tuner/initial_epoch\n",
      "1                 |2                 |tuner/bracket\n",
      "0                 |2                 |tuner/round\n",
      "\n",
      "Epoch 1/4\n",
      "1050/1050 [==============================] - 7s 6ms/step - loss: 1.1430 - accuracy: 0.6723 - val_loss: 0.5150 - val_accuracy: 0.8513\n",
      "Epoch 2/4\n",
      "1050/1050 [==============================] - 6s 6ms/step - loss: 0.4250 - accuracy: 0.8782 - val_loss: 0.3609 - val_accuracy: 0.8946\n",
      "Epoch 3/4\n",
      "1040/1050 [============================>.] - ETA: 0s - loss: 0.3296 - accuracy: 0.9064"
     ]
    }
   ],
   "source": [
    "early_stop_cb = callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "tuner.search(x_train, y_train, epochs=50, validation_split=0.2, callbacks=[early_stop_cb])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Oct 13 2022, 09:48:40) [Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b20fb8559386147dc77cba9f7518a8c175fc982b8c10bebe842520e16b3ac5d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
