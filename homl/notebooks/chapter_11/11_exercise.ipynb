{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Training Deep Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import (\n",
    "    datasets,\n",
    "    callbacks,\n",
    "    optimizers,\n",
    "    regularizers,\n",
    "    layers,\n",
    "    Sequential,\n",
    "    activations,\n",
    "    metrics,\n",
    "    losses,\n",
    "    backend,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 5e-4\n",
    "\n",
    "LOGS_PATH = os.path.join(\"..\", \"logs\", \"chapter_11\")\n",
    "MODEL_PATH = os.path.join(\"models\", \"my_cifar_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Practice training a deep neural network on the CIFAR10 image dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend.clear_session()\n",
    "tf.random.set_seed(1992)\n",
    "np.random.seed(1992)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
    "\n",
    "model.add(layers.Dense(10, activation=\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "x_train, x_valid = x_train[:40000], x_train[40000:]\n",
    "y_train, y_valid = y_train[:40000], y_train[40000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Nadam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    metrics=[\"accuracy\"],\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stop_cb = callbacks.EarlyStopping(patience=5, monitor='val_loss')\n",
    "tensorboard_cb = callbacks.TensorBoard(log_dir=LOGS_PATH)\n",
    "model_checkpoint_cb = callbacks.ModelCheckpoint(MODEL_PATH, save_best_only=True)\n",
    "\n",
    "callbacks_ = [early_stop_cb, tensorboard_cb, model_checkpoint_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "79/79 [==============================] - 10s 94ms/step - loss: 12.5227 - accuracy: 0.1213 - val_loss: 2.8022 - val_accuracy: 0.1451\n",
      "Epoch 2/100\n",
      "79/79 [==============================] - 6s 79ms/step - loss: 2.4382 - accuracy: 0.1685 - val_loss: 2.9737 - val_accuracy: 0.1566\n",
      "Epoch 3/100\n",
      "79/79 [==============================] - 6s 79ms/step - loss: 2.2385 - accuracy: 0.2008 - val_loss: 2.3230 - val_accuracy: 0.1852\n",
      "Epoch 4/100\n",
      "79/79 [==============================] - 6s 78ms/step - loss: 2.1605 - accuracy: 0.2218 - val_loss: 2.3706 - val_accuracy: 0.1853\n",
      "Epoch 5/100\n",
      "79/79 [==============================] - 6s 78ms/step - loss: 2.0975 - accuracy: 0.2384 - val_loss: 2.0688 - val_accuracy: 0.2523\n",
      "Epoch 6/100\n",
      "79/79 [==============================] - 6s 77ms/step - loss: 2.0594 - accuracy: 0.2512 - val_loss: 2.1007 - val_accuracy: 0.2366\n",
      "Epoch 7/100\n",
      "79/79 [==============================] - 6s 79ms/step - loss: 2.0246 - accuracy: 0.2620 - val_loss: 2.1944 - val_accuracy: 0.2022\n",
      "Epoch 8/100\n",
      "79/79 [==============================] - 6s 79ms/step - loss: 2.0044 - accuracy: 0.2691 - val_loss: 2.0516 - val_accuracy: 0.2531\n",
      "Epoch 9/100\n",
      "79/79 [==============================] - 6s 77ms/step - loss: 1.9661 - accuracy: 0.2853 - val_loss: 2.0764 - val_accuracy: 0.2478\n",
      "Epoch 10/100\n",
      "79/79 [==============================] - 6s 77ms/step - loss: 1.9418 - accuracy: 0.2982 - val_loss: 2.1440 - val_accuracy: 0.2284\n",
      "Epoch 11/100\n",
      "79/79 [==============================] - 6s 77ms/step - loss: 1.9225 - accuracy: 0.2988 - val_loss: 2.2696 - val_accuracy: 0.2270\n",
      "Epoch 12/100\n",
      "79/79 [==============================] - 6s 79ms/step - loss: 1.9065 - accuracy: 0.3086 - val_loss: 2.1175 - val_accuracy: 0.2509\n",
      "Epoch 13/100\n",
      "79/79 [==============================] - 6s 81ms/step - loss: 1.8856 - accuracy: 0.3132 - val_loss: 1.9684 - val_accuracy: 0.2823\n",
      "Epoch 14/100\n",
      "79/79 [==============================] - 6s 80ms/step - loss: 1.8739 - accuracy: 0.3208 - val_loss: 2.2548 - val_accuracy: 0.2564\n",
      "Epoch 15/100\n",
      "79/79 [==============================] - 6s 80ms/step - loss: 1.8517 - accuracy: 0.3292 - val_loss: 1.9120 - val_accuracy: 0.3110\n",
      "Epoch 16/100\n",
      "79/79 [==============================] - 6s 80ms/step - loss: 1.8349 - accuracy: 0.3372 - val_loss: 2.3016 - val_accuracy: 0.2277\n",
      "Epoch 17/100\n",
      "79/79 [==============================] - 6s 80ms/step - loss: 1.8339 - accuracy: 0.3348 - val_loss: 2.1914 - val_accuracy: 0.2517\n",
      "Epoch 18/100\n",
      "79/79 [==============================] - 6s 81ms/step - loss: 1.8026 - accuracy: 0.3497 - val_loss: 1.9096 - val_accuracy: 0.3071\n",
      "Epoch 19/100\n",
      "79/79 [==============================] - 7s 83ms/step - loss: 1.8069 - accuracy: 0.3460 - val_loss: 1.8932 - val_accuracy: 0.3219\n",
      "Epoch 20/100\n",
      "79/79 [==============================] - 7s 83ms/step - loss: 1.7704 - accuracy: 0.3558 - val_loss: 1.9884 - val_accuracy: 0.2842\n",
      "Epoch 21/100\n",
      "79/79 [==============================] - 6s 79ms/step - loss: 1.7783 - accuracy: 0.3580 - val_loss: 1.8677 - val_accuracy: 0.3275\n",
      "Epoch 22/100\n",
      "79/79 [==============================] - 6s 77ms/step - loss: 1.7631 - accuracy: 0.3633 - val_loss: 1.9810 - val_accuracy: 0.2991\n",
      "Epoch 23/100\n",
      "79/79 [==============================] - 6s 78ms/step - loss: 1.7514 - accuracy: 0.3676 - val_loss: 1.9072 - val_accuracy: 0.3156\n",
      "Epoch 24/100\n",
      "79/79 [==============================] - 6s 80ms/step - loss: 1.7364 - accuracy: 0.3703 - val_loss: 2.1247 - val_accuracy: 0.2638\n",
      "Epoch 25/100\n",
      "79/79 [==============================] - 6s 81ms/step - loss: 1.7337 - accuracy: 0.3726 - val_loss: 1.7966 - val_accuracy: 0.3532\n",
      "Epoch 26/100\n",
      "79/79 [==============================] - 6s 81ms/step - loss: 1.7286 - accuracy: 0.3763 - val_loss: 2.0194 - val_accuracy: 0.2827\n",
      "Epoch 27/100\n",
      "79/79 [==============================] - 6s 81ms/step - loss: 1.7049 - accuracy: 0.3813 - val_loss: 1.7717 - val_accuracy: 0.3606\n",
      "Epoch 28/100\n",
      "79/79 [==============================] - 6s 78ms/step - loss: 1.7001 - accuracy: 0.3853 - val_loss: 1.8484 - val_accuracy: 0.3254\n",
      "Epoch 29/100\n",
      "79/79 [==============================] - 6s 76ms/step - loss: 1.6984 - accuracy: 0.3821 - val_loss: 1.8620 - val_accuracy: 0.3261\n",
      "Epoch 30/100\n",
      "79/79 [==============================] - 6s 77ms/step - loss: 1.6846 - accuracy: 0.3906 - val_loss: 1.9144 - val_accuracy: 0.3215\n",
      "Epoch 31/100\n",
      "79/79 [==============================] - 6s 78ms/step - loss: 1.6756 - accuracy: 0.3972 - val_loss: 1.8805 - val_accuracy: 0.3353\n",
      "Epoch 32/100\n",
      "79/79 [==============================] - 6s 77ms/step - loss: 1.6833 - accuracy: 0.3928 - val_loss: 1.7834 - val_accuracy: 0.3568\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 1.7520 - accuracy: 0.3704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7520089149475098, 0.37040001153945923]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(\n",
    "    x_train, \n",
    "    y_train,\n",
    "    validation_data=[x_valid, y_valid],\n",
    "    callbacks=callbacks_,\n",
    "    epochs=100,\n",
    "    batch_size=512,\n",
    ")\n",
    "\n",
    "model = keras.models.load_model(MODEL_PATH)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend.clear_session()\n",
    "tf.random.set_seed(1992)\n",
    "np.random.seed(1992)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [layers.Flatten(input_shape=[32, 32, 3]), layers.BatchNormalization()]\n",
    ")\n",
    "\n",
    "for _ in range(20):\n",
    "    model.add(layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation(\"elu\"))\n",
    "\n",
    "model.add(layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Nadam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    metrics=[\"accuracy\"],\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stop_cb = callbacks.EarlyStopping(patience=5, monitor='val_loss')\n",
    "tensorboard_cb = callbacks.TensorBoard(log_dir=LOGS_PATH)\n",
    "model_checkpoint_cb = callbacks.ModelCheckpoint(MODEL_PATH, save_best_only=True)\n",
    "\n",
    "callbacks_ = [early_stop_cb, tensorboard_cb, model_checkpoint_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "79/79 [==============================] - 30s 261ms/step - loss: 1.8702 - accuracy: 0.3339 - val_loss: 2.6717 - val_accuracy: 0.2384\n",
      "Epoch 2/100\n",
      "79/79 [==============================] - 13s 166ms/step - loss: 1.5728 - accuracy: 0.4396 - val_loss: 2.0931 - val_accuracy: 0.3257\n",
      "Epoch 3/100\n",
      "79/79 [==============================] - 13s 159ms/step - loss: 1.4674 - accuracy: 0.4809 - val_loss: 1.7412 - val_accuracy: 0.4267\n",
      "Epoch 4/100\n",
      "79/79 [==============================] - 12s 158ms/step - loss: 1.3880 - accuracy: 0.5092 - val_loss: 1.7352 - val_accuracy: 0.4111\n",
      "Epoch 5/100\n",
      "79/79 [==============================] - 12s 154ms/step - loss: 1.3262 - accuracy: 0.5311 - val_loss: 1.6525 - val_accuracy: 0.4388\n",
      "Epoch 6/100\n",
      "79/79 [==============================] - 12s 156ms/step - loss: 1.2709 - accuracy: 0.5513 - val_loss: 1.8323 - val_accuracy: 0.4044\n",
      "Epoch 7/100\n",
      "79/79 [==============================] - 12s 155ms/step - loss: 1.2290 - accuracy: 0.5666 - val_loss: 1.6380 - val_accuracy: 0.4422\n",
      "Epoch 8/100\n",
      "79/79 [==============================] - 12s 155ms/step - loss: 1.1798 - accuracy: 0.5817 - val_loss: 1.6421 - val_accuracy: 0.4511\n",
      "Epoch 9/100\n",
      "79/79 [==============================] - 12s 153ms/step - loss: 1.1379 - accuracy: 0.5969 - val_loss: 1.6339 - val_accuracy: 0.4570\n",
      "Epoch 10/100\n",
      "79/79 [==============================] - 12s 148ms/step - loss: 1.0961 - accuracy: 0.6119 - val_loss: 1.8555 - val_accuracy: 0.4087\n",
      "Epoch 11/100\n",
      "79/79 [==============================] - 12s 153ms/step - loss: 1.0623 - accuracy: 0.6249 - val_loss: 1.6384 - val_accuracy: 0.4647\n",
      "Epoch 12/100\n",
      "79/79 [==============================] - 11s 143ms/step - loss: 1.0218 - accuracy: 0.6377 - val_loss: 1.6787 - val_accuracy: 0.4498\n",
      "Epoch 13/100\n",
      "79/79 [==============================] - 12s 149ms/step - loss: 0.9842 - accuracy: 0.6516 - val_loss: 1.7655 - val_accuracy: 0.4361\n",
      "Epoch 14/100\n",
      "79/79 [==============================] - 11s 143ms/step - loss: 0.9619 - accuracy: 0.6601 - val_loss: 2.0164 - val_accuracy: 0.4141\n",
      "313/313 [==============================] - 12s 34ms/step - loss: 1.6154 - accuracy: 0.4564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6154439449310303, 0.4564000070095062]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x_train, \n",
    "    y_train,\n",
    "    validation_data=[x_valid, y_valid],\n",
    "    epochs=100,\n",
    "    callbacks=callbacks_,\n",
    "    batch_size=512,\n",
    ")\n",
    "\n",
    "model = keras.models.load_model(MODEL_PATH)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend.clear_session()\n",
    "tf.random.set_seed(1992)\n",
    "np.random.seed(1992)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [layers.Flatten(input_shape=[32, 32, 3]), layers.BatchNormalization()]\n",
    ")\n",
    "\n",
    "for _ in range(20):\n",
    "    model.add(layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "\n",
    "model.add(layers.Dense(10, activation=\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b20fb8559386147dc77cba9f7518a8c175fc982b8c10bebe842520e16b3ac5d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
