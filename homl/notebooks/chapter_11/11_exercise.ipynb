{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Training Deep Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import (\n",
    "    datasets,\n",
    "    callbacks,\n",
    "    optimizers,\n",
    "    regularizers,\n",
    "    layers,\n",
    "    Sequential,\n",
    "    activations,\n",
    "    metrics,\n",
    "    losses,\n",
    "    backend,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 5e-4\n",
    "\n",
    "LOGS_PATH = os.path.join(\"..\", \"logs\", \"chapter_11\")\n",
    "MODEL_PATH = os.path.join(\"models\", \"my_cifar_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logdir():\n",
    "    now = datetime.now()\n",
    "    id_logdir = \"run_\" + now.strftime(\"%d_%m_%H%M%S\")\n",
    "    return os.path.join(LOGS_PATH, id_logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Practice training a deep neural network on the CIFAR10 image dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend.clear_session()\n",
    "tf.random.set_seed(1992)\n",
    "np.random.seed(1992)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
    "\n",
    "model.add(layers.Dense(10, activation=\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "x_train, x_valid = x_train[:40000], x_train[40000:]\n",
    "y_train, y_valid = y_train[:40000], y_train[40000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Nadam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    metrics=[\"accuracy\"],\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stop_cb = callbacks.EarlyStopping(patience=5, monitor='val_loss')\n",
    "tensorboard_cb = callbacks.TensorBoard(log_dir=get_logdir())\n",
    "model_checkpoint_cb = callbacks.ModelCheckpoint(MODEL_PATH, save_best_only=True)\n",
    "\n",
    "callbacks_ = [early_stop_cb, tensorboard_cb, model_checkpoint_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "79/79 [==============================] - 11s 101ms/step - loss: 9.4358 - accuracy: 0.1457 - val_loss: 2.6496 - val_accuracy: 0.1730\n",
      "Epoch 2/100\n",
      "79/79 [==============================] - 6s 81ms/step - loss: 2.2136 - accuracy: 0.2029 - val_loss: 2.3759 - val_accuracy: 0.1783\n",
      "Epoch 3/100\n",
      "79/79 [==============================] - 7s 85ms/step - loss: 2.0956 - accuracy: 0.2378 - val_loss: 2.1834 - val_accuracy: 0.2130\n",
      "Epoch 4/100\n",
      "79/79 [==============================] - 7s 84ms/step - loss: 2.0427 - accuracy: 0.2549 - val_loss: 2.2031 - val_accuracy: 0.2242\n",
      "Epoch 5/100\n",
      "79/79 [==============================] - 6s 82ms/step - loss: 1.9883 - accuracy: 0.2772 - val_loss: 2.0857 - val_accuracy: 0.2667\n",
      "Epoch 6/100\n",
      "79/79 [==============================] - 7s 84ms/step - loss: 1.9556 - accuracy: 0.2882 - val_loss: 2.0378 - val_accuracy: 0.2695\n",
      "Epoch 7/100\n",
      "79/79 [==============================] - 7s 83ms/step - loss: 1.9148 - accuracy: 0.3020 - val_loss: 2.1044 - val_accuracy: 0.2356\n",
      "Epoch 8/100\n",
      "79/79 [==============================] - 7s 85ms/step - loss: 1.8930 - accuracy: 0.3143 - val_loss: 1.9380 - val_accuracy: 0.3057\n",
      "Epoch 9/100\n",
      "79/79 [==============================] - 7s 90ms/step - loss: 1.8655 - accuracy: 0.3263 - val_loss: 2.0374 - val_accuracy: 0.2765\n",
      "Epoch 10/100\n",
      "79/79 [==============================] - 6s 80ms/step - loss: 1.8481 - accuracy: 0.3326 - val_loss: 2.1068 - val_accuracy: 0.2488\n",
      "Epoch 11/100\n",
      "79/79 [==============================] - 6s 79ms/step - loss: 1.8322 - accuracy: 0.3378 - val_loss: 2.0082 - val_accuracy: 0.3038\n",
      "Epoch 12/100\n",
      "79/79 [==============================] - 6s 79ms/step - loss: 1.8228 - accuracy: 0.3423 - val_loss: 2.1855 - val_accuracy: 0.2385\n",
      "Epoch 13/100\n",
      "79/79 [==============================] - 6s 81ms/step - loss: 1.7985 - accuracy: 0.3523 - val_loss: 1.8706 - val_accuracy: 0.3359\n",
      "Epoch 14/100\n",
      "79/79 [==============================] - 6s 81ms/step - loss: 1.7781 - accuracy: 0.3580 - val_loss: 1.9014 - val_accuracy: 0.3095\n",
      "Epoch 15/100\n",
      "79/79 [==============================] - 6s 79ms/step - loss: 1.7731 - accuracy: 0.3637 - val_loss: 1.8759 - val_accuracy: 0.3323\n",
      "Epoch 16/100\n",
      "79/79 [==============================] - 6s 80ms/step - loss: 1.7498 - accuracy: 0.3686 - val_loss: 2.0847 - val_accuracy: 0.2741\n",
      "Epoch 17/100\n",
      "79/79 [==============================] - 6s 78ms/step - loss: 1.7586 - accuracy: 0.3671 - val_loss: 1.9107 - val_accuracy: 0.3205\n",
      "Epoch 18/100\n",
      "79/79 [==============================] - 6s 80ms/step - loss: 1.7257 - accuracy: 0.3798 - val_loss: 1.8329 - val_accuracy: 0.3413\n",
      "Epoch 19/100\n",
      "79/79 [==============================] - 6s 80ms/step - loss: 1.7182 - accuracy: 0.3823 - val_loss: 1.9334 - val_accuracy: 0.3196\n",
      "Epoch 20/100\n",
      "79/79 [==============================] - 6s 79ms/step - loss: 1.7110 - accuracy: 0.3841 - val_loss: 1.9271 - val_accuracy: 0.3146\n",
      "Epoch 21/100\n",
      "79/79 [==============================] - 6s 80ms/step - loss: 1.6979 - accuracy: 0.3891 - val_loss: 1.7737 - val_accuracy: 0.3624\n",
      "Epoch 22/100\n",
      "79/79 [==============================] - 6s 76ms/step - loss: 1.6963 - accuracy: 0.3914 - val_loss: 1.8361 - val_accuracy: 0.3524\n",
      "Epoch 23/100\n",
      "79/79 [==============================] - 6s 81ms/step - loss: 1.6659 - accuracy: 0.4007 - val_loss: 1.9646 - val_accuracy: 0.3097\n",
      "Epoch 24/100\n",
      "79/79 [==============================] - 6s 77ms/step - loss: 1.6636 - accuracy: 0.4007 - val_loss: 1.9639 - val_accuracy: 0.3035\n",
      "Epoch 25/100\n",
      "79/79 [==============================] - 6s 76ms/step - loss: 1.6637 - accuracy: 0.4006 - val_loss: 1.9384 - val_accuracy: 0.3133\n",
      "Epoch 26/100\n",
      "79/79 [==============================] - 6s 80ms/step - loss: 1.6487 - accuracy: 0.4047 - val_loss: 1.7696 - val_accuracy: 0.3743\n",
      "Epoch 27/100\n",
      "79/79 [==============================] - 6s 79ms/step - loss: 1.6334 - accuracy: 0.4109 - val_loss: 1.9066 - val_accuracy: 0.3459\n",
      "Epoch 28/100\n",
      "79/79 [==============================] - 6s 80ms/step - loss: 1.6273 - accuracy: 0.4149 - val_loss: 1.8872 - val_accuracy: 0.3232\n",
      "Epoch 29/100\n",
      "79/79 [==============================] - 7s 85ms/step - loss: 1.6272 - accuracy: 0.4173 - val_loss: 2.0015 - val_accuracy: 0.2909\n",
      "Epoch 30/100\n",
      "79/79 [==============================] - 7s 85ms/step - loss: 1.6114 - accuracy: 0.4224 - val_loss: 2.0044 - val_accuracy: 0.3247\n",
      "Epoch 31/100\n",
      "79/79 [==============================] - 7s 89ms/step - loss: 1.6079 - accuracy: 0.4222 - val_loss: 1.8576 - val_accuracy: 0.3434\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7451 - accuracy: 0.3807\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7451152801513672, 0.380700021982193]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(\n",
    "    x_train, \n",
    "    y_train,\n",
    "    validation_data=[x_valid, y_valid],\n",
    "    callbacks=callbacks_,\n",
    "    epochs=100,\n",
    "    batch_size=512,\n",
    ")\n",
    "\n",
    "model = keras.models.load_model(MODEL_PATH)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend.clear_session()\n",
    "tf.random.set_seed(1992)\n",
    "np.random.seed(1992)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [layers.Flatten(input_shape=[32, 32, 3]), layers.BatchNormalization()]\n",
    ")\n",
    "\n",
    "for _ in range(20):\n",
    "    model.add(layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation(\"elu\"))\n",
    "\n",
    "model.add(layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Nadam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    metrics=[\"accuracy\"],\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stop_cb = callbacks.EarlyStopping(patience=5, monitor='val_loss')\n",
    "tensorboard_cb = callbacks.TensorBoard(log_dir=get_logdir())\n",
    "model_checkpoint_cb = callbacks.ModelCheckpoint(MODEL_PATH, save_best_only=True)\n",
    "\n",
    "callbacks_ = [early_stop_cb, tensorboard_cb, model_checkpoint_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x_train, \n",
    "    y_train,\n",
    "    validation_data=[x_valid, y_valid],\n",
    "    epochs=100,\n",
    "    callbacks=callbacks_,\n",
    "    batch_size=512,\n",
    ")\n",
    "\n",
    "model = keras.models.load_model(MODEL_PATH)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend.clear_session()\n",
    "tf.random.set_seed(1992)\n",
    "np.random.seed(1992)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [layers.Flatten(input_shape=[32, 32, 3]), layers.BatchNormalization()]\n",
    ")\n",
    "\n",
    "for _ in range(20):\n",
    "    model.add(layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "\n",
    "model.add(layers.AlphaDropout(rate=0.1))\n",
    "model.add(layers.Dense(10, activation=\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Nadam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    metrics=[\"accuracy\"],\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stop_cb = callbacks.EarlyStopping(patience=5, monitor='val_loss')\n",
    "tensorboard_cb = callbacks.TensorBoard(log_dir=get_logdir())\n",
    "model_checkpoint_cb = callbacks.ModelCheckpoint(MODEL_PATH, save_best_only=True)\n",
    "\n",
    "callbacks_ = [early_stop_cb, tensorboard_cb, model_checkpoint_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = layers.Normalization()\n",
    "normalizer.adapt(x_train)\n",
    "x_train_norm = normalizer(x_train)\n",
    "x_valid_norm = normalizer(x_valid)\n",
    "x_test_norm = normalizer(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "79/79 [==============================] - 15s 142ms/step - loss: 2.0276 - accuracy: 0.2992 - val_loss: 1.9749 - val_accuracy: 0.3366\n",
      "Epoch 2/100\n",
      "79/79 [==============================] - 7s 94ms/step - loss: 1.7228 - accuracy: 0.3869 - val_loss: 2.0700 - val_accuracy: 0.3235\n",
      "Epoch 3/100\n",
      "79/79 [==============================] - 7s 86ms/step - loss: 1.5917 - accuracy: 0.4335 - val_loss: 1.7898 - val_accuracy: 0.3743\n",
      "Epoch 4/100\n",
      "79/79 [==============================] - 7s 89ms/step - loss: 1.5053 - accuracy: 0.4658 - val_loss: 1.9211 - val_accuracy: 0.3795\n",
      "Epoch 5/100\n",
      "79/79 [==============================] - 7s 88ms/step - loss: 1.4417 - accuracy: 0.4879 - val_loss: 1.8735 - val_accuracy: 0.4049\n",
      "Epoch 6/100\n",
      "79/79 [==============================] - 7s 87ms/step - loss: 1.3836 - accuracy: 0.5084 - val_loss: 2.0781 - val_accuracy: 0.3891\n",
      "Epoch 7/100\n",
      "79/79 [==============================] - 7s 87ms/step - loss: 1.3583 - accuracy: 0.5185 - val_loss: 1.7699 - val_accuracy: 0.4197\n",
      "Epoch 8/100\n",
      "79/79 [==============================] - 7s 85ms/step - loss: 1.2974 - accuracy: 0.5393 - val_loss: 1.8030 - val_accuracy: 0.4371\n",
      "Epoch 9/100\n",
      "79/79 [==============================] - 7s 85ms/step - loss: 1.2498 - accuracy: 0.5575 - val_loss: 1.8223 - val_accuracy: 0.4113\n",
      "Epoch 10/100\n",
      "79/79 [==============================] - 7s 86ms/step - loss: 1.2209 - accuracy: 0.5635 - val_loss: 1.7632 - val_accuracy: 0.4214\n",
      "Epoch 11/100\n",
      "79/79 [==============================] - 7s 84ms/step - loss: 1.1758 - accuracy: 0.5810 - val_loss: 1.8709 - val_accuracy: 0.4010\n",
      "Epoch 12/100\n",
      "79/79 [==============================] - 7s 83ms/step - loss: 1.1429 - accuracy: 0.5933 - val_loss: 1.7441 - val_accuracy: 0.4528\n",
      "Epoch 13/100\n",
      "79/79 [==============================] - 7s 83ms/step - loss: 1.1081 - accuracy: 0.6071 - val_loss: 1.8956 - val_accuracy: 0.4210\n",
      "Epoch 14/100\n",
      "79/79 [==============================] - 6s 82ms/step - loss: 1.0767 - accuracy: 0.6187 - val_loss: 1.8398 - val_accuracy: 0.4636\n",
      "Epoch 15/100\n",
      "79/79 [==============================] - 6s 82ms/step - loss: 1.0360 - accuracy: 0.6337 - val_loss: 1.9179 - val_accuracy: 0.4344\n",
      "Epoch 16/100\n",
      "79/79 [==============================] - 7s 88ms/step - loss: 1.0066 - accuracy: 0.6412 - val_loss: 1.8905 - val_accuracy: 0.4679\n",
      "Epoch 17/100\n",
      "79/79 [==============================] - 7s 83ms/step - loss: 0.9779 - accuracy: 0.6532 - val_loss: 1.9444 - val_accuracy: 0.4435\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 1.7657 - accuracy: 0.4452\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7656570672988892, 0.44520002603530884]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x_train_norm,\n",
    "    y_train,\n",
    "    validation_data=[x_valid_norm, y_valid],\n",
    "    epochs=100,\n",
    "    callbacks=callbacks_,\n",
    "    batch_size=512,\n",
    ")\n",
    "\n",
    "model = keras.models.load_model(MODEL_PATH)\n",
    "model.evaluate(x_test_norm, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAlphaDropout(layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = Sequential(\n",
    "    [\n",
    "        MCAlphaDropout(layer.rate) if isinstance(layer, layers.AlphaDropout) else layer\n",
    "        for layer in model.layers\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predict_probas(mc_model, x, n_samples=10):\n",
    "    y_probas = [mc_model.predict(x) for _ in range(n_samples)]\n",
    "    return np.mean(y_probas, axis=0)\n",
    "\n",
    "def mc_dropout_predic_classes(mc_model, x, n_samples=10):\n",
    "    y_probas = mc_dropout_predict_probas(mc_model, x, n_samples)\n",
    "    return np.argmax(y_probas, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 6s 18ms/step\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 [==============================] - 6s 18ms/step\n",
      "313/313 [==============================] - 6s 18ms/step\n",
      "313/313 [==============================] - 6s 18ms/step\n",
      "313/313 [==============================] - 6s 18ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.45"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backend.clear_session()\n",
    "tf.random.set_seed(1992)\n",
    "np.random.seed(1992)\n",
    "\n",
    "y_pred = mc_dropout_predic_classes(mc_model, x_valid_norm)\n",
    "accuracy = np.mean(y_pred == y_valid[:, 0])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend.clear_session()\n",
    "tf.random.set_seed(1992)\n",
    "np.random.seed(1992)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [layers.Flatten(input_shape=[32, 32, 3]), layers.BatchNormalization()]\n",
    ")\n",
    "\n",
    "for _ in range(20):\n",
    "    model.add(layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "\n",
    "model.add(layers.AlphaDropout(rate=0.1))\n",
    "model.add(layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.learning_rate))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.learning_rate)\n",
    "    K.set_value(model.optimizer.learning_rate, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.learning_rate, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b20fb8559386147dc77cba9f7518a8c175fc982b8c10bebe842520e16b3ac5d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
