{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Training Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> No, it is not. When initializing all the weights to the same value, the effect of the gradient when updating will be the same to all neurons. This will cause the model not converging to the optimal and the model resulting will be a suboptimal solution. This symetry in the weights is not good for the model, it is necessary to generate all the weights randomly being different one from another. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Is it OK to initialize the bias terms to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The effect of the bias in the layers is not that big, these can be initialized to zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Name three advantages of the SELU activation function over ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. When the model contains only dense layers, the use of SELU may help to self-normalize the model, which avoids the . \n",
    "> 2. It has non-zero derivative for zero, which avoids dying units. \n",
    "> 3. It can output negative values, which will avoid dying units and vanishing neurons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - SELU is a better fit for models that only include dense layers and match all the requirements to self-normalization. \n",
    "> - Leaky ReLU works for those scenarios where SELU cannot be applied and we want the model to converge better. \n",
    "> - ReLU will work better when fast training and fast inference is a priority (e.g., edge inference).\n",
    "> - Tanh can be used when we are working with clasification problems, where the output may be between -1 and 1.\n",
    "> - The logistic function is used in classification problems where we need to calculate the probability of a class between 0 and 1. \n",
    "> - The softmax function is used when we are working with a multi-class problem, to determine the probability of belonging to different classes that are mutually exclusive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If the momentum hyperparameter is too close to 1, then the model will take a lot of speed passing the minimum, and returning and so on. This will imply that the model will take a long time to converge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Name three ways you can produce a sparse model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - By using the TensorFlow Model Optimization toolkit\n",
    "> - By training the model and then zeroing the tiny weights. \n",
    "> - A third option would be using $l1$ regularization, that tends to zero the tiny weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Yes, Dropout slows down the convergence of the model, but this is only during training. In inference, dropout is not active.\n",
    "> In the case of MC dropout, it will be in both cases. While training the model the dropout regularization will slow down the convergence of the model, and while inferencing, dropout is still active and the model will be required to predict several times to average the output. Therefore, in the latter scenario, both the training and the inference will be slower than without it. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b20fb8559386147dc77cba9f7518a8c175fc982b8c10bebe842520e16b3ac5d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
