{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12: Custom Models and Trainig with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Implement a custom layer that performs layer normalization (we will use this type of layer in Chapter 15):\n",
    "\n",
    "a. The `build()` method should define two trainable weights $\\alpha$ and $\\beta$, both of shape `input_shape[-1:]` and data type `tf.float32`. $\\alpha$ should be initialized with 1s, and $\\beta$ with 0s.\n",
    "\n",
    "b. The `call()` method should compute the mean $\\mu$ and standard deviation $\\sigma$ of each instance’s features. For this, you can use `tf.nn.moments(inputs, axes=-1, keepdims=True)`, which returns the mean $\\mu$ and the variance $\\sigma^2$ of all instances (compute the square root of the variance to get the standard deviation). Then the function should compute and return $\\alpha \\otimes (X-\\mu)/(\\sigma+\\epsilon) + \\beta$, where $\\otimes$ represents itemwise multiplication (*) and $\\epsilon$ is a smoothing term (a small constant to avoid division by zero, e.g., 0.001).\n",
    "\n",
    "c. Ensure that your custom layer produces the same (or very nearly the same) output as the `tf.keras.layers.LayerNormalization` layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import (\n",
    "    activations,\n",
    "    datasets,\n",
    "    layers,\n",
    "    losses,\n",
    "    metrics,\n",
    "    optimizers,\n",
    "    regularizers,\n",
    "    Sequential,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.epsilon = 1e-9\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.alpha = self.add_weight(\n",
    "            name=\"alpha\", shape=input_shape[-1:], initializer=\"ones\", dtype=tf.float32\n",
    "        )\n",
    "        self.beta = self.add_weight(\n",
    "            name=\"beta\", shape=input_shape[-1:], initializer=\"zeros\", dtype=tf.float32\n",
    "        )\n",
    "    \n",
    "    def call(self, x):\n",
    "        mean, var = tf.nn.moments(x, axes=-1, keepdims=True)\n",
    "        std_var = tf.sqrt(var)\n",
    "        x_normalized = tf.math.multiply(self.alpha, (x - mean))/((std_var + self.epsilon) + self.beta)\n",
    "        return x_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 7), dtype=float32, numpy=\n",
       "array([[-5.9604645e-08, -5.9604645e-08,  0.0000000e+00, -2.9802322e-08,\n",
       "         0.0000000e+00,  1.1920929e-07,  0.0000000e+00],\n",
       "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00]], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [\n",
    "    [40, 50, 55, 78, 345, 324, 22],\n",
    "    [99, 22, 15, 88, 44, 5, 567],\n",
    "    [4, 324, 2, 44, 77, 9234, 6],\n",
    "]\n",
    "x = tf.Variable(x, dtype=tf.float32)\n",
    "\n",
    "normalizer = NormLayer()\n",
    "x_normalized = normalizer(x)\n",
    "\n",
    "norm_layer = layers.LayerNormalization()\n",
    "x_normalized_keras = norm_layer(x)\n",
    "\n",
    "x_normalized - x_normalized_keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The difference between the two results is really low $10^{-8}$ or $10^{-7}$. We can say that the implementation of the layer has been successful. In cases where this difference may be a problem (i.e., when we need the maximum precision) it would be better to use the keras implementation.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Train a model using a custom training loop to tackle the Fashion MNIST dataset (see Chapter 10):\n",
    "\n",
    "a. Display the epoch, iteration, mean training loss, and mean accuracy over each epoch (updated at each iteration), as well as the validation loss and accuracy at the end of each epoch.\n",
    "\n",
    "\n",
    "b. Try using a different optimizer with a different learning rate for the upper layers and the lower layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Let's first download the data of the **Fashion MNIST** dataset:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 GPU found at /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "gpus = len(tf.config.list_physical_devices(\"GPU\"))\n",
    "text_gpus = f\"{gpus} GPUs\" if gpus > 1 else f\"{gpus} GPU\"\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if gpus > 0:\n",
    "    print(f\"{text_gpus} found at {device_name}\")\n",
    "    BATCH_SIZE = 1024\n",
    "else:\n",
    "    raise SystemError(\"No GPU found\")\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "tf.random.set_seed(1992)\n",
    "\n",
    "MODEL_PATH = os.path.join('..', '..', 'models', 'chapter_12')\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def random_batch(x, y, batch_size):\n",
    "    idx = tf.random.uniform(shape=[batch_size,], maxval=tf.shape(x)[0], dtype=tf.int32)\n",
    "    x_tf = tf.gather(x, indices=idx)\n",
    "    y_tf = tf.gather(y, indices=idx)\n",
    "    return x_tf, y_tf\n",
    "\n",
    "\n",
    "def print_status_bar(step, total, loss, metrics=None):\n",
    "    metrics = \" - \".join([f\"{m.name}: {m.result()}\" for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if step < total else \"\\n\"\n",
    "    tf.print(f\"\\r{step}/{total} - \" + metrics, end=end)\n",
    "\n",
    "\n",
    "def training_loop(model, x_train, y_train, n_epochs, batch_size=BATCH_SIZE, training=True):\n",
    "    \n",
    "    n_steps = len(x_train)// batch_size\n",
    "    optimizer_low = optimizers.Adam(learning_rate=1e-4)\n",
    "    optimizer_up = optimizers.Nadam(learning_rate=3e-4)\n",
    "    loss_fn = losses.sparse_categorical_crossentropy\n",
    "    mean_loss = keras.metrics.Mean(name='mean_loss')\n",
    "    metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        tf.print(f\"Epoch {epoch}/{n_epochs}\")\n",
    "        for step in tf.range(1, n_steps + 1, 1):\n",
    "            x_train_batch, y_train_batch = random_batch(x_train, y_train, batch_size)\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = model(x_train_batch, training=training)\n",
    "                main_loss = tf.reduce_mean(loss_fn(y_train_batch, y_pred))\n",
    "                loss = tf.add_n([main_loss] + model.losses)\n",
    "            \n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer_low.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            mean_loss(loss)\n",
    "            for metric in metrics:\n",
    "                metric(y_train_batch, y_pred)\n",
    "\n",
    "            print_status_bar(step, n_steps, mean_loss, metrics)\n",
    "        \n",
    "        for metric in [mean_loss] + metrics:\n",
    "            metric.reset_states()\n",
    "    \n",
    "    return model\n",
    "\n",
    "@tf.function\n",
    "def load_fashion_mnist():\n",
    "    (x_train, y_train), (x_test, y_test) = datasets.fashion_mnist.load_data()\n",
    "    x_train, x_valid = x_train[:50000], x_train[50000:]\n",
    "    y_train, y_valid = y_train[:50000], y_train[50000:]\n",
    "    \n",
    "    return {\n",
    "        \"x_train\": x_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"x_valid\": x_valid,\n",
    "        \"y_valid\": y_valid,\n",
    "        \"x_test\": x_test,\n",
    "        \"y_test\": y_test,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(n_epochs=10, callbacks=False):\n",
    "    \n",
    "    # obtain the data\n",
    "    data = load_fashion_mnist()\n",
    "    \n",
    "    regularizer = regularizers.l1_l2()\n",
    "\n",
    "    model_name = 'fashion_mnist_classifier'\n",
    "    model = Sequential(\n",
    "        [\n",
    "            layers.Flatten(input_shape=(28, 28)),\n",
    "            layers.BatchNormalization(),\n",
    "        ],\n",
    "        name=model_name\n",
    "    )\n",
    "\n",
    "    for i in tf.range(1, 5, 1):\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                100,\n",
    "                kernel_initializer=\"he_normal\",\n",
    "                kernel_regularizer=regularizer,\n",
    "            )\n",
    "        )\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Activation(\"elu\"))\n",
    "\n",
    "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
    "    \n",
    "    model_trained = training_loop(model, data['x_train'], data['y_train'], n_epochs=n_epochs)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "48/48 - mean_loss: 74.74301147460938 - sparse_categorical_accuracy: 0.44689941406256564\n",
      "Epoch 2/50\n",
      "48/48 - mean_loss: 69.30262756347656 - sparse_categorical_accuracy: 0.6154785156255916\n",
      "Epoch 3/50\n",
      "48/48 - mean_loss: 64.22531127929688 - sparse_categorical_accuracy: 0.6595458984375325\n",
      "Epoch 4/50\n",
      "48/48 - mean_loss: 59.453487396240234 - sparse_categorical_accuracy: 0.6914062525036628\n",
      "Epoch 5/50\n",
      "48/48 - mean_loss: 54.98728942871094 - sparse_categorical_accuracy: 0.71840417385101327\n",
      "Epoch 6/50\n",
      "48/48 - mean_loss: 50.82316589355469 - sparse_categorical_accuracy: 0.73406982421875962\n",
      "Epoch 7/50\n",
      "48/48 - mean_loss: 46.95559310913086 - sparse_categorical_accuracy: 0.75046795606613167\n",
      "Epoch 8/50\n",
      "48/48 - mean_loss: 43.37821960449219 - sparse_categorical_accuracy: 0.76430261135101321\n",
      "Epoch 9/50\n",
      "48/48 - mean_loss: 40.068580627441406 - sparse_categorical_accuracy: 0.7766317129135132\n",
      "Epoch 10/50\n",
      "48/48 - mean_loss: 37.022850036621094 - sparse_categorical_accuracy: 0.7848714590072632\n",
      "Epoch 11/50\n",
      "48/48 - mean_loss: 34.21119689941406 - sparse_categorical_accuracy: 0.79561364650726327\n",
      "Epoch 12/50\n",
      "48/48 - mean_loss: 31.629383087158203 - sparse_categorical_accuracy: 0.8035075068473816\n",
      "Epoch 13/50\n",
      "48/48 - mean_loss: 29.26141929626465 - sparse_categorical_accuracy: 0.81561279296875532\n",
      "Epoch 14/50\n",
      "48/48 - mean_loss: 27.0965576171875 - sparse_categorical_accuracy: 0.821411132812583572\n",
      "Epoch 15/50\n",
      "48/48 - mean_loss: 25.115825653076172 - sparse_categorical_accuracy: 0.8255819082260132\n",
      "Epoch 16/50\n",
      "48/48 - mean_loss: 23.306812286376953 - sparse_categorical_accuracy: 0.8329671621322632\n",
      "Epoch 17/50\n",
      "48/48 - mean_loss: 21.674537658691406 - sparse_categorical_accuracy: 0.8327230215072632\n",
      "Epoch 18/50\n",
      "48/48 - mean_loss: 20.167959213256836 - sparse_categorical_accuracy: 0.8404337763786316\n",
      "Epoch 19/50\n",
      "48/48 - mean_loss: 18.80026626586914 - sparse_categorical_accuracy: 0.84737145900726322\n",
      "Epoch 20/50\n",
      "48/48 - mean_loss: 17.571002960205078 - sparse_categorical_accuracy: 0.8483276367187572\n",
      "Epoch 21/50\n",
      "48/48 - mean_loss: 16.436349868774414 - sparse_categorical_accuracy: 0.8559367060661316\n",
      "Epoch 22/50\n",
      "48/48 - mean_loss: 15.418403625488281 - sparse_categorical_accuracy: 0.8545532226562598\n",
      "Epoch 23/50\n",
      "48/48 - mean_loss: 14.487930297851562 - sparse_categorical_accuracy: 0.8532714843751487\n",
      "Epoch 24/50\n",
      "48/48 - mean_loss: 13.630203247070312 - sparse_categorical_accuracy: 0.8581542968750876\n",
      "Epoch 25/50\n",
      "48/48 - mean_loss: 12.831838607788086 - sparse_categorical_accuracy: 0.8622843623161316\n",
      "Epoch 26/50\n",
      "48/48 - mean_loss: 12.107458114624023 - sparse_categorical_accuracy: 0.8599650263786316\n",
      "Epoch 27/50\n",
      "48/48 - mean_loss: 11.4110746383667 - sparse_categorical_accuracy: 0.865946471691131683\n",
      "Epoch 28/50\n",
      "48/48 - mean_loss: 10.758402824401855 - sparse_categorical_accuracy: 0.8651326894760132\n",
      "Epoch 29/50\n",
      "48/48 - mean_loss: 10.149337768554688 - sparse_categorical_accuracy: 0.8663533926010132\n",
      "Epoch 30/50\n",
      "48/48 - mean_loss: 9.560356140136719 - sparse_categorical_accuracy: 0.8677164912223816\n",
      "Epoch 31/50\n",
      "48/48 - mean_loss: 9.015787124633789 - sparse_categorical_accuracy: 0.8667806386947632\n",
      "Epoch 32/50\n",
      "48/48 - mean_loss: 8.490060806274414 - sparse_categorical_accuracy: 0.8673502802848816\n",
      "Epoch 33/50\n",
      "48/48 - mean_loss: 7.986014366149902 - sparse_categorical_accuracy: 0.8700358271598816\n",
      "Epoch 34/50\n",
      "48/48 - mean_loss: 7.523484706878662 - sparse_categorical_accuracy: 0.87127685546875334\n",
      "Epoch 35/50\n",
      "48/48 - mean_loss: 7.088465690612793 - sparse_categorical_accuracy: 0.86861169338226324\n",
      "Epoch 36/50\n",
      "48/48 - mean_loss: 6.668408393859863 - sparse_categorical_accuracy: 0.86777752637863166\n",
      "Epoch 37/50\n",
      "48/48 - mean_loss: 6.252211570739746 - sparse_categorical_accuracy: 0.87209069728851328\n",
      "Epoch 38/50\n",
      "48/48 - mean_loss: 5.8712158203125 - sparse_categorical_accuracy: 0.8732910156255476686\n",
      "Epoch 39/50\n",
      "48/48 - mean_loss: 5.526206970214844 - sparse_categorical_accuracy: 0.87117516994476326\n",
      "Epoch 40/50\n",
      "48/48 - mean_loss: 5.1737236976623535 - sparse_categorical_accuracy: 0.8760783076286316\n",
      "Epoch 41/50\n",
      "48/48 - mean_loss: 4.864221572875977 - sparse_categorical_accuracy: 0.87432861328125584\n",
      "Epoch 42/50\n",
      "48/48 - mean_loss: 4.572388648986816 - sparse_categorical_accuracy: 0.87211102247238169\n",
      "Epoch 43/50\n",
      "48/48 - mean_loss: 4.286474227905273 - sparse_categorical_accuracy: 0.87459313869476326\n",
      "Epoch 44/50\n",
      "48/48 - mean_loss: 4.012294769287109 - sparse_categorical_accuracy: 0.87723797559738168\n",
      "Epoch 45/50\n",
      "48/48 - mean_loss: 3.761559247970581 - sparse_categorical_accuracy: 0.87774658203125578\n",
      "Epoch 46/50\n",
      "48/48 - mean_loss: 3.540473461151123 - sparse_categorical_accuracy: 0.87280273437542728\n",
      "Epoch 47/50\n",
      "48/48 - mean_loss: 3.3054933547973633 - sparse_categorical_accuracy: 0.8784993886947632\n",
      "Epoch 48/50\n",
      "48/48 - mean_loss: 3.1083571910858154 - sparse_categorical_accuracy: 0.8771769404411316\n",
      "Epoch 49/50\n",
      "48/48 - mean_loss: 2.9107065200805664 - sparse_categorical_accuracy: 0.8783162832260132\n",
      "Epoch 50/50\n",
      "48/48 - mean_loss: 2.735363483428955 - sparse_categorical_accuracy: 0.87797039747238169\n"
     ]
    }
   ],
   "source": [
    "training_model(n_epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b20fb8559386147dc77cba9f7518a8c175fc982b8c10bebe842520e16b3ac5d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
