{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12: Custom Models and Trainig with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How would you describe TensorFlow in a short sentence? What are its main features? Can you name other popular deep learning libraries?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow is a Python library that allows us to process tensors in an efficient way. Among its main features is the use of a GPU to process the tensors in parallel, to create graphs to optimize the functions, provides updated optimizers, and an entire production ecosystem.\n",
    "\n",
    "Other popular DL libraries are: _PyTorch, Caffe, Scikit-Learn, Transformers, Theano, NLTK_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is TensorFlow a drop-in replacement for NumPy? What are the main differences between the two?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow offers some functionalities that are available in Numpy with some differences that may affect when deciding which to use:\n",
    "- TensorFlow uses floats with 32 bits by default, since these are more efficient when processing tensors, while Numpy uses 64 bit tensors by default. \n",
    "- In TensorFlow, the data type conversion is not performed automatically, therefore, it is not possible to work with 2 tensors that have different data type, something that is natural in Numpy. \n",
    "\n",
    "TensorFlow may offer some restriction in terms of the use and less flexibility than numpy, but on the other hand it offers an optimized library to process tensor in a more efficient way, requiring less time and memory to do it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do you get the same result with tf.range(10) and tf.constant(np.ara⁠nge(10))?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both scenarios we will get a tf.Tensor with shape `(10,)`, but when executing `tf.range(10)` we will get a 32 bit tensor, when we execute `tf.constant(np.arange(10))` we obtain a 64 bit tensor. The latter will use more memory when processing it or operating with other tensors. It would be necessary to first convert it to 32 bit tensor to make sure it can be processed with other tensors. \n",
    "\n",
    "Additionaly, it would be less efficient to process this tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you name six other data structures available in TensorFlow, beyond regular tensors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Sparse Tensors**: Tensors containing mostly zeros.\n",
    "- **Ragged Tensors**: List of tensors with the same data type but with different sizes.\n",
    "- **String Tensors**: Tensors of type `tf.string` that represent byte strings.\n",
    "- **Queues**: This data structure stores tensors accross multiple steps.\n",
    "- **Tensor Arrays**: List of tensors. \n",
    "- **Sets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can define a custom loss function by writing a function or by subclassing the `tf.keras.losses.Loss` class. When would you use each option?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to define a loss funtion is by writing a normal function that simplifies its creation. However, the hyperparameters are not saved and it is necessary to manually track them. If we want to keep save the values of the hyperparameters with the model, we should subclass the `tf.keras.losses.Loss` class. In both cases is necessary to pass the dictionary with the function or class when loading the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarly, you can define a custom metric in a function or as a subclass of `tf.keras.metrics.Metric`. When would you use each option?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a custom metric by writing a function when the calculation will not be a streaming metric. If we want to save the values of the hyperparameters and/or we are working with a streaming metric, we need to subclass the `tf.keras.metrics.Metric` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When should you create a custom layer versus a custom model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should use a custom layer whenever we are working with a custom layer, or a block that will be repeated in our model. We should use a custom model whenever we are working with a functional model that is able to be trained and make inference afterwards.\n",
    "\n",
    "A custom model offers all the methods included in a custom layer, plus some methods specifics of the model class. Even though these 2 could be used indistinctly, we should use each in its context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are some use cases that require writing your own custom training loop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should write a custom training loop whenever we need an extra of flexibility. Some examples may be if we need to use different optimizers in the same model, or if we need to control the gradients in the training loop, when calculating some metric in a specific way may be another factor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can custom Keras components contain arbitrary Python code, or must they be convertible to TF functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the code within a custom component should be convertible to TF functions, otherwise, there may be some errors. If the code used is not available for TensorFlow, or if we use some external libraries, it may raise some errors or return values different from those expected. It is possible to tell TensorFlow not to convert the code within a customo component, but this may cause performance issues when training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the main rules to respect if you want a function to be convertible to a TF function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main rules are:\n",
    "\n",
    "- If we use any external library, this portion of the code will run only during tracing, and they will not be part of the graph. It is possible to wrap any arbitrary code under `tf.py_function()` but this will affect the performance since there will not be an optimized graph.\n",
    "- We can use other python functions or TF functions, but they must follow the same rules.\n",
    "- It is better to create variables outside of the function to avoid exceptions. If we need to assign a new value to a variable we can use the `assign()` method.\n",
    "- The source code of the function should be available to TensorFlow, otherwise the graph will fail or have limited functionality. \n",
    "- When creating a loop function, this loop can only iterate over a `tf.Tensor` or a `tf.data.Dataset`. It is important to use `tf.range(x)` instead of `range(x)`\n",
    "- Whenever it is possible, we should use a vectorized implementation instead of a for loop. This will improve the performance of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When would you need to create a dynamic Keras model? How do you do that? Why not make all your models dynamic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dynamic Keras model can be useful for debugging, as it will not compile any custom component to a TF Function, and you can use any Python debugger to debug your code. It can also be useful if you want to include arbitrary Python code in your model (or in your training code), including calls to external libraries. To make a model dynamic, you must set `dynamic=True` when creating it. Alternatively, you can set `run_eagerly=True` when calling the model’s `compile()` method. Making a model dynamic prevents Keras from using any of TensorFlow’s graph features, so it will slow down training and inference, and you will not have the possibility to export the computation graph, which will limit your model’s portability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b20fb8559386147dc77cba9f7518a8c175fc982b8c10bebe842520e16b3ac5d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
