{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 14 Deep Computer Vision Using Convolutional Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmenendezg/Developer/Books/.venv/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/Users/mmenendezg/Developer/Books/.venv/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: [\"[Errno 2] The file to load file system plugin from does not exist.: '/Users/mmenendezg/Developer/Books/.venv/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so'\"]\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/Users/mmenendezg/Developer/Books/.venv/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/Users/mmenendezg/Developer/Books/.venv/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: [\"dlopen(/Users/mmenendezg/Developer/Books/.venv/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so, 0x0006): tried: '/Users/mmenendezg/Developer/Books/.venv/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/mmenendezg/Developer/Books/.venv/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so' (no such file), '/Users/mmenendezg/Developer/Books/.venv/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so' (no such file)\"]\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n",
      "/Users/mmenendezg/Developer/Books/.venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from keras import (\n",
    "    Sequential,\n",
    "    applications,\n",
    "    layers,\n",
    "    optimizers,\n",
    "    losses,\n",
    "    callbacks,\n",
    "    metrics\n",
    ")\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.environ[\"DATA_PATH\"]\n",
    "MODEL_PATH = os.path.join(\"..\", \"..\", \"models\", \"chapter_14\")\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "tf.autograph.set_verbosity(0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Build your own CNN from scartch and try to achieve the highest possible accuracy on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder to store the images\n",
    "mnist_folder = os.path.join(DATA_PATH, \"MNIST\")\n",
    "os.makedirs(mnist_folder, exist_ok=True)\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_set, valid_set, test_set = tfds.load(\n",
    "    \"mnist\",\n",
    "    split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\n",
    "    data_dir=mnist_folder,\n",
    "    as_supervised=True,\n",
    ")\n",
    "\n",
    "normalize_pixels = lambda x, y: (x / 255, y)\n",
    "\n",
    "train_set = (\n",
    "    train_set.cache()\n",
    "    .map(normalize_pixels, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(4096)\n",
    ")\n",
    "\n",
    "valid_set = (\n",
    "    valid_set.cache()\n",
    "    .map(normalize_pixels, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(4096)\n",
    ")\n",
    "\n",
    "test_set = (\n",
    "    test_set.cache()\n",
    "    .map(normalize_pixels, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(4096)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST images have dimensions `[28, 28, 1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(1992)\n",
    "\n",
    "conv_layer = partial(\n",
    "    layers.SeparableConv2D,\n",
    "    kernel_size=3,\n",
    "    padding=\"same\",\n",
    "    activation=\"elu\",\n",
    "    kernel_initializer=\"he_normal\"\n",
    ")\n",
    "\n",
    "mnist_model = Sequential([\n",
    "    layers.Conv2D(filters=32, kernel_size=7, input_shape=[28, 28, 1]),\n",
    "    layers.MaxPool2D(),\n",
    "    conv_layer(filters=64),\n",
    "    conv_layer(filters=64),\n",
    "    layers.MaxPool2D(),\n",
    "    conv_layer(filters=128),\n",
    "    conv_layer(filters=128),\n",
    "    layers.GlobalAvgPool2D(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_optimizer = optimizers.Nadam()\n",
    "mnist_loss = losses.sparse_categorical_crossentropy\n",
    "mnist_metric = \"accuracy\"\n",
    "\n",
    "mnist_model.compile(loss=mnist_loss, optimizer=mnist_optimizer, metrics=[mnist_metric])\n",
    "\n",
    "# Define the callbacks to avoid the model been trained longer\n",
    "# than necessary\n",
    "\n",
    "local_logs_path = os.path.join(\"..\", \"..\", \"reports\", \"logs\", \"chapter_14\", \"mnist\")\n",
    "log_dir = help_functions.get_logdir(date_type=\"datetime\", path_folder=local_logs_path)\n",
    "model_path = os.path.join(MODEL_PATH, \"mnist_model.h5\")\n",
    "tensorboard_cb = callbacks.TensorBoard(\n",
    "    log_dir=log_dir, profile_batch=10, histogram_freq=1\n",
    ")\n",
    "early_stopping_cb = callbacks.EarlyStopping(patience=5)\n",
    "model_checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "    filepath=model_path, save_best_only=True\n",
    ")\n",
    "callbacks_list = [tensorboard_cb, early_stopping_cb, model_checkpoint_cb]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model.fit(\n",
    "    train_set,\n",
    "    validation_data=valid_set,\n",
    "    callbacks=callbacks_list,\n",
    "    epochs=100,\n",
    ")\n",
    "\n",
    "evaluation = mnist_model.evaluate(test_set, verbose=0)\n",
    "print(f'The accuracy of the model is {evaluation[1] * 100:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Use transfer learning for large image classification, going through these steps:\n",
    "\n",
    "##### a. Create a training set containing at aleast 100 images per class.\n",
    "##### b. Split it into a training set, a validation set, and a test set.\n",
    "##### c. Build the input pipeline, apply the appropriate preprocessing operations, and optionally add data augmentation.\n",
    "##### d. Fine-tune a pretrained model on this dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The dataset we'll use for this example is the [`colorectal_histology`](https://www.tensorflow.org/datasets/catalog/colorectal_histology) dataset from TensorFlow. The dataset is composed of 5000 images `150x150x3` classified in 8 different classes. The dataset returns only training set, and for the splitting is necessary to sample the same amount for each class. There are 625 images per class, and we will divide them in 70% for training, 15% for validation, and 15% for test*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIST_CLASSES = [\n",
    "    \"Tumor\",\n",
    "    \"Stroma\",\n",
    "    \"Complex\",\n",
    "    \"Lympho\",\n",
    "    \"Debris\",\n",
    "    \"Mucosa\",\n",
    "    \"Adipose\",\n",
    "    \"Empty\"\n",
    "]\n",
    "\n",
    "MOBILENET_MODEL_PATH = \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/feature_vector/5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder to store the dataset\n",
    "hist_path = os.path.join(DATA_PATH, \"colorectal_histology\")\n",
    "os.makedirs(hist_path, exist_ok=True)\n",
    "\n",
    "histology_dataset, histology_info = tfds.load(\n",
    "    \"colorectal_histology\",\n",
    "    split=\"train\",\n",
    "    data_dir=hist_path,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*When loading the data directly from TensorFlow Datasets, the data cannot be assured to have the same amount of images per class. That is why it is necessary to make this distribution on our side:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set, test_set = ml_functions.balanced_split(histology_dataset, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "for images, labels in train_set.shuffle(5000).take(1):\n",
    "    for idx in range(9):\n",
    "        plt.subplot(3, 3, idx + 1)\n",
    "        plt.imshow(images[idx])\n",
    "        plt.title(f\"Class: {HIST_CLASSES[labels[idx]]}\")\n",
    "        plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_dataset(dataset, split='train'):\n",
    "    dataset = dataset.cache()\n",
    "    if split == 'train':\n",
    "        dataset.shuffle(5000)\n",
    "    dataset = dataset.batch(512).prefetch(AUTOTUNE)\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_set, valid_set, test_set, epochs=100):\n",
    "\n",
    "    # Pipeline for the datasets\n",
    "    train_set = pipeline_dataset(train_set)\n",
    "    valid_set = pipeline_dataset(valid_set)\n",
    "    test_set = pipeline_dataset(test_set)\n",
    "\n",
    "    # # Normalization of the pixel values\n",
    "    # train_sample = train_set.take(1).map(lambda image, label: image)\n",
    "    # normalizer = layers.Normalization(input_shape=[None, 150, 150, 3])\n",
    "    # normalizer.adapt(train_sample)\n",
    "\n",
    "    # train_set = normalizer(train_set)\n",
    "    # valid_set = normalizer(valid_set)\n",
    "    # test_set = normalizer(test_set)\n",
    "\n",
    "    # Data Augmentation\n",
    "    data_augmentation = Sequential(\n",
    "        [\n",
    "            layers.RandomBrightness(factor=0.3),\n",
    "            layers.RandomFlip(),\n",
    "            layers.RandomContrast(factor=0.3),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Get model from Tensorflow Hub\n",
    "    resnet_101 = Sequential(\n",
    "        [\n",
    "            hub.KerasLayer(\n",
    "                MOBILENET_MODEL_PATH,\n",
    "                trainable=True,\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Model\n",
    "    model = Sequential(\n",
    "        [\n",
    "            layers.Normalization(input_shape=[150, 150, 3]),\n",
    "            layers.ZeroPadding2D(padding=37),\n",
    "            data_augmentation,\n",
    "            resnet_101,\n",
    "            layers.Dense(8, activation=\"softmax\"),\n",
    "        ],\n",
    "        name=\"colorectal_histology_model\",\n",
    "    )\n",
    "\n",
    "    # Compile the model\n",
    "\n",
    "    optimizer = optimizers.Adam(learning_rate=1e-4)\n",
    "    model.compile(\n",
    "        loss=losses.sparse_categorical_crossentropy,\n",
    "        metrics=[\"accuracy\"],\n",
    "        optimizer=optimizer,\n",
    "    )\n",
    "\n",
    "    # Callbacks\n",
    "    exponential_decay_fn = learning_rate_functions.exponential_decay_with_warmup(\n",
    "        lr_start=1e-4\n",
    "    )\n",
    "    lr_scheduler_cb = callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "\n",
    "    hist_logs_path = os.path.join(\n",
    "        \"..\", \"..\", \"reports\", \"logs\", \"chapter_14\", \"colorectal_histology\"\n",
    "    )\n",
    "    log_dir = help_functions.get_logdir(path_folder=hist_logs_path)\n",
    "    tensorboard_cb = callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "    model_path = os.path.join(MODEL_PATH, \"colorectal_histology_model.h5\")\n",
    "    model_checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "        filepath=model_path, save_best_only=True\n",
    "    )\n",
    "    \n",
    "    early_stopping_cb = callbacks.EarlyStopping(patience=5)\n",
    "    \n",
    "    callbacks_list = [\n",
    "        lr_scheduler_cb,\n",
    "        tensorboard_cb,\n",
    "        model_checkpoint_cb,\n",
    "        early_stopping_cb\n",
    "    ]\n",
    "    \n",
    "    # Training of the model\n",
    "    model.fit(\n",
    "        train_set,\n",
    "        validation_data=valid_set,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks_list,\n",
    "    )\n",
    "    \n",
    "    evaluation = model.evaluate(test_set, verbose=0)\n",
    "    \n",
    "    print(f\"The Training has finished. The result is the following:\")\n",
    "    print(f\"\\n\\tModel Accuracy: {evaluation[1] * 100:.4f}\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(1992)\n",
    "np.random.seed(1992)\n",
    "\n",
    "colorectal_hist_model = train_model(train_set, valid_set, test_set, epochs=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Go through Tensorflow's [Style Transfer Tutorial](https://www.tensorflow.org/tutorials/generative/style_transfer)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"] = \"COMPRESSED\"\n",
    "\n",
    "TF_MODEL = \"https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import matplotlib as mpl\n",
    "import PIL.Image\n",
    "import time\n",
    "import functools\n",
    "\n",
    "mpl.rcParams[\"figure.figsize\"] = (12, 12)\n",
    "mpl.rcParams[\"axes.grid\"] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_image(tensor):\n",
    "    tensor = tensor * 255\n",
    "    tensor = np.array(tensor, dtype=np.uint8)\n",
    "\n",
    "    if np.ndim(tensor) > 3:\n",
    "        assert tensor.shape[0] == 1\n",
    "        tensor = tensor[0]\n",
    "    return PIL.Image.fromarray(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_path = tf.keras.utils.get_file(\n",
    "    \"YellowLabradorLooking_new.jpg\",\n",
    "    \"https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg\",\n",
    ")\n",
    "style_path = tf.keras.utils.get_file(\n",
    "    \"kandinsky5.jpg\",\n",
    "    \"https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(path_to_img):\n",
    "    max_dim = 512\n",
    "    img = tf.io.read_file(path_to_img)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
    "    long_dim = max(shape)\n",
    "    scale = max_dim / long_dim\n",
    "\n",
    "    new_shape = tf.cast(shape * scale, tf.int32)\n",
    "\n",
    "    img = tf.image.resize(img, new_shape)\n",
    "    img = img[tf.newaxis, :]\n",
    "    return img\n",
    "\n",
    "\n",
    "def imshow(image, title=None):\n",
    "    if len(image.shape) > 3:\n",
    "        image = tf.squeeze(image, axis=0)\n",
    "    \n",
    "    plt.imshow(image)\n",
    "    if title:\n",
    "        plt.title(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = load_img(content_path)\n",
    "style_image = load_img(style_path)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "imshow(content_image, \"Content Image\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "imshow(style_image, \"Style Image\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fast Style Transfer Using TF-Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model = hub.load(TF_MODEL)\n",
    "stylized_image = transfer_model(tf.constant(content_image), tf.constant(style_image))[0]\n",
    "tensor_to_image(stylized_image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define content and style Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_image = keras.applications.vgg19.preprocess_input(content_image * 255)\n",
    "pre_image = tf.image.resize(pre_image, (224, 224))\n",
    "vgg19 = keras.applications.VGG19(include_top=True, weights=\"imagenet\")\n",
    "prediction_probs = vgg19(pre_image)\n",
    "prediction_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5 = tf.keras.applications.vgg19.decode_predictions(prediction_probs.numpy())[0]\n",
    "[(class_name, prob) for (number, class_name, prob) in top_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19 = applications.VGG19(include_top=False, weights=\"imagenet\")\n",
    "\n",
    "print()\n",
    "for layer in vgg19.layers:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layers = [\"block5_conv2\"]\n",
    "\n",
    "style_layers = [\n",
    "    \"block1_conv1\",\n",
    "    \"block2_conv1\",\n",
    "    \"block3_conv1\",\n",
    "    \"block4_conv1\",\n",
    "    \"block5_conv1\",\n",
    "]\n",
    "\n",
    "num_content_layers = len(content_layers)\n",
    "num_style_layers = len(style_layers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_layers(layer_names):\n",
    "    vgg = tf.keras.applications.VGG19(include_top=False, weights=\"imagenet\")\n",
    "    vgg.trainable = False\n",
    "    \n",
    "    outputs = [vgg.get_layer(name).output for name in layer_names]\n",
    "\n",
    "    model = tf.keras.Model([vgg.input], outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_extractor = vgg_layers(style_layers)\n",
    "style_outputs = style_extractor(style_image * 255)\n",
    "\n",
    "for name, output in zip(style_layers, style_outputs):\n",
    "    output = output.numpy()\n",
    "    print(name)\n",
    "    print(f\"\\tshape: {output.shape}\")\n",
    "    print(f\"\\tmin: {output.min()}\")\n",
    "    print(f\"\\tmax: {output.max()}\")\n",
    "    print(f\"\\tmean:{output.mean()}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(input_tensor):\n",
    "    result = tf.linalg.einsum(\"bijc,bijd->bcd\", input_tensor, input_tensor)\n",
    "    input_shape = tf.shape(input_tensor)\n",
    "    num_locations = tf.cast(input_shape[1] * input_shape[2], tf.float32)\n",
    "    return result / (num_locations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract Style and Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleContentModel(tf.keras.models.Model):\n",
    "    def __init__(self, style_layers, content_layers):\n",
    "        super(StyleContentModel, self).__init__()\n",
    "        self.vgg = vgg_layers(style_layers + content_layers)\n",
    "        self.style_layers = style_layers\n",
    "        self.content_layers = content_layers\n",
    "        self.num_style_layers = len(style_layers)\n",
    "        self.vgg.trainable = False\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"Expects float input in [0,1]\"\n",
    "        inputs = inputs * 255.0\n",
    "        preprocessed_image = applications.vgg19.preprocess_input(inputs)\n",
    "        outputs = self.vgg(preprocessed_image)\n",
    "        \n",
    "        style_outputs, content_outputs = (\n",
    "            outputs[: self.num_style_layers],\n",
    "            outputs[self.num_style_layers :],\n",
    "        )\n",
    "        style_outputs = [gram_matrix(output) for output in style_outputs]\n",
    "        content_dict = {\n",
    "            content_name: value\n",
    "            for content_name, value in zip(self.content_layers, content_outputs)\n",
    "        }\n",
    "        style_dict = {\n",
    "            style_name: value\n",
    "            for style_name, value in zip(self.style_layers, style_outputs)\n",
    "        }\n",
    "\n",
    "        return {\"content\": content_dict, \"style\": style_dict}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = StyleContentModel(style_layers, content_layers)\n",
    "\n",
    "results = extractor(tf.constant(content_image))\n",
    "\n",
    "print(\"Styles:\")\n",
    "for name, output in sorted(results[\"style\"].items()):\n",
    "    print(\"  \", name)\n",
    "    print(\"    shape: \", output.numpy().shape)\n",
    "    print(\"    min: \", output.numpy().min())\n",
    "    print(\"    max: \", output.numpy().max())\n",
    "    print(\"    mean: \", output.numpy().mean())\n",
    "    print()\n",
    "\n",
    "print(\"Contents:\")\n",
    "for name, output in sorted(results[\"content\"].items()):\n",
    "    print(\"  \", name)\n",
    "    print(\"    shape: \", output.numpy().shape)\n",
    "    print(\"    min: \", output.numpy().min())\n",
    "    print(\"    max: \", output.numpy().max())\n",
    "    print(\"    mean: \", output.numpy().mean())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_targets = extractor(style_image)['style']\n",
    "content_targets = extractor(content_image)['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.Variable(content_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_0_1(image):\n",
    "    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n",
    "style_weight = 1e-2\n",
    "content_weight = 1e4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_content_loss(outputs):\n",
    "    style_outputs = outputs[\"style\"]\n",
    "    content_outputs = outputs[\"content\"]\n",
    "    style_loss = tf.add_n(\n",
    "        [\n",
    "            tf.reduce_mean((style_outputs[name] - style_targets[name]) ** 2)\n",
    "            for name in style_outputs.keys()\n",
    "        ]\n",
    "    )\n",
    "    style_loss *= style_weight / num_style_layers\n",
    "\n",
    "    content_loss = tf.add_n(\n",
    "        [\n",
    "            tf.reduce_mean((content_outputs[name] - content_targets[name]) ** 2)\n",
    "            for name in content_outputs.keys()\n",
    "        ]\n",
    "    )\n",
    "    content_loss *= content_weight / num_content_layers\n",
    "    loss = style_loss + content_loss\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = extractor(image)\n",
    "        loss = style_content_loss(outputs)\n",
    "\n",
    "    grad = tape.gradient(loss, image)\n",
    "    opt.apply_gradients([(grad, image)])\n",
    "    image.assign(clip_0_1(image))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    train_step(image)\n",
    "\n",
    "tensor_to_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "epochs = 15\n",
    "steps_per_epoch = 100\n",
    "\n",
    "step = 0\n",
    "for n in range(epochs):\n",
    "    for m in range(steps_per_epoch):\n",
    "        step += 1\n",
    "        train_step(image)\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(tensor_to_image(image))\n",
    "    print(f\"Train step : {step}\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Total time: {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_pass_x_y(image):\n",
    "    x_var = image[:, :, 1:, :] - image[:, :, :-1, :]\n",
    "    y_var = image[:, 1:, :, :] - image[:, :-1, :, :]\n",
    "\n",
    "    return x_var, y_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_deltas, y_deltas = high_pass_x_y(content_image)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "imshow(clip_0_1(2*y_deltas+0.5), \"Horizontal Deltas: Original\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "imshow(clip_0_1(2*x_deltas+0.5), \"Vertical Deltas: Original\")\n",
    "\n",
    "x_deltas, y_deltas = high_pass_x_y(image)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "imshow(clip_0_1(2*y_deltas+0.5), \"Horizontal Deltas: Styled\")\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "imshow(clip_0_1(2*x_deltas+0.5), \"Vertical Deltas: Styled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "sobel = tf.image.sobel_edges(content_image)\n",
    "plt.subplot(1, 2, 1)\n",
    "imshow(clip_0_1(sobel[..., 0]/4+0.5), \"Horizontal Sobel-edges\")\n",
    "plt.subplot(1, 2, 2)\n",
    "imshow(clip_0_1(sobel[..., 1]/4+0.5), \"Vertical Sobel-edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.image.total_variation(image).numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Re-run the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_variation_weight = 30\n",
    "\n",
    "@tf.function()\n",
    "def train_step(image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = extractor(image)\n",
    "        loss = style_content_loss(outputs)\n",
    "        loss += total_variation_weight * tf.image.total_variation(image)\n",
    "    \n",
    "    grad = tape.gradient(loss, image)\n",
    "    opt.apply_gradients([(grad, image)])\n",
    "    image.assign(clip_0_1(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.Adam(learning_rate=2e-2, beta_1=0.99, epsilon=1e-1)\n",
    "image = tf.Variable(content_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "for n in range(epochs):\n",
    "    for m in range(steps_per_epoch):\n",
    "        step += 1\n",
    "        train_step(image)\n",
    "        print(f\".\", end=\"\", flush=True)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(tensor_to_image(image))\n",
    "    print(f\"Train step: {step}\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Total Time: {end-start:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../../reports/figures/chapter_14/stylized-image.png'\n",
    "tensor_to_image(image).save(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b20fb8559386147dc77cba9f7518a8c175fc982b8c10bebe842520e16b3ac5d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
