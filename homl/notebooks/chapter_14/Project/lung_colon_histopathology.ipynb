{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lung and Colon Histopathology Classification\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmenendezg/Developer/Books/.venv/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/Users/mmenendezg/Developer/Books/.venv/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: [\"[Errno 2] The file to load file system plugin from does not exist.: '/Users/mmenendezg/Developer/Books/.venv/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so'\"]\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/Users/mmenendezg/Developer/Books/.venv/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/Users/mmenendezg/Developer/Books/.venv/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: [\"dlopen(/Users/mmenendezg/Developer/Books/.venv/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so, 0x0006): tried: '/Users/mmenendezg/Developer/Books/.venv/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/mmenendezg/Developer/Books/.venv/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so' (no such file), '/Users/mmenendezg/Developer/Books/.venv/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so' (no such file)\"]\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from contextlib import ExitStack\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import (\n",
    "    Sequential,\n",
    "    layers,\n",
    "    optimizers,\n",
    "    regularizers,\n",
    "    callbacks,\n",
    "    metrics,\n",
    "    losses,\n",
    "    activations,\n",
    ")\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.train import Example, Feature, Features, BytesList, Int64List, FloatList\n",
    "\n",
    "# Local Libraries\n",
    "import ml_datasets\n",
    "import ml_learning_rate\n",
    "import ml_plotting\n",
    "import ml_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "SHUFFLE_BUFFER = 1000\n",
    "DATASET_PATH = tf.io.gfile.join(os.environ[\"DATA_PATH\"], \"lung_colon_histopathology\")\n",
    "RAW_DATASET_PATH = tf.io.gfile.join(DATASET_PATH, \"raw_data\")\n",
    "TFRECORDS_PATH = tf.io.gfile.join(DATASET_PATH, \"tfrecord_data\")\n",
    "MODEL_PATH = os.path.join(\"..\", \"..\", \"models\", \"chapter_14\")\n",
    "SEED = 1992\n",
    "\n",
    "# Determine the organ the sample is from\n",
    "ORGAN_CLASSES = {\n",
    "    0: \"Colon\",\n",
    "    1: \"Lung\",\n",
    "}\n",
    "\n",
    "# Determine if the tissue is benign, adenocarcinoma or scamous cell carcinoma\n",
    "TYPE_TISSUE = {\n",
    "    0: \"Benign\",\n",
    "    1: \"Adenocarcinoma\",\n",
    "    2: \"Squamous Cell Carcinoma\",\n",
    "}\n",
    "\n",
    "IMG_SIZE = (768, 768)\n",
    "IMG_CHANNELS = 3\n",
    "NEW_IMG_SIZE = [512, 512, 3]\n",
    "\n",
    "# Model\n",
    "MODEL_URL = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/5\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "\n",
    "- The architecture used is ResNet-18, as used and suggested in [this article](https://arxiv.org/pdf/1901.11489v1.pdf).\n",
    "- They used learning rate with decay; metrics were accuracy, precision, recall and f1; and the loss function was multi-class crosstentropy\n",
    "- They implemented data augmentation to increase the size of the datasets. This will be dismissed in this notebook since the data has been previously augmented.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Pipeline\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset used is [Lung and Colon Cancer Histopathological Images | Kaggle](https://www.kaggle.com/datasets/andrewmvd/lung-and-colon-cancer-histopathological-images/code)\n",
    "- Images are in folders, each folder a class\n",
    "- Images size is `[256, 256, 3]`\n",
    "- Dataset has been augmented using [`Augmentor`](https://augmentor.readthedocs.io/en/stable/) package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_from_value(dictionary, value):\n",
    "    return [key for key, v in dictionary.items() if v == value][0]\n",
    "\n",
    "\n",
    "def get_folders_images(filepath):\n",
    "    name_folders = tf.io.gfile.listdir(filepath)\n",
    "    path_folders = [tf.io.gfile.join(filepath, folder) for folder in name_folders]\n",
    "\n",
    "    list_folders = {folder: path for folder, path in zip(name_folders, path_folders)}\n",
    "    return list_folders\n",
    "\n",
    "\n",
    "def create_example(image, organ_label, tissue_label):\n",
    "    image_data = tf.io.serialize_tensor(image)\n",
    "    feature = {\n",
    "        \"image\": Feature(bytes_list=BytesList(value=[image_data.numpy()])),\n",
    "        \"organ_label\": Feature(int64_list=Int64List(value=[organ_label.numpy()])),\n",
    "        \"tissue_label\": Feature(int64_list=Int64List(value=[tissue_label.numpy()])),\n",
    "    }\n",
    "\n",
    "    return Example(features=Features(feature=feature))\n",
    "\n",
    "\n",
    "def save_protobufs(dataset, type_set=\"train\", n_shards=10):\n",
    "\n",
    "    set_folder = tf.io.gfile.join(TFRECORDS_PATH, type_set)\n",
    "\n",
    "    tf.io.gfile.makedirs(set_folder)\n",
    "    file_paths = [tf.io.gfile.join(set_folder, filepath) for filepath in files]\n",
    "\n",
    "    if type_set == \"train\":\n",
    "        dataset.shuffle(SHUFFLE_BUFFER)\n",
    "    files = [\n",
    "        f\"{type_set}.tfrecord-{shard.numpy() + 1:02d}-of-{n_shards:02d}\"\n",
    "        for shard in tf.range(n_shards)\n",
    "    ]\n",
    "\n",
    "    with ExitStack() as stack:\n",
    "        writers = [\n",
    "            stack.enter_context(tf.io.TFRecordWriter(file)) for file in file_paths\n",
    "        ]\n",
    "        for index, (image, organ_label, tissue_label) in dataset.enumerate():\n",
    "            shard = index % n_shards\n",
    "            example = create_example(image, organ_label, tissue_label)\n",
    "            writers[shard].write(example.SerializeToString())\n",
    "    return file_paths\n",
    "\n",
    "\n",
    "def get_folders_tfrecords(type_set=\"train\"):\n",
    "    folder = tf.io.gfile.join(TFRECORDS_PATH, type_set)\n",
    "    files = tf.io.gfile.listdir(folder)\n",
    "    list_files = [tf.io.gfile.join(folder, filepath) for filepath in files]\n",
    "    return list_files\n",
    "\n",
    "\n",
    "def save_images_protobufs():\n",
    "\n",
    "    # Verify if the folder for the TFRecords exist to process the data\n",
    "    if not tf.io.gfile.exists(TFRECORDS_PATH):\n",
    "\n",
    "        tf.io.gfile.makedirs(TFRECORDS_PATH)\n",
    "\n",
    "        list_folders = get_folders_images(RAW_DATASET_PATH)\n",
    "\n",
    "        # Create the dataset per folder, split the dataset into train, valid and test sets\n",
    "        # Add the organ label\n",
    "        # New dimension is (image, organ_label, tissue_label)\n",
    "        # organ_label -> 0: Colon, 1: Lung\n",
    "        # tissue_label -> 0: Benign, 1: Adenocarcinoma, 2: Squamous Cell Carcinoma\n",
    "        colon_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            list_folders[\"colon_images\"], batch_size=None, image_size=IMG_SIZE\n",
    "        )\n",
    "        train_colon, valid_colon, test_colon = ml_functions.balanced_split(\n",
    "            colon_dataset\n",
    "        )\n",
    "        train_colon = train_colon.map(\n",
    "            lambda image, label: (image, 0, label), num_parallel_calls=AUTOTUNE\n",
    "        )\n",
    "        valid_colon = valid_colon.map(\n",
    "            lambda image, label: (image, 0, label), num_parallel_calls=AUTOTUNE\n",
    "        )\n",
    "        test_colon = test_colon.map(\n",
    "            lambda image, label: (image, 0, label), num_parallel_calls=AUTOTUNE\n",
    "        )\n",
    "\n",
    "        lung_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            list_folders[\"lung_images\"], batch_size=None, image_size=IMG_SIZE\n",
    "        )\n",
    "        train_lung, valid_lung, test_lung = ml_functions.balanced_split(lung_dataset)\n",
    "        train_lung = train_lung.map(\n",
    "            lambda image, label: (image, 1, label), num_parallel_calls=AUTOTUNE\n",
    "        )\n",
    "        valid_lung = valid_lung.map(\n",
    "            lambda image, label: (image, 1, label), num_parallel_calls=AUTOTUNE\n",
    "        )\n",
    "        test_lung = test_lung.map(\n",
    "            lambda image, label: (image, 1, label), num_parallel_calls=AUTOTUNE\n",
    "        )\n",
    "\n",
    "        # Combines the datasets into a single one\n",
    "        train_set = train_colon.concatenate(train_lung)\n",
    "        valid_set = valid_colon.concatenate(valid_lung)\n",
    "        test_set = test_colon.concatenate(test_lung)\n",
    "\n",
    "        train_paths = save_protobufs(train_set, \"train\")\n",
    "        valid_paths = save_protobufs(valid_set, \"valid\")\n",
    "        test_paths = save_protobufs(test_set, \"test\")\n",
    "    else:\n",
    "        train_paths = get_folders_tfrecords(\"train\")\n",
    "        valid_paths = get_folders_tfrecords(\"valid\")\n",
    "        test_paths = get_folders_tfrecords(\"test\")\n",
    "\n",
    "    return train_paths, valid_paths, test_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files, valid_files, test_files = save_images_protobufs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_record_multilabel(tfrecord):\n",
    "    feature_descriptions = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "        \"organ_label\": tf.io.FixedLenFeature([], tf.int64, default_value=-1),\n",
    "        \"tissue_label\": tf.io.FixedLenFeature([], tf.int64, default_value=-1),\n",
    "    }\n",
    "\n",
    "    example = tf.io.parse_single_example(tfrecord, feature_descriptions)\n",
    "    image = tf.io.parse_tensor(example[\"image\"], out_type=tf.float32)\n",
    "    image = tf.reshape(image, shape=[IMG_SIZE[0], IMG_SIZE[1], IMG_CHANNELS])\n",
    "    image = tf.image.resize(image, size=[NEW_IMG_SIZE[0], NEW_IMG_SIZE[1]])\n",
    "    return image, (example[\"organ_label\"], example[\"tissue_label\"])\n",
    "\n",
    "\n",
    "def get_record(tfrecord):\n",
    "    feature_descriptions = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "        \"organ_label\": tf.io.FixedLenFeature([], tf.int64, default_value=-1),\n",
    "        \"tissue_label\": tf.io.FixedLenFeature([], tf.int64, default_value=-1),\n",
    "    }\n",
    "\n",
    "    example = tf.io.parse_single_example(tfrecord, feature_descriptions)\n",
    "    image = tf.io.parse_tensor(example[\"image\"], out_type=tf.float32)\n",
    "    image = tf.reshape(image, shape=[IMG_SIZE[0], IMG_SIZE[1], IMG_CHANNELS])\n",
    "    image = tf.image.resize(image, size=[NEW_IMG_SIZE[0], NEW_IMG_SIZE[1]])\n",
    "    return image, example[\"tissue_label\"]\n",
    "\n",
    "\n",
    "def get_dataset(file_paths, cache=True, shuffle_buffer=None, multi_label=False):\n",
    "    dataset = tf.data.TFRecordDataset(file_paths, num_parallel_reads=AUTOTUNE)\n",
    "\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    if shuffle_buffer:\n",
    "        dataset = dataset.shuffle(shuffle_buffer)\n",
    "\n",
    "    if multi_label:\n",
    "        dataset = (\n",
    "            dataset.map(get_record_multilabel, num_parallel_calls=AUTOTUNE)\n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(AUTOTUNE)\n",
    "        )\n",
    "    else:\n",
    "        dataset = (\n",
    "            dataset.map(get_record, num_parallel_calls=AUTOTUNE)\n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(AUTOTUNE)\n",
    "        )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_set = get_dataset(train_files, shuffle_buffer=SHUFFLE_BUFFER)\n",
    "valid_set = get_dataset(valid_files)\n",
    "test_set = get_dataset(test_files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    train_set,\n",
    "    valid_set,\n",
    "    test_set,\n",
    "    epochs,\n",
    "    learning_rate,\n",
    "    trainable=False,\n",
    "    lr_function=None,\n",
    "):\n",
    "    # Normalization of the data\n",
    "    def normalize(image, label):\n",
    "        norm_image = image / 255.0\n",
    "        return (norm_image, label)\n",
    "    \n",
    "    train_set_normalized = train_set.map(normalize, num_parallel_calls=AUTOTUNE)\n",
    "    valid_set_normalized = valid_set.map(normalize, num_parallel_calls=AUTOTUNE)\n",
    "    test_set_normalized = test_set.map(normalize, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    # Load the ResNet model\n",
    "    base_model = hub.KerasLayer(\n",
    "        MODEL_URL,\n",
    "        trainable=trainable,\n",
    "    )\n",
    "\n",
    "    model = Sequential([base_model, layers.Dense(3, activation=\"softmax\")])\n",
    "    model.build([None, NEW_IMG_SIZE[0], NEW_IMG_SIZE[1], NEW_IMG_SIZE[2]])\n",
    "\n",
    "    # Compilation of the model\n",
    "    optimizer_ = optimizers.Adam(learning_rate=learning_rate)\n",
    "    metrics_ = [\"accuracy\"]\n",
    "    loss_ = \"sparse_categorical_crossentropy\"\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer_,\n",
    "        metrics=metrics_,\n",
    "        loss=loss_\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    exponential_decay_fn = ml_learning_rate.exponential_decay_with_warmup(\n",
    "        lr_start=learning_rate,\n",
    "        lr_max=learning_rate * 10,\n",
    "        lr_min=learning_rate / 10,\n",
    "    )\n",
    "    lr_scheduler_cb = callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "    \n",
    "    folder_logs = tf.io.gfile.join(\n",
    "        \"..\", \"..\", \"..\", \"reports\", \"logs\", \"chapter_14\", \"lung_colon_histopathology\"\n",
    "    )\n",
    "    logdir = ml_functions.get_logdir(path_folder=folder_logs)\n",
    "    tensorboard_cb = callbacks.TensorBoard(log_dir=logdir)\n",
    "    \n",
    "    model_path = tf.io.gfile.join(MODEL_PATH, \"lung_colon_histopathology.h5\")\n",
    "    model_checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "        filepath=model_path, save_best_only=True\n",
    "    )\n",
    "    \n",
    "    early_stopping_cb = callbacks.EarlyStopping(patience=5)\n",
    "    \n",
    "    callbacks_list = [\n",
    "        lr_scheduler_cb,\n",
    "        tensorboard_cb,\n",
    "        model_checkpoint_cb,\n",
    "        early_stopping_cb\n",
    "    ]\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        train_set_normalized,\n",
    "        validation_data=valid_set_normalized,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks_list,\n",
    "    )\n",
    "    \n",
    "    evaluation = model.evaluate(test_set, verbose=0)\n",
    "    \n",
    "    return model, evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n",
      "Epoch 1/5\n",
      "   2005/Unknown - 304s 149ms/step - loss: 0.2739 - accuracy: 0.9074"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "model, evaluation = train_model(\n",
    "    train_set,\n",
    "    valid_set,\n",
    "    test_set,\n",
    "    epochs=5,\n",
    "    learning_rate=1e-4,\n",
    "    trainable=False,\n",
    "    lr_function=None\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Training\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Fine-Tuning\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b20fb8559386147dc77cba9f7518a8c175fc982b8c10bebe842520e16b3ac5d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
