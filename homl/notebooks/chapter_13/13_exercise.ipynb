{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Loading and Preprocessing Data with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from contextlib import ExitStack\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras, data\n",
    "from keras import (\n",
    "    callbacks,\n",
    "    layers,\n",
    "    Sequential,\n",
    "    optimizers,\n",
    ")\n",
    "from tensorflow.train import Example, Feature, Features, BytesList, Int64List, FloatList\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = os.path.join('/', 'Users', 'mmenendezg', 'Developer', 'Datasets', 'fashion_mnist')\n",
    "SHUFFLE_BUFFER = 60000\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 4096\n",
    "\n",
    "os.makedirs(DATASET_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_dir():\n",
    "    datetime_dir = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    log_dir = os.path.join('..', \"..\", \"reports\", \"logs\", datetime_dir)\n",
    "    return log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Load the Fashion MNIST dataset (introduced in Chapter 10); split it into a training set, a validation set, and a test set; shuffle the training set; and save each dataset to multiple TFRecord files. Each record should be a serialized Example protobuf with two features: the serialized image (use tf.io.serialize_tensor() to serialize each image), and the label.⁠ Then use tf.data to create an efficient dataset for each set. Finally, use a Keras model to train these datasets, including a preprocessing layer to standardize each input feature. Try to make the input pipeline as efficient as possible, using TensorBoard to visualize profiling data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1992)\n",
    "\n",
    "train_set, valid_set, test_set = tfds.load(\n",
    "    name=\"fashion_mnist\",\n",
    "    data_dir=DATASET_PATH,\n",
    "    split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\n",
    "    as_supervised=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_example(image, label):\n",
    "    \"\"\"This function creates an example protobufs using the image \n",
    "    and label of the `fashion_mnist` dataset\n",
    "\n",
    "    Args:\n",
    "        image (tf.Tensor): Array containing the values of the image\n",
    "        label (int): Value of the class of the image\n",
    "\n",
    "    Returns:\n",
    "        binary protobuf: Protobuf containing the image and the label \n",
    "    \"\"\"\n",
    "    image_data = tf.io.serialize_tensor(image)\n",
    "    label_data = tf.io.serialize_tensor(label)\n",
    "    feature = {\n",
    "        \"image\": Feature(bytes_list=BytesList(value=[image_data.numpy()])),\n",
    "        \"label\": Feature(int64_list=Int64List(value=[label.numpy()])),\n",
    "    }\n",
    "    \n",
    "    return Example(\n",
    "        features=Features(\n",
    "            feature=feature\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def save_protobufs(dataset, set_type=\"train\", n_shards=10):\n",
    "    \"\"\"This function divides the set into multiple mini sets,\n",
    "    and saves the images as protobufs in several files\n",
    "\n",
    "    Args:\n",
    "        set (tf.data.Dataset): set of tuples (images, target) of the fashion_mnist dataset\n",
    "    \"\"\"\n",
    "    if set_type == \"train\":\n",
    "        dataset.shuffle(SHUFFLE_BUFFER)\n",
    "    files = [\n",
    "        f\"{set_type}.tfrecord-{shard:05d}-of-{n_shards:05d}\"\n",
    "        for shard in range(n_shards)\n",
    "    ]\n",
    "    file_paths = [os.path.join(DATASET_PATH, file_path) for file_path in files]\n",
    "    with ExitStack() as stack:\n",
    "        writers = [stack.enter_context(tf.io.TFRecordWriter(file)) for file in file_paths]\n",
    "        for index, (image, label) in dataset.enumerate():\n",
    "            shard = index % n_shards\n",
    "            example = create_example(image, label)\n",
    "            writers[shard].write(example.SerializeToString())\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = save_protobufs(train_set, 'train')\n",
    "valid_files = save_protobufs(valid_set, 'valid')\n",
    "test_files = save_protobufs(test_set, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_record(tfrecord):\n",
    "    feature_descriptions = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64, default_value=-1),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(tfrecord, feature_descriptions)\n",
    "    image = tf.io.parse_tensor(example[\"image\"], out_type=tf.uint8)\n",
    "    image = tf.reshape(image, shape=[28, 28])\n",
    "    return image, example[\"label\"]\n",
    "\n",
    "\n",
    "def mnist_dataset(file_paths, cache=True, shuffle_buffer_size=None):\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(file_paths, num_parallel_reads=AUTOTUNE)\n",
    "\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    if shuffle_buffer_size:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = (\n",
    "        dataset.map(get_record, num_parallel_calls=AUTOTUNE)\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(AUTOTUNE)\n",
    "    )\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = mnist_dataset(train_files, shuffle_buffer_size=SHUFFLE_BUFFER)\n",
    "valid_set = mnist_dataset(valid_files)\n",
    "test_set = mnist_dataset(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for x, y in train_set.take(1):\n",
    "    for i in range(5):\n",
    "        plt.subplot(1, 5, i + 1)\n",
    "        plt.imshow(x[i].numpy(), cmap=\"binary\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(str(y[i].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "14/14 [==============================] - 1s 70ms/step - loss: 1.3607 - accuracy: 0.5495 - val_loss: 0.7148 - val_accuracy: 0.7525\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 1s 61ms/step - loss: 0.6364 - accuracy: 0.7777 - val_loss: 0.5667 - val_accuracy: 0.8005\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 1s 60ms/step - loss: 0.5246 - accuracy: 0.8164 - val_loss: 0.4895 - val_accuracy: 0.8243\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 1s 61ms/step - loss: 0.4756 - accuracy: 0.8304 - val_loss: 0.4767 - val_accuracy: 0.8255\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 1s 58ms/step - loss: 0.4481 - accuracy: 0.8404 - val_loss: 0.4319 - val_accuracy: 0.8435\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 1s 61ms/step - loss: 0.4242 - accuracy: 0.8496 - val_loss: 0.4469 - val_accuracy: 0.8388\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 1s 59ms/step - loss: 0.4101 - accuracy: 0.8542 - val_loss: 0.4021 - val_accuracy: 0.8563\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 1s 58ms/step - loss: 0.3967 - accuracy: 0.8593 - val_loss: 0.3884 - val_accuracy: 0.8598\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 1s 58ms/step - loss: 0.3844 - accuracy: 0.8639 - val_loss: 0.4009 - val_accuracy: 0.8528\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 1s 60ms/step - loss: 0.3790 - accuracy: 0.8651 - val_loss: 0.3792 - val_accuracy: 0.8628\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 1s 61ms/step - loss: 0.3592 - accuracy: 0.8729 - val_loss: 0.3714 - val_accuracy: 0.8635\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 1s 63ms/step - loss: 0.3584 - accuracy: 0.8719 - val_loss: 0.3731 - val_accuracy: 0.8623\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 1s 60ms/step - loss: 0.3536 - accuracy: 0.8747 - val_loss: 0.3963 - val_accuracy: 0.8535\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 1s 60ms/step - loss: 0.3527 - accuracy: 0.8731 - val_loss: 0.3581 - val_accuracy: 0.8710\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 1s 59ms/step - loss: 0.3373 - accuracy: 0.8801 - val_loss: 0.3559 - val_accuracy: 0.8665\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 1s 58ms/step - loss: 0.3340 - accuracy: 0.8808 - val_loss: 0.3691 - val_accuracy: 0.8638\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 1s 57ms/step - loss: 0.3292 - accuracy: 0.8832 - val_loss: 0.3562 - val_accuracy: 0.8698\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 1s 58ms/step - loss: 0.3205 - accuracy: 0.8859 - val_loss: 0.3483 - val_accuracy: 0.8727\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 1s 58ms/step - loss: 0.3127 - accuracy: 0.8895 - val_loss: 0.3462 - val_accuracy: 0.8738\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 1s 58ms/step - loss: 0.3113 - accuracy: 0.8903 - val_loss: 0.3483 - val_accuracy: 0.8727\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 1s 58ms/step - loss: 0.3114 - accuracy: 0.8886 - val_loss: 0.3501 - val_accuracy: 0.8728\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 1s 59ms/step - loss: 0.3095 - accuracy: 0.8891 - val_loss: 0.3407 - val_accuracy: 0.8767\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 1s 59ms/step - loss: 0.2974 - accuracy: 0.8946 - val_loss: 0.3362 - val_accuracy: 0.8767\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 1s 59ms/step - loss: 0.2987 - accuracy: 0.8926 - val_loss: 0.3319 - val_accuracy: 0.8798\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 1s 60ms/step - loss: 0.2884 - accuracy: 0.8971 - val_loss: 0.3262 - val_accuracy: 0.8808\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 1s 62ms/step - loss: 0.2885 - accuracy: 0.8975 - val_loss: 0.3456 - val_accuracy: 0.8748\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 1s 64ms/step - loss: 0.2861 - accuracy: 0.8981 - val_loss: 0.3419 - val_accuracy: 0.8742\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 1s 61ms/step - loss: 0.2783 - accuracy: 0.9008 - val_loss: 0.3302 - val_accuracy: 0.8812\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 1s 59ms/step - loss: 0.2829 - accuracy: 0.8984 - val_loss: 0.3136 - val_accuracy: 0.8852\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 1s 59ms/step - loss: 0.2757 - accuracy: 0.9019 - val_loss: 0.3150 - val_accuracy: 0.8885\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 1s 58ms/step - loss: 0.2742 - accuracy: 0.9023 - val_loss: 0.3154 - val_accuracy: 0.8842\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 1s 59ms/step - loss: 0.2681 - accuracy: 0.9046 - val_loss: 0.3230 - val_accuracy: 0.8793\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 1s 58ms/step - loss: 0.2719 - accuracy: 0.9035 - val_loss: 0.3130 - val_accuracy: 0.8855\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 1s 59ms/step - loss: 0.2596 - accuracy: 0.9080 - val_loss: 0.3140 - val_accuracy: 0.8850\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 1s 57ms/step - loss: 0.2678 - accuracy: 0.9042 - val_loss: 0.3312 - val_accuracy: 0.8767\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 1s 59ms/step - loss: 0.2591 - accuracy: 0.9073 - val_loss: 0.3349 - val_accuracy: 0.8775\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 1s 61ms/step - loss: 0.2641 - accuracy: 0.9046 - val_loss: 0.3341 - val_accuracy: 0.8773\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 1s 58ms/step - loss: 0.2563 - accuracy: 0.9083 - val_loss: 0.3361 - val_accuracy: 0.8742\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 1s 59ms/step - loss: 0.2482 - accuracy: 0.9128 - val_loss: 0.3075 - val_accuracy: 0.8888\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 1s 58ms/step - loss: 0.2520 - accuracy: 0.9097 - val_loss: 0.3088 - val_accuracy: 0.8863\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 1s 58ms/step - loss: 0.2421 - accuracy: 0.9150 - val_loss: 0.3302 - val_accuracy: 0.8782\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 1s 59ms/step - loss: 0.2420 - accuracy: 0.9150 - val_loss: 0.3033 - val_accuracy: 0.8865\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 1s 58ms/step - loss: 0.2382 - accuracy: 0.9164 - val_loss: 0.3034 - val_accuracy: 0.8917\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 1s 60ms/step - loss: 0.2428 - accuracy: 0.9136 - val_loss: 0.3344 - val_accuracy: 0.8775\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 1s 60ms/step - loss: 0.2349 - accuracy: 0.9181 - val_loss: 0.2993 - val_accuracy: 0.8912\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 1s 60ms/step - loss: 0.2308 - accuracy: 0.9192 - val_loss: 0.2950 - val_accuracy: 0.8952\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 1s 58ms/step - loss: 0.2324 - accuracy: 0.9179 - val_loss: 0.3306 - val_accuracy: 0.8788\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 1s 57ms/step - loss: 0.2566 - accuracy: 0.9061 - val_loss: 0.3242 - val_accuracy: 0.8805\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 1s 58ms/step - loss: 0.2261 - accuracy: 0.9212 - val_loss: 0.3008 - val_accuracy: 0.8908\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 1s 58ms/step - loss: 0.2267 - accuracy: 0.9198 - val_loss: 0.3017 - val_accuracy: 0.8913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bd24cb20>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(1992)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Normalization of the data\n",
    "sample_images = train_set.take(250).map(lambda image, label: image)\n",
    "normalizer = layers.Normalization(input_shape=[28, 28])\n",
    "normalizer.adapt(sample_images)\n",
    "\n",
    "# Callbacks\n",
    "tensorboard_cb = callbacks.TensorBoard(\n",
    "    log_dir=get_log_dir(), profile_batch=10, histogram_freq=1\n",
    ")\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        normalizer,\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(100, activation=\"relu\"),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=\"nadam\"\n",
    ")\n",
    "\n",
    "model.fit(train_set, epochs=50, validation_data=valid_set, callbacks=[tensorboard_cb])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. In this exercise you will download a dataset, split it, create a tf.data.Dataset to load it and preprocess it efficiently, then build and train a binary classification model containing an Embedding layer:\n",
    "\n",
    "#### - Download the Large Movie Review Dataset, which contains 50,000 movie reviews from the Internet Movie Database (IMDb). The data is organized in two directories, train and test, each containing a pos subdirectory with 12,500 positive reviews and a neg subdirectory with 12,500 negative reviews. Each review is stored in a separate text file. There are other files and folders (including preprocessed bag-of-words versions), but we will ignore them in this exercise.\n",
    "\n",
    "#### - Split the test set into a validation set (15,000) and a test set (10,000).\n",
    "\n",
    "#### - Use tf.data to create an efficient dataset for each set.\n",
    "\n",
    "#### - Create a binary classification model, using a TextVectorization layer to preprocess each review.\n",
    "\n",
    "#### - Add an Embedding layer and compute the mean embedding for each review, multiplied by the square root of the number of words (see Chapter 16). This rescaled mean embedding can then be passed to the rest of your model.\n",
    "\n",
    "#### - Train the model and see what accuracy you get. Try to optimize your pipelines to make training as fast as possible.\n",
    "\n",
    "#### - Use TFDS to load the same dataset more easily: tfds.load(\"imdb_reviews\")."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b20fb8559386147dc77cba9f7518a8c175fc982b8c10bebe842520e16b3ac5d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
