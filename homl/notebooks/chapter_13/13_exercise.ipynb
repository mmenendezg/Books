{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Loading and Preprocessing Data with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmenendezg/Developer/Books/.venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from contextlib import ExitStack\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras, data\n",
    "from keras import (\n",
    "    callbacks,\n",
    "    layers,\n",
    "    losses,\n",
    "    metrics,\n",
    "    optimizers,\n",
    "    Sequential,\n",
    ")\n",
    "from tensorflow.train import Example, Feature, Features, BytesList, Int64List, FloatList\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = os.path.join('/', 'Users', 'mmenendezg', 'Developer', 'Datasets', 'fashion_mnist')\n",
    "IMDB_PATH = os.path.join('/', 'Users', 'mmenendezg', 'Developer', 'Datasets', 'imdb_reviews')\n",
    "SAVE_MODEL_PATH = os.path.join(\"..\", \"..\", \"models\", \"chapter_13\")\n",
    "SHUFFLE_BUFFER = 60000\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 4096\n",
    "\n",
    "os.makedirs(DATASET_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_dir():\n",
    "    datetime_dir = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    log_dir = os.path.join('..', \"..\", \"reports\", \"logs\", datetime_dir)\n",
    "    return log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Load the Fashion MNIST dataset (introduced in Chapter 10); split it into a training set, a validation set, and a test set; shuffle the training set; and save each dataset to multiple TFRecord files. Each record should be a serialized Example protobuf with two features: the serialized image (use tf.io.serialize_tensor() to serialize each image), and the label.⁠ Then use tf.data to create an efficient dataset for each set. Finally, use a Keras model to train these datasets, including a preprocessing layer to standardize each input feature. Try to make the input pipeline as efficient as possible, using TensorBoard to visualize profiling data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1992)\n",
    "\n",
    "train_set, valid_set, test_set = tfds.load(\n",
    "    name=\"fashion_mnist\",\n",
    "    data_dir=DATASET_PATH,\n",
    "    split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\n",
    "    as_supervised=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_example(image, label):\n",
    "    \"\"\"This function creates an example protobufs using the image \n",
    "    and label of the `fashion_mnist` dataset\n",
    "\n",
    "    Args:\n",
    "        image (tf.Tensor): Array containing the values of the image\n",
    "        label (int): Value of the class of the image\n",
    "\n",
    "    Returns:\n",
    "        binary protobuf: Protobuf containing the image and the label \n",
    "    \"\"\"\n",
    "    image_data = tf.io.serialize_tensor(image)\n",
    "    label_data = tf.io.serialize_tensor(label)\n",
    "    feature = {\n",
    "        \"image\": Feature(bytes_list=BytesList(value=[image_data.numpy()])),\n",
    "        \"label\": Feature(int64_list=Int64List(value=[label.numpy()])),\n",
    "    }\n",
    "    \n",
    "    return Example(\n",
    "        features=Features(\n",
    "            feature=feature\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def save_protobufs(dataset, set_type=\"train\", n_shards=10):\n",
    "    \"\"\"This function divides the set into multiple mini sets,\n",
    "    and saves the images as protobufs in several files\n",
    "\n",
    "    Args:\n",
    "        set (tf.data.Dataset): set of tuples (images, target) of the fashion_mnist dataset\n",
    "    \"\"\"\n",
    "    if set_type == \"train\":\n",
    "        dataset.shuffle(SHUFFLE_BUFFER)\n",
    "    files = [\n",
    "        f\"{set_type}.tfrecord-{shard:05d}-of-{n_shards:05d}\"\n",
    "        for shard in range(n_shards)\n",
    "    ]\n",
    "    file_paths = [os.path.join(DATASET_PATH, file_path) for file_path in files]\n",
    "    with ExitStack() as stack:\n",
    "        writers = [stack.enter_context(tf.io.TFRecordWriter(file)) for file in file_paths]\n",
    "        for index, (image, label) in dataset.enumerate():\n",
    "            shard = index % n_shards\n",
    "            example = create_example(image, label)\n",
    "            writers[shard].write(example.SerializeToString())\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = save_protobufs(train_set, \"train\", n_shards=2)\n",
    "valid_files = save_protobufs(valid_set, \"valid\", n_shards=2)\n",
    "test_files = save_protobufs(test_set, \"test\", n_shards=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_record(tfrecord):\n",
    "    feature_descriptions = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64, default_value=-1),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(tfrecord, feature_descriptions)\n",
    "    image = tf.io.parse_tensor(example[\"image\"], out_type=tf.uint8)\n",
    "    image = tf.reshape(image, shape=[28, 28])\n",
    "    return image, example[\"label\"]\n",
    "\n",
    "\n",
    "def mnist_dataset(file_paths, cache=True, shuffle_buffer_size=None):\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(file_paths, num_parallel_reads=AUTOTUNE)\n",
    "\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    if shuffle_buffer_size:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = (\n",
    "        dataset.map(get_record, num_parallel_calls=AUTOTUNE)\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(AUTOTUNE)\n",
    "    )\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = mnist_dataset(train_files, shuffle_buffer_size=SHUFFLE_BUFFER)\n",
    "valid_set = mnist_dataset(valid_files)\n",
    "test_set = mnist_dataset(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for x, y in train_set.take(1):\n",
    "    for i in range(5):\n",
    "        plt.subplot(1, 5, i + 1)\n",
    "        plt.imshow(x[i].numpy(), cmap=\"binary\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(str(y[i].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "     14/Unknown - 1s 45ms/step - loss: 1.3150 - accuracy: 0.5630INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 2s 114ms/step - loss: 1.3150 - accuracy: 0.5630 - val_loss: 0.7107 - val_accuracy: 0.7532\n",
      "Epoch 2/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6378 - accuracy: 0.7784INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 96ms/step - loss: 0.6368 - accuracy: 0.7785 - val_loss: 0.5670 - val_accuracy: 0.8017\n",
      "Epoch 3/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5315 - accuracy: 0.8123INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 2s 109ms/step - loss: 0.5317 - accuracy: 0.8122 - val_loss: 0.5347 - val_accuracy: 0.8017\n",
      "Epoch 4/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4864 - accuracy: 0.8268INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 90ms/step - loss: 0.4856 - accuracy: 0.8273 - val_loss: 0.4615 - val_accuracy: 0.8347\n",
      "Epoch 5/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4492 - accuracy: 0.8406INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 96ms/step - loss: 0.4490 - accuracy: 0.8406 - val_loss: 0.5146 - val_accuracy: 0.8138\n",
      "Epoch 6/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4435 - accuracy: 0.8414INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 97ms/step - loss: 0.4436 - accuracy: 0.8414 - val_loss: 0.4439 - val_accuracy: 0.8365\n",
      "Epoch 7/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4113 - accuracy: 0.8556INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 90ms/step - loss: 0.4113 - accuracy: 0.8556 - val_loss: 0.4266 - val_accuracy: 0.8485\n",
      "Epoch 8/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3946 - accuracy: 0.8620INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 97ms/step - loss: 0.3946 - accuracy: 0.8619 - val_loss: 0.4099 - val_accuracy: 0.8570\n",
      "Epoch 9/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3916 - accuracy: 0.8624INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 88ms/step - loss: 0.3911 - accuracy: 0.8625 - val_loss: 0.3878 - val_accuracy: 0.8607\n",
      "Epoch 10/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3741 - accuracy: 0.8682INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 98ms/step - loss: 0.3744 - accuracy: 0.8681 - val_loss: 0.4507 - val_accuracy: 0.8378\n",
      "Epoch 11/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3853 - accuracy: 0.8629INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 98ms/step - loss: 0.3848 - accuracy: 0.8632 - val_loss: 0.3775 - val_accuracy: 0.8667\n",
      "Epoch 12/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3641 - accuracy: 0.8719INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 88ms/step - loss: 0.3637 - accuracy: 0.8722 - val_loss: 0.3802 - val_accuracy: 0.8638\n",
      "Epoch 13/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3482 - accuracy: 0.8778INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 97ms/step - loss: 0.3482 - accuracy: 0.8777 - val_loss: 0.3773 - val_accuracy: 0.8633\n",
      "Epoch 14/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3440 - accuracy: 0.8798INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 91ms/step - loss: 0.3441 - accuracy: 0.8799 - val_loss: 0.3828 - val_accuracy: 0.8618\n",
      "Epoch 15/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3479 - accuracy: 0.8772INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 96ms/step - loss: 0.3480 - accuracy: 0.8770 - val_loss: 0.3663 - val_accuracy: 0.8655\n",
      "Epoch 16/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3332 - accuracy: 0.8827INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 97ms/step - loss: 0.3328 - accuracy: 0.8829 - val_loss: 0.3662 - val_accuracy: 0.8678\n",
      "Epoch 17/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3370 - accuracy: 0.8794INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 89ms/step - loss: 0.3365 - accuracy: 0.8796 - val_loss: 0.3608 - val_accuracy: 0.8717\n",
      "Epoch 18/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3222 - accuracy: 0.8858INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 98ms/step - loss: 0.3223 - accuracy: 0.8859 - val_loss: 0.3698 - val_accuracy: 0.8663\n",
      "Epoch 19/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3294 - accuracy: 0.8829INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 88ms/step - loss: 0.3295 - accuracy: 0.8831 - val_loss: 0.3560 - val_accuracy: 0.8722\n",
      "Epoch 20/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3173 - accuracy: 0.8881INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 99ms/step - loss: 0.3172 - accuracy: 0.8882 - val_loss: 0.3594 - val_accuracy: 0.8707\n",
      "Epoch 21/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3117 - accuracy: 0.8896INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 88ms/step - loss: 0.3114 - accuracy: 0.8897 - val_loss: 0.3524 - val_accuracy: 0.8735\n",
      "Epoch 22/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3049 - accuracy: 0.8916INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 97ms/step - loss: 0.3046 - accuracy: 0.8916 - val_loss: 0.3464 - val_accuracy: 0.8737\n",
      "Epoch 23/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3018 - accuracy: 0.8933INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 96ms/step - loss: 0.3018 - accuracy: 0.8933 - val_loss: 0.3325 - val_accuracy: 0.8820\n",
      "Epoch 24/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3065 - accuracy: 0.8903INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 89ms/step - loss: 0.3060 - accuracy: 0.8905 - val_loss: 0.3371 - val_accuracy: 0.8783\n",
      "Epoch 25/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2993 - accuracy: 0.8936INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 97ms/step - loss: 0.2997 - accuracy: 0.8935 - val_loss: 0.3656 - val_accuracy: 0.8690\n",
      "Epoch 26/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2932 - accuracy: 0.8960INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 89ms/step - loss: 0.2931 - accuracy: 0.8962 - val_loss: 0.3361 - val_accuracy: 0.8798\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2856 - accuracy: 0.8988INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 98ms/step - loss: 0.2856 - accuracy: 0.8988 - val_loss: 0.3389 - val_accuracy: 0.8798\n",
      "Epoch 28/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2850 - accuracy: 0.8977INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 96ms/step - loss: 0.2853 - accuracy: 0.8978 - val_loss: 0.3363 - val_accuracy: 0.8807\n",
      "Epoch 29/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2825 - accuracy: 0.9001INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 88ms/step - loss: 0.2827 - accuracy: 0.9002 - val_loss: 0.3319 - val_accuracy: 0.8795\n",
      "Epoch 30/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2810 - accuracy: 0.8991INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 97ms/step - loss: 0.2808 - accuracy: 0.8993 - val_loss: 0.3216 - val_accuracy: 0.8830\n",
      "Epoch 31/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2732 - accuracy: 0.9019INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 90ms/step - loss: 0.2731 - accuracy: 0.9020 - val_loss: 0.3399 - val_accuracy: 0.8770\n",
      "Epoch 32/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2799 - accuracy: 0.8996INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 97ms/step - loss: 0.2799 - accuracy: 0.8998 - val_loss: 0.3169 - val_accuracy: 0.8885\n",
      "Epoch 33/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2651 - accuracy: 0.9066INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 97ms/step - loss: 0.2649 - accuracy: 0.9066 - val_loss: 0.3323 - val_accuracy: 0.8788\n",
      "Epoch 34/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2657 - accuracy: 0.9056INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 88ms/step - loss: 0.2655 - accuracy: 0.9057 - val_loss: 0.3230 - val_accuracy: 0.8833\n",
      "Epoch 35/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2772 - accuracy: 0.8999INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 100ms/step - loss: 0.2768 - accuracy: 0.8998 - val_loss: 0.3208 - val_accuracy: 0.8845\n",
      "Epoch 36/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2613 - accuracy: 0.9074INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 92ms/step - loss: 0.2611 - accuracy: 0.9073 - val_loss: 0.3330 - val_accuracy: 0.8788\n",
      "Epoch 37/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2588 - accuracy: 0.9076INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 103ms/step - loss: 0.2590 - accuracy: 0.9075 - val_loss: 0.3212 - val_accuracy: 0.8840\n",
      "Epoch 38/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2519 - accuracy: 0.9111INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 104ms/step - loss: 0.2515 - accuracy: 0.9112 - val_loss: 0.3413 - val_accuracy: 0.8762\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2695 - accuracy: 0.9021INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 94ms/step - loss: 0.2695 - accuracy: 0.9021 - val_loss: 0.3236 - val_accuracy: 0.8850\n",
      "Epoch 40/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2515 - accuracy: 0.9104INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 98ms/step - loss: 0.2517 - accuracy: 0.9101 - val_loss: 0.3428 - val_accuracy: 0.8768\n",
      "Epoch 41/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2513 - accuracy: 0.9105INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 87ms/step - loss: 0.2513 - accuracy: 0.9105 - val_loss: 0.3161 - val_accuracy: 0.8842\n",
      "Epoch 42/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2420 - accuracy: 0.9146INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 97ms/step - loss: 0.2417 - accuracy: 0.9147 - val_loss: 0.3436 - val_accuracy: 0.8733\n",
      "Epoch 43/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2512 - accuracy: 0.9109INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 96ms/step - loss: 0.2510 - accuracy: 0.9112 - val_loss: 0.3437 - val_accuracy: 0.8767\n",
      "Epoch 44/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2430 - accuracy: 0.9134INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 88ms/step - loss: 0.2431 - accuracy: 0.9132 - val_loss: 0.3107 - val_accuracy: 0.8900\n",
      "Epoch 45/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2369 - accuracy: 0.9161INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 96ms/step - loss: 0.2368 - accuracy: 0.9161 - val_loss: 0.3253 - val_accuracy: 0.8783\n",
      "Epoch 46/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2357 - accuracy: 0.9160INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 88ms/step - loss: 0.2358 - accuracy: 0.9159 - val_loss: 0.3291 - val_accuracy: 0.8788\n",
      "Epoch 47/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2363 - accuracy: 0.9157INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 96ms/step - loss: 0.2363 - accuracy: 0.9158 - val_loss: 0.3093 - val_accuracy: 0.8905\n",
      "Epoch 48/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2298 - accuracy: 0.9190INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 87ms/step - loss: 0.2297 - accuracy: 0.9189 - val_loss: 0.3051 - val_accuracy: 0.8912\n",
      "Epoch 49/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2297 - accuracy: 0.9187INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 96ms/step - loss: 0.2298 - accuracy: 0.9187 - val_loss: 0.3322 - val_accuracy: 0.8787\n",
      "Epoch 50/50\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2316 - accuracy: 0.9177INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/fashion_mnist/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 96ms/step - loss: 0.2319 - accuracy: 0.9176 - val_loss: 0.3211 - val_accuracy: 0.8822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bf4214e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(1992)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Normalization of the data\n",
    "sample_images = train_set.take(250).map(lambda image, label: image)\n",
    "normalizer = layers.Normalization(input_shape=[28, 28])\n",
    "normalizer.adapt(sample_images)\n",
    "\n",
    "# Callbacks\n",
    "tensorboard_cb = callbacks.TensorBoard(\n",
    "    log_dir=get_log_dir(), profile_batch=10, histogram_freq=1\n",
    ")\n",
    "model_checkpoint_cb = callbacks.ModelCheckpoint(filepath=os.path.join(SAVE_MODEL_PATH, \"fashion_mnist\"))\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        normalizer,\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(100, activation=\"relu\"),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=\"nadam\"\n",
    ")\n",
    "\n",
    "model.fit(train_set, epochs=50, validation_data=valid_set, callbacks=[tensorboard_cb, model_checkpoint_cb])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. In this exercise you will download a dataset, split it, create a tf.data.Dataset to load it and preprocess it efficiently, then build and train a binary classification model containing an Embedding layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Download the Large Movie Review Dataset, which contains 50,000 movie reviews from the Internet Movie Database (IMDb). The data is organized in two directories, train and test, each containing a pos subdirectory with 12,500 positive reviews and a neg subdirectory with 12,500 negative reviews. Each review is stored in a separate text file. There are other files and folders (including preprocessed bag-of-words versions), but we will ignore them in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Split the test set into a validation set (15,000) and a test set (10,000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set, test_set = tfds.load(\n",
    "    name=\"imdb_reviews\",\n",
    "    data_dir=IMDB_PATH,\n",
    "    split=[\"train\", \"test[:15000]\", \"test[15000:]\"],\n",
    "    as_supervised=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for text in train_set.take(1):\n",
    "    print(text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Use tf.data to create an efficient dataset for each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.cache()\n",
    "valid_set = valid_set.cache()\n",
    "test_set = test_set.cache()\n",
    "\n",
    "train_set = train_set.shuffle(SHUFFLE_BUFFER).batch(32).prefetch(AUTOTUNE)\n",
    "valid_set = valid_set.batch(32).prefetch(AUTOTUNE)\n",
    "test_set = test_set.batch(32).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Create a binary classification model, using a TextVectorization layer to preprocess each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_set, valid_set, test_set, epochs=100):\n",
    "    \"\"\"This function trains a model with the data feeded.\n",
    "    The function saves the model in a separate folder.\n",
    "\n",
    "    Args:\n",
    "        train_set (tf.data.Dataset): Dataset containing the training records\n",
    "        valid_set (tf.data.Dataset): Dataset containing the values for validation\n",
    "        test_set (tf.data.Dataset): Dataset containing the values to evaluate the model\n",
    "        epochs (int, optional): Max number of epochs the model will be trained. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        model(tf.keras.Model): Model trained\n",
    "    \"\"\"\n",
    "    keras.backend.clear_session()\n",
    "    tf.random.set_seed(1992)\n",
    "\n",
    "    max_tokens = 5000\n",
    "    sample_text = train_set.map(lambda review, label: review)\n",
    "    vectorizer = layers.TextVectorization(max_tokens=max_tokens, output_mode=\"tf_idf\")\n",
    "    vectorizer.adapt(sample_text)\n",
    "\n",
    "    model = Sequential(\n",
    "        layers=[\n",
    "            vectorizer,\n",
    "            layers.Dense(\n",
    "                100,\n",
    "                kernel_initializer=\"he_normal\",\n",
    "                activation=\"elu\",\n",
    "            ),\n",
    "            layers.Dense(\n",
    "                100,\n",
    "                kernel_initializer=\"he_normal\",\n",
    "                activation=\"elu\",\n",
    "            ),\n",
    "            layers.Dense(1, activation=\"sigmoid\"),\n",
    "        ],\n",
    "        name=\"imdb_model\",\n",
    "    )\n",
    "\n",
    "    # Compile the model\n",
    "    loss = losses.binary_crossentropy\n",
    "    optimizer = optimizers.Nadam()\n",
    "    metric = metrics.accuracy\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "\n",
    "    # Callbacks\n",
    "    tensorboard_cb = callbacks.TensorBoard(log_dir=get_log_dir(), profile_batch=5)\n",
    "    early_stopping_cb = callbacks.EarlyStopping(patience=5)\n",
    "    model_checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(SAVE_MODEL_PATH, \"imdb_model\"), save_best_only=True\n",
    "    )\n",
    "    callbacks_list = [tensorboard_cb, early_stopping_cb, model_checkpoint_cb]\n",
    "\n",
    "    model.fit(\n",
    "        train_set,\n",
    "        validation_data=valid_set,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks_list,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    evaluation = model.evaluate(test_set, verbose=0)\n",
    "\n",
    "    print(f\"The model {model.name} has successfully been trained.\")\n",
    "    print(\n",
    "        f\"The training metrics are: \\n\\tAccuracy: {(evaluation[0]) * 100:.2f}\\n\\tLoss: {evaluation[1]:.2f}\"\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/imdb_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_13/imdb_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model imdb_model has successfully been trained.\n",
      "The training metrics are: \n",
      "\tAccuracy: 87.62\n",
      "\tLoss: 0.15\n"
     ]
    }
   ],
   "source": [
    "imdb_model = train_model(train_set, valid_set, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Add an Embedding layer and compute the mean embedding for each review, multiplied by the square root of the number of words (see Chapter 16). This rescaled mean embedding can then be passed to the rest of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Train the model and see what accuracy you get. Try to optimize your pipelines to make training as fast as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Use TFDS to load the same dataset more easily: tfds.load(\"imdb_reviews\")."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b20fb8559386147dc77cba9f7518a8c175fc982b8c10bebe842520e16b3ac5d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
