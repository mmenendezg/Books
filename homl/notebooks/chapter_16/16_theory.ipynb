{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16: Natural Language Processing with RNNs and Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Setup"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import (\n",
    "    callbacks,\n",
    "    layers,\n",
    "    optimizers,\n",
    "    losses,\n",
    "    Sequential,\n",
    "    utils,\n",
    ")\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Legacy issues with the optimizers \n",
    "from tensorflow.keras.optimizers.legacy import Adam, Nadam\n",
    "\n",
    "# Transformers from Hugging Face\n",
    "from transformers import pipeline, AutoTokenizer, TFAutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 128\n",
    "SEED = 1992\n",
    "LOGS_DIR = \"../../reports/logs/chapter_16/\"\n",
    "MODELS_PATH = \"../../models/chapter_16/\"\n",
    "DATASETS_PATH = \"../../datasets/chapter_16\"\n",
    "\n",
    "HUB_MODEL = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "\n",
    "# Controls if the models are fit when loading the notebook\n",
    "TRAIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for path in [LOGS_DIR, MODELS_PATH, DATASETS_PATH]:\n",
    "    if not tf.io.gfile.exists(path):\n",
    "        tf.io.gfile.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Shakespearean Text Using a Character RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAKESPEARE_URL = \"https://homl.info/shakespeare\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = utils.get_file(\"shakespeare.txt\", SHAKESPEARE_URL)\n",
    "\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shakespeare_text[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer = layers.TextVectorization(split=\"character\", standardize=\"lower\")\n",
    "text_vec_layer.adapt([shakespeare_text])\n",
    "encoded = text_vec_layer([shakespeare_text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded -= 2\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2\n",
    "dataset_size = len(encoded)\n",
    "\n",
    "print(f\"There are {n_tokens} different, and the dataset has {dataset_size:_} total characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=BATCH_SIZE):\n",
    "    \n",
    "    def flat_map_fn(window):\n",
    "        return window.batch(length + 1)\n",
    "    \n",
    "    def map_fn(window):\n",
    "        return (window[:, :-1], window[:, 1:])\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.window(length + 1, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(flat_map_fn)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=100_000, seed=seed)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.map(map_fn, num_parallel_calls=AUTOTUNE).prefetch(\n",
    "        AUTOTUNE\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 100\n",
    "tf.random.set_seed(SEED)\n",
    "train_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True, seed=SEED)\n",
    "valid_set = to_dataset(encoded[1_000_000:1_060_000], length=length)\n",
    "test_set = to_dataset(encoded[:1_060_000:], length=length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "char_rnn_model = Sequential(\n",
    "    [\n",
    "        layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "        layers.GRU(128, return_sequences=True),\n",
    "        layers.Dense(n_tokens, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "optimizer = optimizers.Nadam()\n",
    "loss = losses.sparse_categorical_crossentropy\n",
    "char_rnn_model.compile(\n",
    "    loss=loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Callbacks and training\n",
    "model_filepath = tf.io.gfile.join(MODELS_PATH, \"char_rnn\")\n",
    "model_checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "    model_filepath,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "log_dir = tf.io.gfile.join(LOGS_DIR, \"char_rnn\")\n",
    "profile_batch = int(len(encoded) / BATCH_SIZE) * 2\n",
    "tensorboard_cb = callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=f\"1, {profile_batch}\")\n",
    "callbacks_ = [model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "if TRAIN:\n",
    "    history = char_rnn_model.fit(\n",
    "        train_set,\n",
    "        validation_data=valid_set,\n",
    "        epochs=2,\n",
    "        callbacks=callbacks_\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_rnn_model = Sequential([\n",
    "    text_vec_layer,\n",
    "    layers.Lambda(lambda X: X - 2),\n",
    "    char_rnn_model,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = char_rnn_model.predict([\"To be or not to b\"])[0, -1]\n",
    "y_pred = tf.argmax(y_proba)\n",
    "text_vec_layer.get_vocabulary()[y_pred + 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Fake Shakespearean Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `tf.random.categorical()` function to generate random classes indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    y_proba = char_rnn_model.predict([text])[0, -1:]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
    "    return text_vec_layer.get_vocabulary()[char_id + 2]\n",
    "\n",
    "\n",
    "def extent_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(SEED)\n",
    "input_text = \"to be or not to b\"\n",
    "for temp in [0.001, 1, 10, 1000]:\n",
    "    text = extent_text(input_text, temperature=temp)\n",
    "    print(f\"TEMP:{temp}\")\n",
    "    print(f\"\\n\\t{text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stateful RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset_for_stateful_rnn(sequence, length):\n",
    "    \n",
    "    def window_to_batch(window):\n",
    "        return window.batch(length + 1)\n",
    "    \n",
    "    def map_fn(window):\n",
    "        return (window[:, :-1], window[:, 1:])\n",
    "    \n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=length, drop_remainder=True)\n",
    "    ds = ds.flat_map(window_to_batch).batch(1)\n",
    "    return ds.map(map_fn, num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateful_train_set = to_dataset_for_stateful_rnn(encoded[:1_000_000], length=length)\n",
    "stateful_valid_set = to_dataset_for_stateful_rnn(\n",
    "    encoded[1_000_000:1_060_000], length=length\n",
    ")\n",
    "stateful_test_set = to_dataset_for_stateful_rnn(encoded[:1_060_000:], length=length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the model requires in this case to specify the batch size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateful_model = Sequential(\n",
    "    [\n",
    "        layers.Embedding(\n",
    "            input_dim=n_tokens, output_dim=16, batch_input_shape=[1, None]\n",
    "        ),\n",
    "        layers.GRU(128, return_sequences=True, stateful=True),\n",
    "        layers.Dense(n_tokens, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetStatesCallback(callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateful_model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=Nadam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Callbacks and training\n",
    "model_filepath = tf.io.gfile.join(MODELS_PATH, \"char_rnn\")\n",
    "model_checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "    model_filepath,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "log_dir = tf.io.gfile.join(LOGS_DIR, \"char_rnn\")\n",
    "profile_batch = int(len(encoded) / BATCH_SIZE) * 2\n",
    "tensorboard_cb = callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=f\"1, {profile_batch}\")\n",
    "callbacks_ = [model_checkpoint_cb, tensorboard_cb, ResetStatesCallback()]\n",
    "\n",
    "if TRAIN:\n",
    "    stateful_model.fit(\n",
    "        stateful_train_set,\n",
    "        validation_data=stateful_valid_set,\n",
    "        epochs=2,\n",
    "        callbacks=callbacks_\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_set, raw_valid_set, raw_test_set = tfds.load(\n",
    "    name=\"imdb_reviews\",\n",
    "    split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\n",
    "    as_supervised=True\n",
    ")\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "train_set = raw_train_set.shuffle(5000, seed=SEED).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "valid_set = raw_valid_set.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "test_set = raw_test_set.batch(BATCH_SIZE).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review, label in raw_train_set.shuffle(5000).take(7):\n",
    "    print(review.numpy().decode(\"utf-8\"))\n",
    "    print(f\"Label: {label.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "\n",
    "def get_reviews(review, label):\n",
    "    return review\n",
    "\n",
    "text_vec_layer = layers.TextVectorization(max_tokens=vocab_size)\n",
    "text_vec_layer.adapt(train_set.map(get_reviews, num_parallel_calls=AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "sentiment_imdb = Sequential([\n",
    "    text_vec_layer,\n",
    "    layers.Embedding(vocab_size, embed_size),\n",
    "    layers.GRU(128),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "sentiment_imdb.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=Nadam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Callbacks and training\n",
    "model_filepath = tf.io.gfile.join(MODELS_PATH, \"sentiment_imdb\")\n",
    "model_checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "    model_filepath,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "log_dir = tf.io.gfile.join(LOGS_DIR, \"sentiment_imdb\")\n",
    "tensorboard_cb = callbacks.TensorBoard(log_dir=log_dir)\n",
    "callbacks_ = [model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "if TRAIN:\n",
    "    sentiment_imdb.fit(\n",
    "        train_set,\n",
    "        validation_data=valid_set,\n",
    "        epochs=2,\n",
    "        callbacks=callbacks_\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain the previous model using masking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "sentiment_masking_imdb = Sequential(\n",
    "    [\n",
    "        text_vec_layer,\n",
    "        layers.Embedding(vocab_size, embed_size, mask_zero=True),\n",
    "        layers.GRU(128),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"sentiment_masking_imdb\",\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "sentiment_masking_imdb.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=Nadam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Callbacks and training\n",
    "model_filepath = tf.io.gfile.join(MODELS_PATH, \"sentiment_masking_imdb\")\n",
    "model_checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "    model_filepath,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "log_dir = tf.io.gfile.join(LOGS_DIR, \"sentiment_masking_imdb\")\n",
    "tensorboard_cb = callbacks.TensorBoard(log_dir=log_dir)\n",
    "callbacks_ = [model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "if TRAIN:\n",
    "    sentiment_masking_imdb.fit(\n",
    "        train_set, validation_data=valid_set, epochs=2, callbacks=callbacks_\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing the mask using the functional API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=[], dtype=tf.string)\n",
    "token_ids = text_vec_layer(inputs)\n",
    "mask = tf.math.not_equal(token_ids, 0)\n",
    "z = layers.Embedding(vocab_size, embed_size)(token_ids)\n",
    "z = layers.GRU(128, dropout=0.2)(z, mask=mask)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(z)\n",
    "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `TextVectorization` layer with ragged tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer_ragged = layers.TextVectorization(max_tokens=vocab_size, ragged=True)\n",
    "text_vec_layer_ragged.adapt(train_set.map(get_reviews, num_parallel_calls=AUTOTUNE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = [\n",
    "    \"Incredible movie! The best that Washington has done!\",\n",
    "    \"DiCaprio was incredible in this movie!\",\n",
    "]\n",
    "\n",
    "text_vec_layer(test_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer_ragged(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing Pretrained Embedings and Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TFHUB_CACHE_DIR\"] = tf.io.gfile.join(MODELS_PATH, \"tfhub_cache\")\n",
    "model = Sequential(\n",
    "    [\n",
    "        hub.KerasLayer(HUB_MODEL, trainable=True, dtype=tf.string, input_shape=[]),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"tfhub_model\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=losses.binary_crossentropy,\n",
    "    optimizer=Nadam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if TRAIN:\n",
    "    model.fit(\n",
    "        train_set,\n",
    "        validation_data=valid_set,\n",
    "        epochs=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Encoder-Decoder Network for Neural Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
    "path = tf.keras.utils.get_file(\n",
    "    \"spa-eng.zip\", origin=url, cache_dir=DATASETS_PATH, extract=True\n",
    ")\n",
    "text = (Path(path).with_name(\"spa-eng\") / \"spa.txt\").read_text()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original english text and its translation are separated by tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace(\"¡\", \"\").replace(\"¿\", \"\")\n",
    "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
    "np.random.shuffle(pairs)\n",
    "sentences_en, sentences_es = zip(*pairs) # Separates the pairs into 2 lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(f\"{sentences_en[i]} => {sentences_es[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create two `TextVectorization` layers -one per language- and adapt them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 2000\n",
    "max_length = 50\n",
    "# English TextVectorization layer\n",
    "text_vec_layer_en = layers.TextVectorization(\n",
    "    vocab_size, output_sequence_length=max_length\n",
    ")\n",
    "text_vec_layer_en.adapt(sentences_en)\n",
    "\n",
    "# Spanish TextVectorization layer\n",
    "text_vec_layer_es = layers.TextVectorization(\n",
    "    vocab_size, output_sequence_length=max_length\n",
    ")\n",
    "text_vec_layer_es.adapt([f\"startofseq {s} endofseq\" for s in sentences_es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer_en.get_vocabulary()[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer_es.get_vocabulary()[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation inputs for the encoder\n",
    "x_train = tf.constant(sentences_en[:100_000])\n",
    "x_valid = tf.constant(sentences_en[100_000:])\n",
    "# Training and validation inputs for the decoder\n",
    "x_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\n",
    "x_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[100_000:]])\n",
    "# Training and validation outputs for the decoder\n",
    "y_train = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\n",
    "y_valid = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the translation model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs of the model\n",
    "encoder_inputs = layers.Input(shape=[], dtype=tf.string)\n",
    "decoder_inputs = layers.Input(shape=[], dtype=tf.string)\n",
    "\n",
    "# Add the embeddings\n",
    "embed_size = 128\n",
    "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
    "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
    "encoder_embedding_layer = layers.Embedding(vocab_size, embed_size, mask_zero=True)\n",
    "decoder_embedding_layer = layers.Embedding(vocab_size, embed_size, mask_zero=True)\n",
    "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)\n",
    "\n",
    "# Encoder\n",
    "encoder = layers.LSTM(512, return_state=True)\n",
    "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
    "\n",
    "# Decoder\n",
    "decoder = layers.LSTM(512, return_sequences=True)\n",
    "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n",
    "\n",
    "# Output layer\n",
    "output_layer = layers.Dense(vocab_size, activation=\"softmax\")\n",
    "y_proba = output_layer(decoder_outputs)\n",
    "\n",
    "# Create the model\n",
    "translation_model = tf.keras.Model(\n",
    "    inputs=[encoder_inputs, decoder_inputs], outputs=[y_proba]\n",
    ")\n",
    "\n",
    "translation_model.compile(\n",
    "    loss=losses.sparse_categorical_crossentropy,\n",
    "    optimizer=Nadam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "if TRAIN:\n",
    "    translation_model.fit(\n",
    "        (x_train, x_train_dec),\n",
    "        y_train,\n",
    "        epochs=10,\n",
    "        validation_data=((x_valid, x_valid_dec), y_valid),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence_en, model):\n",
    "    translation = \"\"\n",
    "    for word_idx in range(max_length):\n",
    "        x = np.array([sentence_en])\n",
    "        x_dec = np.array([f\"startofseq {translation}\"])\n",
    "        y_proba = model.predict((x, x_dec), verbose=0)[0, word_idx]\n",
    "        \n",
    "        predicted_word_id = np.argmax(y_proba)\n",
    "        predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
    "        if predicted_word == \"endofseq\":\n",
    "            break\n",
    "        translation += f\" {predicted_word}\"\n",
    "    return translation.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = layers.Bidirectional(\n",
    "    layers.LSTM(256, return_state=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
    "encoder_state = [\n",
    "    tf.concat(encoder_state[::2], axis=-1), # short-term state 0 & 2\n",
    "    tf.concat(encoder_state[1::2], axis=-1), # short-term state 1 & 3\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs of the model\n",
    "encoder_inputs = layers.Input(shape=[], dtype=tf.string)\n",
    "decoder_inputs = layers.Input(shape=[], dtype=tf.string)\n",
    "\n",
    "# Add the embeddings\n",
    "embed_size = 128\n",
    "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
    "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
    "encoder_embedding_layer = layers.Embedding(vocab_size, embed_size, mask_zero=True)\n",
    "decoder_embedding_layer = layers.Embedding(vocab_size, embed_size, mask_zero=True)\n",
    "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)\n",
    "\n",
    "# Encoder\n",
    "encoder = layers.Bidirectional(\n",
    "    layers.LSTM(256, return_sequences=True, return_state=True)\n",
    ")\n",
    "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
    "encoder_state = [\n",
    "    tf.concat(encoder_state[::2], axis=-1),\n",
    "    tf.concat(encoder_state[1::2], axis=-1)\n",
    "]\n",
    "\n",
    "# Decoder\n",
    "decoder = layers.LSTM(512, return_sequences=True)\n",
    "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n",
    "\n",
    "# Attention layer\n",
    "attention_layer = layers.Attention()\n",
    "attention_outputs = attention_layer([decoder_outputs, encoder_outputs])\n",
    "\n",
    "# Output layer\n",
    "output_layer = layers.Dense(vocab_size, activation=\"softmax\")\n",
    "y_proba = output_layer(attention_outputs)\n",
    "\n",
    "# Create the model\n",
    "translation_model = tf.keras.Model(\n",
    "    inputs=[encoder_inputs, decoder_inputs], outputs=[y_proba]\n",
    ")\n",
    "\n",
    "translation_model.compile(\n",
    "    loss=losses.sparse_categorical_crossentropy,\n",
    "    optimizer=Nadam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Callbacks and training\n",
    "model_filepath = tf.io.gfile.join(MODELS_PATH, \"translate_attention_model\")\n",
    "model_checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "    model_filepath,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "log_dir = tf.io.gfile.join(LOGS_DIR, \"translate_attention_model\")\n",
    "tensorboard_cb = callbacks.TensorBoard(log_dir=log_dir)\n",
    "callbacks_ = [model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "# Validate if we need to train the model or to load a saved model\n",
    "train_attention = TRAIN\n",
    "if train_attention:\n",
    "    translation_model.fit(\n",
    "        (x_train, x_train_dec),\n",
    "        y_train,\n",
    "        epochs=10,\n",
    "        validation_data=((x_valid, x_valid_dec), y_valid),\n",
    "    )\n",
    "\n",
    "    translation_model.save(model_filepath, save_format=\"tf\")\n",
    "else:\n",
    "    translation_model = tf.keras.models.load_model(model_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"I like to play soccer with my cats\"\n",
    "translate(string, translation_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Is All You Need: The Original Transformer Architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the class for the position encoding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, max_length, embed_size, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        assert embed_size % 2 == 0, \"embed size must be even\"\n",
    "        p, i = np.meshgrid(\n",
    "            np.arange(max_length), \n",
    "            2 * np.arange(embed_size // 2)\n",
    "        )\n",
    "        pos_emb = np.empty((1, max_length, embed_size))\n",
    "        pos_emb[0, :, ::2] = np.sin(p / 10_000 ** (i / embed_size)).T\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / 10_000 ** (i / embed_size)).T\n",
    "        self.pos_encodings = tf.constant(pos_emb.astype(self.dtype))\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_max_length = tf.shape(inputs)[1]\n",
    "        return inputs + self.pos_encodings[:, :batch_max_length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 50\n",
    "embed_size = 128\n",
    "N = 2  # number of encoder and decoder blocks\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "n_units = 128  # for the first dense layers in the feed-forward block\n",
    "\n",
    "# Inputs of the model\n",
    "encoder_inputs = layers.Input(shape=[], dtype=tf.string)\n",
    "decoder_inputs = layers.Input(shape=[], dtype=tf.string)\n",
    "\n",
    "# Add the embeddings\n",
    "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
    "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
    "encoder_embedding_layer = layers.Embedding(vocab_size, embed_size, mask_zero=True)\n",
    "decoder_embedding_layer = layers.Embedding(vocab_size, embed_size, mask_zero=True)\n",
    "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)\n",
    "\n",
    "pos_embed_layer = PositionalEncoding(max_length, embed_size)\n",
    "encoder_in = pos_embed_layer(encoder_embeddings)\n",
    "decoder_in = pos_embed_layer(decoder_embeddings)\n",
    "\n",
    "# Encoder\n",
    "Z = encoder_in\n",
    "for _ in range(N):\n",
    "    skip = Z\n",
    "    attention_layer = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate\n",
    "    )\n",
    "    Z = attention_layer(Z, value=Z)\n",
    "    Z = layers.Add()([Z, skip])\n",
    "    Z = layers.LayerNormalization()(Z)\n",
    "    skip = Z\n",
    "    Z = layers.Dense(n_units, activation=\"relu\")(Z)\n",
    "    Z = layers.Dense(embed_size)(Z)\n",
    "    Z = layers.Dropout(dropout_rate)(Z)\n",
    "    Z = layers.Add()([Z, skip])\n",
    "    Z = layers.LayerNormalization()(Z)\n",
    "encoder_outputs = Z\n",
    "\n",
    "# Decoder\n",
    "Z = decoder_in\n",
    "for _ in range(N):\n",
    "    skip = Z\n",
    "    attention_layer = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate\n",
    "    )\n",
    "    Z = attention_layer(Z, value=Z, use_causal_mask=True)\n",
    "    Z = layers.Add()([Z, skip])\n",
    "    Z = layers.LayerNormalization()(Z)\n",
    "    skip = Z\n",
    "    attention_layer = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate\n",
    "    )\n",
    "    Z = attention_layer(Z, value=encoder_outputs)\n",
    "    Z = layers.Add()([Z, skip])\n",
    "    Z = layers.LayerNormalization()(Z)\n",
    "    skip = Z\n",
    "    Z = layers.Dense(n_units, activation=\"relu\")(Z)\n",
    "    Z = layers.Dense(embed_size)(Z)\n",
    "    Z = layers.Add()([Z, skip])\n",
    "    Z = layers.LayerNormalization()(Z)\n",
    "decoder_outputs = Z\n",
    "y_probs = layers.Dense(vocab_size, activation=\"softmax\")(Z)\n",
    "\n",
    "# Callbacks and training\n",
    "model_filepath = tf.io.gfile.join(MODELS_PATH, \"transformer\")\n",
    "model_checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "    model_filepath,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "log_dir = tf.io.gfile.join(LOGS_DIR, \"transformer\")\n",
    "tensorboard_cb = callbacks.TensorBoard(log_dir=log_dir)\n",
    "callbacks_ = [model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "# Create and train the model\n",
    "transformer = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[y_probs])\n",
    "transformer.compile(\n",
    "    loss=losses.sparse_categorical_crossentropy,\n",
    "    optimizer=Nadam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "train = TRAIN\n",
    "if train:\n",
    "    transformer.fit(\n",
    "        (x_train, x_train_dec),\n",
    "        y_train,\n",
    "        epochs=10,\n",
    "        validation_data=((x_valid, x_valid_dec), y_valid),\n",
    "        callbacks=callbacks_\n",
    "    )\n",
    "    transformer.save(model_filepath, save_format=\"tf\")\n",
    "else:\n",
    "    transformer = tf.keras.models.load_model(model_filepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face's Transformers Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"The actors were very convincing\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = [\n",
    "    \"Marlon did exactly what I requested\",\n",
    "    \"I am from El Salvador\",\n",
    "    \"He is a gang member\"\n",
    "]\n",
    "classifier(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"huggingface/distilbert-base-uncased-finetuned-mnli\"\n",
    "classifier_mnli = pipeline(\"text-classification\", model=model)\n",
    "classifier_mnli(\"She wakes up early. [SEP] I am not happy.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"huggingface/distilbert-base-uncased-finetuned-mnli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer(\n",
    "    [\n",
    "        \"I like soccer. [SEP] We all love soccer!\",\n",
    "        \"Joe lived for a very long time. [SEP] Joe is old.\",\n",
    "    ],\n",
    "    padding=True,\n",
    "    return_tensors=\"tf\",\n",
    ")\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(token_ids)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas = tf.keras.activations.softmax(outputs.logits)\n",
    "y_pred = tf.argmax(y_probas, axis=1)\n",
    "y_pred.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    (\"sky is blue\", \"sky is red\"),\n",
    "    (\"i love her\", \"she loves me\")\n",
    "]\n",
    "\n",
    "x_train = tokenizer(sentences, padding=True, return_tensors=\"tf\").data\n",
    "y_train = tf.constant([0, 2]) # Contradiction and neutral\n",
    "loss = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(\n",
    "    loss=loss,\n",
    "    optimizer=Nadam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "history = model.fit(x_train, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
