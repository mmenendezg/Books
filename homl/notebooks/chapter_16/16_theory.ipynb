{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16: Natural Language Processing with RNNs and Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import (\n",
    "    callbacks,\n",
    "    layers,\n",
    "    optimizers,\n",
    "    losses,\n",
    "    Sequential,\n",
    "    utils,\n",
    ")\n",
    "\n",
    "# Legacy issues with the optimizers \n",
    "from tensorflow.keras.optimizers.legacy import Adam, Nadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 64\n",
    "SEED = 1992\n",
    "LOGS_DIR = \"../../reports/logs/chapter_16/\"\n",
    "MODELS_PATH = \"../../models/chapter_16/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not tf.io.gfile.exists(LOGS_DIR):\n",
    "    tf.io.gfile.mkdir(LOGS_DIR)\n",
    "\n",
    "if not tf.io.gfile.exists(MODELS_PATH):\n",
    "    tf.io.gfile.mkdir(MODELS_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Shakespearean Text Using a Character RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAKESPEARE_URL = \"https://homl.info/shakespeare\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = utils.get_file(\"shakespeare.txt\", SHAKESPEARE_URL)\n",
    "\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n"
     ]
    }
   ],
   "source": [
    "print(shakespeare_text[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer = layers.TextVectorization(split=\"character\", standardize=\"lower\")\n",
    "text_vec_layer.adapt([shakespeare_text])\n",
    "encoded = text_vec_layer([shakespeare_text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 39 different, and the dataset has 1_115_394 total characters.\n"
     ]
    }
   ],
   "source": [
    "encoded -= 2\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2\n",
    "dataset_size = len(encoded)\n",
    "\n",
    "print(f\"There are {n_tokens} different, and the dataset has {dataset_size:_} total characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=BATCH_SIZE):\n",
    "    \n",
    "    def flat_map_fn(window):\n",
    "        return window.batch(length + 1)\n",
    "    \n",
    "    def map_fn(window):\n",
    "        return (window[:, :-1], window[:, 1:])\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.window(length + 1, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(flat_map_fn)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=100_000, seed=seed)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.map(map_fn, num_parallel_calls=AUTOTUNE).prefetch(\n",
    "        AUTOTUNE\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 100\n",
    "tf.random.set_seed(SEED)\n",
    "train_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True, seed=SEED)\n",
    "valid_set = to_dataset(encoded[1_000_000:1_060_000], length=length)\n",
    "test_set = to_dataset(encoded[:1_060_000:], length=length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "char_rnn_model = Sequential(\n",
    "    [\n",
    "        layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "        layers.GRU(128, return_sequences=True),\n",
    "        layers.Dense(n_tokens, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "optimizer = optimizers.Nadam()\n",
    "loss = losses.sparse_categorical_crossentropy\n",
    "char_rnn_model.compile(\n",
    "    loss=loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Callbacks and training\n",
    "model_filepath = tf.io.gfile.join(MODELS_PATH, \"char_rnn\")\n",
    "model_checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "    model_filepath,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "log_dir = tf.io.gfile.join(LOGS_DIR, \"char_rnn\")\n",
    "profile_batch = int(len(encoded) / BATCH_SIZE) * 2\n",
    "tensorboard_cb = callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=f\"1, {profile_batch}\")\n",
    "callbacks_ = [model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "history = char_rnn_model.fit(\n",
    "    train_set,\n",
    "    validation_data=valid_set,\n",
    "    epochs=2,\n",
    "    callbacks=callbacks_\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_rnn_model = Sequential([\n",
    "    text_vec_layer,\n",
    "    layers.Lambda(lambda X: X - 2),\n",
    "    char_rnn_model,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = char_rnn_model.predict([\"To be or not to b\"])[0, -1]\n",
    "y_pred = tf.argmax(y_proba)\n",
    "text_vec_layer.get_vocabulary()[y_pred + 2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Fake Shakespearean Text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `tf.random.categorical()` function to generate random classes indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probas = tf.math.log([0.5, 0.4, 0.1]) # Probas = 50%, 40%, 10%\n",
    "tf.random.set_seed(SEED)\n",
    "tf.random.categorical(log_probas, num_samples=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    y_proba = char_rnn_model.predict([text])[0, -1:]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
    "    return text_vec_layer.get_vocabulary()[char_id + 2]\n",
    "\n",
    "\n",
    "def extent_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(SEED)\n",
    "input_text = \"to be or not to b\"\n",
    "for temp in [0.001, 1, 10, 1000]:\n",
    "    text = extent_text(input_text, temperature=temp)\n",
    "    print(f\"TEMP:{temp}\")\n",
    "    print(f\"\\n\\t{text}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stateful RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset_for_stateful_rnn(sequence, length):\n",
    "    \n",
    "    def window_to_batch(window):\n",
    "        return window.batch(length + 1)\n",
    "    \n",
    "    def map_fn(window):\n",
    "        return (window[:, :-1], window[:, 1:])\n",
    "    \n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=length, drop_remainder=True)\n",
    "    ds = ds.flat_map(window_to_batch).batch(1)\n",
    "    return ds.map(map_fn, num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateful_train_set = to_dataset_for_stateful_rnn(encoded[:1_000_000], length=length)\n",
    "stateful_valid_set = to_dataset_for_stateful_rnn(\n",
    "    encoded[1_000_000:1_060_000], length=length\n",
    ")\n",
    "stateful_test_set = to_dataset_for_stateful_rnn(encoded[:1_060_000:], length=length)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the model requires in this case to specify the batch size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateful_model = Sequential(\n",
    "    [\n",
    "        layers.Embedding(\n",
    "            input_dim=n_tokens, output_dim=16, batch_input_shape=[1, None]\n",
    "        ),\n",
    "        layers.GRU(128, return_sequences=True, stateful=True),\n",
    "        layers.Dense(n_tokens, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetStatesCallback(callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "      4/Unknown - 2s 56ms/step - loss: 3.6576 - accuracy: 0.1275WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0405s vs `on_train_batch_end` time: 0.0483s). Check your callbacks.\n",
      "   9998/Unknown - 237s 24ms/step - loss: 2.1693 - accuracy: 0.3615"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_16/char_rnn/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_16/char_rnn/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999/9999 [==============================] - 246s 24ms/step - loss: 2.1693 - accuracy: 0.3616 - val_loss: 2.0190 - val_accuracy: 0.3922\n",
      "Epoch 2/10\n",
      "9997/9999 [============================>.] - ETA: 0s - loss: 1.9392 - accuracy: 0.4177"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_16/char_rnn/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_16/char_rnn/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999/9999 [==============================] - 243s 24ms/step - loss: 1.9392 - accuracy: 0.4177 - val_loss: 1.9434 - val_accuracy: 0.4057\n",
      "Epoch 3/10\n",
      "9997/9999 [============================>.] - ETA: 0s - loss: 1.8636 - accuracy: 0.4395"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_16/char_rnn/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_16/char_rnn/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999/9999 [==============================] - 242s 24ms/step - loss: 1.8635 - accuracy: 0.4395 - val_loss: 1.9056 - val_accuracy: 0.4226\n",
      "Epoch 4/10\n",
      "9998/9999 [============================>.] - ETA: 0s - loss: 1.8219 - accuracy: 0.4515"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_16/char_rnn/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_16/char_rnn/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999/9999 [==============================] - 279s 28ms/step - loss: 1.8219 - accuracy: 0.4515 - val_loss: 1.8856 - val_accuracy: 0.4283\n",
      "Epoch 5/10\n",
      "9998/9999 [============================>.] - ETA: 0s - loss: 1.7958 - accuracy: 0.4589"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_16/char_rnn/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_16/char_rnn/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999/9999 [==============================] - 240s 24ms/step - loss: 1.7958 - accuracy: 0.4589 - val_loss: 1.8723 - val_accuracy: 0.4323\n",
      "Epoch 6/10\n",
      "9999/9999 [==============================] - ETA: 0s - loss: 1.7779 - accuracy: 0.4639"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_16/char_rnn/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_16/char_rnn/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999/9999 [==============================] - 242s 24ms/step - loss: 1.7779 - accuracy: 0.4639 - val_loss: 1.8624 - val_accuracy: 0.4364\n",
      "Epoch 7/10\n",
      "9998/9999 [============================>.] - ETA: 0s - loss: 1.7649 - accuracy: 0.4676"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_16/char_rnn/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_16/char_rnn/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999/9999 [==============================] - 241s 24ms/step - loss: 1.7649 - accuracy: 0.4676 - val_loss: 1.8546 - val_accuracy: 0.4383\n",
      "Epoch 8/10\n",
      "9997/9999 [============================>.] - ETA: 0s - loss: 1.7547 - accuracy: 0.4705"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_16/char_rnn/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../models/chapter_16/char_rnn/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999/9999 [==============================] - 244s 24ms/step - loss: 1.7547 - accuracy: 0.4705 - val_loss: 1.8482 - val_accuracy: 0.4411\n",
      "Epoch 9/10\n",
      "7107/9999 [====================>.........] - ETA: 1:14 - loss: 1.7388 - accuracy: 0.4756"
     ]
    }
   ],
   "source": [
    "stateful_model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=Nadam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Callbacks and training\n",
    "model_filepath = tf.io.gfile.join(MODELS_PATH, \"char_rnn\")\n",
    "model_checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "    model_filepath,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "log_dir = tf.io.gfile.join(LOGS_DIR, \"char_rnn\")\n",
    "profile_batch = int(len(encoded) / BATCH_SIZE) * 2\n",
    "tensorboard_cb = callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=f\"1, {profile_batch}\")\n",
    "callbacks_ = [model_checkpoint_cb, tensorboard_cb, ResetStatesCallback()]\n",
    "\n",
    "stateful_model.fit(\n",
    "    stateful_train_set,\n",
    "    validation_data=stateful_valid_set,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks_\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
