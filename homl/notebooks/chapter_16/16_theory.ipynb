{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16: Natural Language Processing with RNNs and Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "dlopen(/Users/mmenendezg/Developer/Books/.venv/lib/python3.10/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 0x0006): symbol not found in flat namespace '__ZN10tensorflow8internal10LogMessage16VmoduleActivatedEPKci'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m keras\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     callbacks,\n\u001b[1;32m      5\u001b[0m     layers,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     utils,\n\u001b[1;32m     10\u001b[0m )\n",
      "File \u001b[0;32m~/Developer/Books/.venv/lib/python3.10/site-packages/tensorflow/__init__.py:440\u001b[0m\n\u001b[1;32m    438\u001b[0m _plugin_dir \u001b[39m=\u001b[39m _os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(_s, \u001b[39m'\u001b[39m\u001b[39mtensorflow-plugins\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    439\u001b[0m \u001b[39mif\u001b[39;00m _os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(_plugin_dir):\n\u001b[0;32m--> 440\u001b[0m   _ll\u001b[39m.\u001b[39;49mload_library(_plugin_dir)\n\u001b[1;32m    441\u001b[0m   \u001b[39m# Load Pluggable Device Library\u001b[39;00m\n\u001b[1;32m    442\u001b[0m   _ll\u001b[39m.\u001b[39mload_pluggable_device_library(_plugin_dir)\n",
      "File \u001b[0;32m~/Developer/Books/.venv/lib/python3.10/site-packages/tensorflow/python/framework/load_library.py:151\u001b[0m, in \u001b[0;36mload_library\u001b[0;34m(library_location)\u001b[0m\n\u001b[1;32m    148\u001b[0m     kernel_libraries \u001b[39m=\u001b[39m [library_location]\n\u001b[1;32m    150\u001b[0m   \u001b[39mfor\u001b[39;00m lib \u001b[39min\u001b[39;00m kernel_libraries:\n\u001b[0;32m--> 151\u001b[0m     py_tf\u001b[39m.\u001b[39;49mTF_LoadLibrary(lib)\n\u001b[1;32m    153\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m       errno\u001b[39m.\u001b[39mENOENT,\n\u001b[1;32m    156\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mThe file or folder to load kernel libraries from does not exist.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    157\u001b[0m       library_location)\n",
      "\u001b[0;31mNotFoundError\u001b[0m: dlopen(/Users/mmenendezg/Developer/Books/.venv/lib/python3.10/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 0x0006): symbol not found in flat namespace '__ZN10tensorflow8internal10LogMessage16VmoduleActivatedEPKci'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import (\n",
    "    callbacks,\n",
    "    layers,\n",
    "    optimizers,\n",
    "    losses,\n",
    "    Sequential,\n",
    "    utils,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 64\n",
    "SEED = 1992\n",
    "LOGS_DIR = \"../../reports/logs/chapter_16/\"\n",
    "MODELS_PATH = \"../../models/chapter_16/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not tf.io.gfile.exists(LOGS_DIR):\n",
    "    tf.io.gfile.mkdir(LOGS_DIR)\n",
    "\n",
    "if not tf.io.gfile.exists(MODELS_PATH):\n",
    "    tf.io.gfile.mkdir(MODELS_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Shakespearean Text Using a Character RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAKESPEARE_URL = \"https://homl.info/shakespeare\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = utils.get_file(\"shakespeare.txt\", SHAKESPEARE_URL)\n",
    "\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shakespeare_text[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer = layers.TextVectorization(split=\"character\", standardize=\"lower\")\n",
    "text_vec_layer.adapt([shakespeare_text])\n",
    "encoded = text_vec_layer([shakespeare_text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded -= 2\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2\n",
    "dataset_size = len(encoded)\n",
    "\n",
    "print(f\"There are {n_tokens} different, and the dataset has {dataset_size:_} total characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=BATCH_SIZE):\n",
    "    \n",
    "    def flat_map_fn(window):\n",
    "        return window.batch(length + 1)\n",
    "    \n",
    "    def map_fn(window):\n",
    "        return (window[:, :-1], window[:, 1:])\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.window(length + 1, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(flat_map_fn)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=100_000, seed=seed)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.map(map_fn, num_parallel_calls=AUTOTUNE).prefetch(\n",
    "        AUTOTUNE\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 100\n",
    "tf.random.set_seed(SEED)\n",
    "train_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True, seed=SEED)\n",
    "valid_set = to_dataset(encoded[1_000_000:1_060_000], length=length)\n",
    "test_set = to_dataset(encoded[:1_060_000:], length=length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "char_rnn_model = Sequential(\n",
    "    [\n",
    "        layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "        layers.GRU(128, return_sequences=True),\n",
    "        layers.Dense(n_tokens, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "optimizer = optimizers.Nadam()\n",
    "loss = losses.sparse_categorical_crossentropy\n",
    "char_rnn_model.compile(\n",
    "    loss=loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Callbacks and training\n",
    "model_filepath = tf.io.gfile.join(MODELS_PATH, \"char_rnn\")\n",
    "model_checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "    model_filepath,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "log_dir = tf.io.gfile.join(LOGS_DIR, \"char_rnn\")\n",
    "profile_batch = int(len(encoded) / BATCH_SIZE) * 2\n",
    "tensorboard_cb = callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=f\"1, {profile_batch}\")\n",
    "callbacks_ = [model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "history = char_rnn_model.fit(\n",
    "    train_set,\n",
    "    validation_data=valid_set,\n",
    "    epochs=2,\n",
    "    callbacks=callbacks_\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_rnn_model = Sequential([\n",
    "    text_vec_layer,\n",
    "    layers.Lambda(lambda X: X - 2),\n",
    "    char_rnn_model,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = char_rnn_model.predict([\"To be or not to b\"])[0, -1]\n",
    "y_pred = tf.argmax(y_proba)\n",
    "text_vec_layer.get_vocabulary()[y_pred + 2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Fake Shakespearean Text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `tf.random.categorical()` function to generate random classes indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probas = tf.math.log([0.5, 0.4, 0.1]) # Probas = 50%, 40%, 10%\n",
    "tf.random.set_seed(SEED)\n",
    "tf.random.categorical(log_probas, num_samples=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    y_proba = char_rnn_model.predict([text])[0, -1:]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
    "    return text_vec_layer.get_vocabulary()[char_id + 2]\n",
    "\n",
    "\n",
    "def extent_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(SEED)\n",
    "input_text = \"to be or not to b\"\n",
    "for temp in [0.001, 1, 10, 1000]:\n",
    "    text = extent_text(input_text, temperature=temp)\n",
    "    print(f\"TEMP:{temp}\")\n",
    "    print(f\"\\n\\t{text}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stateful RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset_for_stateful_rnn(sequence, length):\n",
    "    \n",
    "    def window_to_batch(window):\n",
    "        return window.batch(length + 1)\n",
    "    \n",
    "    def map_fn(window):\n",
    "        return (window[:, :-1], window[:, 1:])\n",
    "    \n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=length, drop_remainder=True)\n",
    "    ds = ds.flat_map(window_to_batch).batch(1)\n",
    "    return ds.map(map_fn, num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m stateful_train_set \u001b[39m=\u001b[39m to_dataset_for_stateful_rnn(encoded[:\u001b[39m1_000_000\u001b[39m], length\u001b[39m=\u001b[39mlength)\n\u001b[1;32m      2\u001b[0m stateful_valid_set \u001b[39m=\u001b[39m to_dataset_for_stateful_rnn(\n\u001b[1;32m      3\u001b[0m     encoded[\u001b[39m1_000_000\u001b[39m:\u001b[39m1_060_000\u001b[39m], length\u001b[39m=\u001b[39mlength\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m stateful_test_set \u001b[39m=\u001b[39m to_dataset_for_stateful_rnn(encoded[:\u001b[39m1_060_000\u001b[39m:], length\u001b[39m=\u001b[39mlength)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encoded' is not defined"
     ]
    }
   ],
   "source": [
    "stateful_train_set = to_dataset_for_stateful_rnn(encoded[:1_000_000], length=length)\n",
    "stateful_valid_set = to_dataset_for_stateful_rnn(\n",
    "    encoded[1_000_000:1_060_000], length=length\n",
    ")\n",
    "stateful_test_set = to_dataset_for_stateful_rnn(encoded[:1_060_000:], length=length)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the model requires in this case to specify the batch size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateful_model = Sequential(\n",
    "    [\n",
    "        layers.Embedding(\n",
    "            input_dim=n_tokens, output_dim=16, batch_input_shape=[1, None]\n",
    "        ),\n",
    "        layers.GRU(128, return_sequences=True, stateful=True),\n",
    "        layers.Dense(n_tokens, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetStatesCallback(callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateful_model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"nadam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Callbacks and training\n",
    "model_filepath = tf.io.gfile.join(MODELS_PATH, \"char_rnn\")\n",
    "model_checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "    model_filepath,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "log_dir = tf.io.gfile.join(LOGS_DIR, \"char_rnn\")\n",
    "profile_batch = int(len(encoded) / BATCH_SIZE) * 2\n",
    "tensorboard_cb = callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=f\"1, {profile_batch}\")\n",
    "callbacks_ = [model_checkpoint_cb, tensorboard_cb, ResetStatesCallback()]\n",
    "\n",
    "stateful_model.fit(\n",
    "    stateful_train_set,\n",
    "    validation_data=stateful_valid_set,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks_\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
