{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, datasets, losses, models, optimizers, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the configuration for the logging output\n",
    "\n",
    "logging_format = '%(message)s'\n",
    "logging.basicConfig(format=logging_format, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Models and Training with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the solution for the exercises 12 and 13 of the chapter 12: *Custom Models and Training with TensorFlow* of the book *Hands On Machine Learning with Scikit-Learn, Keras & TensorFlow* of Aurélien Géron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 12\n",
    "\n",
    "**Implement a custom layer that performs Layer Normalization:**\n",
    "- **The ```build()``` method should define two trainable weights $\\alpha$ and $\\beta$, both of shape ```input_shape[-1:]``` and data type ```tf.float32```. $\\alpha$ should be initialized with ones, and $\\beta$ with zeros.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **The ```call()``` method should compute the mean $\\mu$ and standard deviation $\\sigma$ of each instance’s features. For this, you can use ```tf.nn.moments(inputs, axes=-1, keepdims=True)```, which returns the mean $\\mu$ and the variance $\\sigma^2$ of all instances (compute the square root of the variance to get the standard deviation). Then the function should compute and return $\\alpha \\otimes (\\textbf{X}-\\mu)/(\\sigma + \\varepsilon) + \\beta$, where $\\otimes$ represents itemwise multiplication (*) and $\\varepsilon$ is a smoothing term (small constant to avoid division by zero, e.g., 0.001).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Ensure that your custom layer produces the same (or very nearly the same) output as the ```keras.layers.LayerNormalization``` layer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a custom layer, it is require to create a class that subclasses the ```keras.layers.Layer``` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayerNormalization(layers.Layer):\n",
    "    \n",
    "    \n",
    "    def __init__(self, epsilon=0.001, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    \n",
    "    def build(self, batch_input_shape):\n",
    "        self.alpha = self.add_weight(\n",
    "            name='alpha', \n",
    "            shape=batch_input_shape[-1],\n",
    "            initializer='ones'\n",
    "        )\n",
    "        self.beta = self.add_weight(\n",
    "            name='beta',\n",
    "            shape=batch_input_shape[-1],\n",
    "            initializer='zeros'\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def call(self, X):\n",
    "        mean, variance = tf.nn.moments(X, axes=-1, keepdims=True)\n",
    "        return self.alpha * (X - mean)/(tf.sqrt(variance) + self.epsilon) + self.beta\n",
    "\n",
    "    \n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape\n",
    "    \n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = self.get_config()\n",
    "        return {**base_config, 'epsilon':self.epsilon}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to verify that our custom layer produces the same output as the ```keras.layers.LayerNormalization``` layer, let's create an array of random numbers to evaluate the MSE between the outputs of our layer and the output immplemented by keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-29 21:40:34.588788: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2021-12-29 21:40:34.588908: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "The mean difference between the LayerNormalization layer and the custom layer is 8.283178431156557e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = np.random.rand(40000, 32, 32).astype(np.float32)\n",
    "\n",
    "keras_layer = layers.LayerNormalization()\n",
    "custom_layer = CustomLayerNormalization()\n",
    "\n",
    "difference = losses.mean_squared_error(keras_layer(data), custom_layer(data))\n",
    "logging.info(f'The mean difference between the LayerNormalization layer and the custom layer is {np.mean(difference)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference is really low, and therefore we can conclude the outputs are almost the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train a model using a custom training loop to tackle the Fashion MNIST dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first download the Fashion MNIST dataset. This is a collection of 60,000 images of fashion items, with size 28 x 28. The classes are the following:\n",
    "\n",
    "|Label|Description|\n",
    "|:---:|:---:|\n",
    "|0|T-shirt/top|\n",
    "|1|Trouser|\n",
    "|2|Pullover|\n",
    "|3|Dress|\n",
    "|4|Coat|\n",
    "|5|Sandal|\n",
    "|6|Shirt|\n",
    "|7|Sneaker|\n",
    "|8|Bag|\n",
    "|9|Ankle Boot|\n",
    "\n",
    "If you want to have more details on the dataset, please visit the [documentation of Keras](https://keras.io/api/datasets/fashion_mnist/#fashion-mnist-dataset-an-alternative-to-mnist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = datasets.fashion_mnist.load_data()\n",
    "x_train, x_valid = x_train[10000:], x_train[:10000]\n",
    "y_train, y_valid = y_train[10000:], y_train[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Display the epoch, iteration, mean training loss, and mean accuracy over each epoch (updated at each iteration), as well as the validation loss and accuracy at the end of each epoch.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a custom training loop adds extra flexibility to our model, but also adds complexity to the model. Furthermore, makes our code harder to maintain.\n",
    "\n",
    "Even though it seems like a good alternative, only consider this option when you really need this extra flexibility. One good example as suggested per Aurélien Géron in his book, is when implementing some models from papers that require extra flexibility. Another good example when to use it, as well suggested per Aurélien Géron, is when trying to create models that use different optimizers in different layers.\n",
    "\n",
    "Let's first create two functions. The first one, ```random_sampling```, will sample $n$ instances from the training sets. Keep in mind that $n$ is defined by the parameter batch_size. The second function ```status_bar``` will show the relevant information per epoch. In this case, Géron stablishes the relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to sample randomly from the training set\n",
    "def random_sampling(x, y, batch_size=32):\n",
    "    idx = np.random.randint(len(x), size=batch_size)\n",
    "    return x[idx], y[idx]\n",
    "\n",
    "\n",
    "# Create a function that simulates the status bar printed when training a neural network\n",
    "def status_bar(epoch, total_epochs, loss, val_loss, time_epoch, metrics=None, val_accuracy=None):\n",
    "    metrics = ' - '.join(['{}: {:.4f}'.format(m.name, m.result()) for m in [loss] + (metrics or [])])\n",
    "    \n",
    "    logging.info(f'Epoch {epoch}/{total_epochs} ({time_epoch} s) - ' + metrics + ' - val_loss: {:.4f} - val_accuracy: {:.4f}'.format(val_loss, val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple model: the input layer will be a ```layers.Flatten()```, then 2 additional hidden layers with ```activation='elu'``` and ```kernel_initializer='he_normal'```, and the output layer will have ```activation='softmax``` with a ```kernel_initializer='glorot_normal'```. We have added a ```BatchNormalization()``` layer before the activation function (as suggested per Sergey Ioffe and Christian Szegedy in their [paper](https://arxiv.org/abs/1502.03167)).\n",
    "\n",
    "Then, we will create the variables for the training loop. Among them, we will create an optimizer: the Nesterov Accelerated Gradient (NAG). \n",
    "\n",
    "The *NAG* optimizer is a variant of the original *Momentum optimization* algorithm. The idea of this optimizer is to add momentum to the gradient. The same way a ball rolling down a slope (like in a street) has some momentum, e.g., the ball accelerates when rolls down, the gradient here will gain momentum when approaches to the minimum.\n",
    "\n",
    "The *NAG* extends the *Momentum Optimization* algorithm, by measuring the gradient of the cost function not at local point, but at different position. Being $\\theta$ the vector of the weights, NAG measures the gradient not in local $\\theta$, but in $\\theta + \\beta m$, where $\\beta$ is a friction factor, and $m$ is the momentum vector, in direction of the momentum. \n",
    "\n",
    "The algorithm for *NAG* is the following:\n",
    "\n",
    "> 1. $m \\gets \\beta m - \\eta \\nabla_\\theta J(\\theta + \\beta m)$\n",
    "> 2. $\\theta \\gets \\theta + m$\n",
    "\n",
    "*NAG* keeps track of the previous gradients in the momentum vector $m$. Substract the Gradient at the point $\\theta + \\beta m$, and then updates the weights $\\theta$.\n",
    "\n",
    "For further information of the *NAG* algorithm, please visit [this post](https://machinelearningmastery.com/gradient-descent-with-nesterov-momentum-from-scratch/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple model, using BatchNormalization layers\n",
    "\n",
    "mnist_model = models.Sequential([\n",
    "    layers.Flatten(input_shape=[28, 28]),\n",
    "    layers.LayerNormalization(),        # To normalize the input of the model\n",
    "    layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    layers.Dense(10, activation='softmax', kernel_initializer='glorot_normal')\n",
    "])\n",
    "\n",
    "# Set the variables for the training loop\n",
    "n_epochs = 25\n",
    "batch_size = 1500\n",
    "n_steps = len(x_train) // batch_size\n",
    "optimizer = optimizers.SGD(momentum=0.9, nesterov=True)     # Nesterov Accelerated Gradient\n",
    "loss_fn = losses.SparseCategoricalCrossentropy(name='Val_Loss')\n",
    "mean_loss = metrics.Mean(name='Loss')\n",
    "metrics_model = [metrics.SparseCategoricalAccuracy(name='Accuracy')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the actual training loop. We will create to loops: one for the epochs and other for the steps.\n",
    "\n",
    "The steps of the training loop are the following:\n",
    "\n",
    "- First we randomly sample $n$ instances of the training set.\n",
    "- The [```tf.GradientTape()```](https://www.tensorflow.org/api_docs/python/tf/GradientTape) records the operations that are executed within this block. This allows to use the results of this operations to compute the automatic differentiation.\n",
    "- The ```tape.gradient()``` method calculates the gradientes of the loss of the function, and the ```optimizer.apply_gradients()``` method applies the gradients to all the trainable variables (weights) of the model or layer (see Exercise 13).\n",
    "- The mean of the loss is calculated, and we iterate on each metric (in this case just ```keras.metrics.SparseCategoricalAccuracy()```) to compute them.\n",
    "- In order to calculate the loss and the accuracy on the validation set, it is necessary to run the model as function on the validation set. Note that previously we set the parameter ```training=True```, now it is necessary to be ```False``` (which is the default value). Since the function ```keras.losses.SparseCategoricalCrossentropy()``` calculates the loss per instance, it is necessary to calculate its mean, the same with the ```keras.metrics.sparse_categorical_accuracy```.\n",
    "- We calculate the time of processing by recording the start time, and the end time. This will allow us to measure the time of traning per epoch.\n",
    "- Finally we call the ```status_bar()``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 (0.92 s) - Loss: 1.0735 - Accuracy: 0.6676 - val_loss: 0.6729 - val_accuracy: 0.7702\n",
      "Epoch 2/25 (0.76 s) - Loss: 0.8709 - Accuracy: 0.7188 - val_loss: 0.5840 - val_accuracy: 0.7946\n",
      "Epoch 3/25 (0.72 s) - Loss: 0.7798 - Accuracy: 0.7497 - val_loss: 0.5456 - val_accuracy: 0.8034\n",
      "Epoch 4/25 (0.77 s) - Loss: 0.7191 - Accuracy: 0.7654 - val_loss: 0.4834 - val_accuracy: 0.8222\n",
      "Epoch 5/25 (0.76 s) - Loss: 0.6829 - Accuracy: 0.7759 - val_loss: 0.4992 - val_accuracy: 0.8155\n",
      "Epoch 6/25 (0.75 s) - Loss: 0.6440 - Accuracy: 0.7852 - val_loss: 0.5192 - val_accuracy: 0.8145\n",
      "Epoch 7/25 (0.75 s) - Loss: 0.6206 - Accuracy: 0.7896 - val_loss: 0.4556 - val_accuracy: 0.8413\n",
      "Epoch 8/25 (0.76 s) - Loss: 0.6013 - Accuracy: 0.7958 - val_loss: 0.4901 - val_accuracy: 0.8268\n",
      "Epoch 9/25 (0.75 s) - Loss: 0.5871 - Accuracy: 0.7999 - val_loss: 0.4852 - val_accuracy: 0.8292\n",
      "Epoch 10/25 (0.76 s) - Loss: 0.5781 - Accuracy: 0.8009 - val_loss: 0.4627 - val_accuracy: 0.8311\n",
      "Epoch 11/25 (0.77 s) - Loss: 0.5639 - Accuracy: 0.8053 - val_loss: 0.4616 - val_accuracy: 0.8318\n",
      "Epoch 12/25 (0.78 s) - Loss: 0.5525 - Accuracy: 0.8074 - val_loss: 0.4907 - val_accuracy: 0.8304\n",
      "Epoch 13/25 (0.82 s) - Loss: 0.5414 - Accuracy: 0.8105 - val_loss: 0.4736 - val_accuracy: 0.8289\n",
      "Epoch 14/25 (0.75 s) - Loss: 0.5370 - Accuracy: 0.8120 - val_loss: 0.4593 - val_accuracy: 0.8370\n",
      "Epoch 15/25 (0.75 s) - Loss: 0.5293 - Accuracy: 0.8148 - val_loss: 0.4334 - val_accuracy: 0.8480\n",
      "Epoch 16/25 (0.8 s) - Loss: 0.5214 - Accuracy: 0.8178 - val_loss: 0.4241 - val_accuracy: 0.8476\n",
      "Epoch 17/25 (0.74 s) - Loss: 0.5115 - Accuracy: 0.8203 - val_loss: 0.4027 - val_accuracy: 0.8576\n",
      "Epoch 18/25 (0.75 s) - Loss: 0.5064 - Accuracy: 0.8215 - val_loss: 0.4245 - val_accuracy: 0.8468\n",
      "Epoch 19/25 (0.76 s) - Loss: 0.5004 - Accuracy: 0.8232 - val_loss: 0.4167 - val_accuracy: 0.8509\n",
      "Epoch 20/25 (0.71 s) - Loss: 0.4954 - Accuracy: 0.8246 - val_loss: 0.4113 - val_accuracy: 0.8552\n",
      "Epoch 21/25 (0.72 s) - Loss: 0.4905 - Accuracy: 0.8255 - val_loss: 0.4194 - val_accuracy: 0.8440\n",
      "Epoch 22/25 (0.77 s) - Loss: 0.4856 - Accuracy: 0.8271 - val_loss: 0.3998 - val_accuracy: 0.8509\n",
      "Epoch 23/25 (0.75 s) - Loss: 0.4810 - Accuracy: 0.8281 - val_loss: 0.3983 - val_accuracy: 0.8569\n",
      "Epoch 24/25 (0.74 s) - Loss: 0.4762 - Accuracy: 0.8296 - val_loss: 0.4066 - val_accuracy: 0.8555\n",
      "Epoch 25/25 (0.73 s) - Loss: 0.4717 - Accuracy: 0.8307 - val_loss: 0.4155 - val_accuracy: 0.8524\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    start_time = time.time()    \n",
    "    for step in range(1, n_steps + 1):\n",
    "        x_batch, y_batch = random_sampling(x_train, y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = mnist_model(x_batch, training=True)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            model_loss = tf.add_n([main_loss] + mnist_model.losses)\n",
    "        gradients = tape.gradient(model_loss, mnist_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, mnist_model.trainable_variables))\n",
    "        mean_loss(model_loss)\n",
    "        for metric in metrics_model:\n",
    "            metric(y_batch, y_pred)\n",
    "\n",
    "    y_valid_pred = mnist_model(x_valid)\n",
    "    val_loss = np.mean(loss_fn(y_valid, y_valid_pred))\n",
    "    val_accuracy = np.mean(metrics.sparse_categorical_accuracy(tf.constant(y_valid), y_valid_pred))\n",
    "        \n",
    "    end_time = round(time.time() - start_time, 2)\n",
    "    status_bar(epoch, n_epochs, mean_loss, val_loss, end_time, metrics_model, val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Try using a different optimizer with a different learning rate for the upper layers and the lower layers.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to achieve this, it will be necessary to create a new model to use different optimizers and learning rates for lower and upper layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_layers = models.Sequential([\n",
    "    layers.Flatten(input_shape=[28, 28]),\n",
    "    layers.LayerNormalization(),\n",
    "    layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "])\n",
    "\n",
    "upper_layers = models.Sequential([\n",
    "    layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    layers.Dense(10, activation='softmax', kernel_initializer='glorot_normal')\n",
    "])\n",
    "\n",
    "mnist_model_2 = models.Sequential([\n",
    "    lower_layers,\n",
    "    upper_layers\n",
    "])\n",
    "\n",
    "# Set the variables for the training loop\n",
    "n_epochs = 25\n",
    "batch_size = 1500\n",
    "n_steps = len(x_train) // batch_size\n",
    "lower_optimizer = optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)     # Nesterov Accelerated Gradient\n",
    "upper_optimizer = optimizers.Nadam(learning_rate=0.002, beta_1=0.9, beta_2=0.999)      # Nadam optimizer\n",
    "loss_fn = losses.SparseCategoricalCrossentropy(name='Val_Loss')\n",
    "mean_loss = metrics.Mean(name='Loss')\n",
    "metrics_model = [metrics.SparseCategoricalAccuracy(name='Accuracy')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some changes to take into consideration when creating the training loop:\n",
    "\n",
    "- It is necessary to add the parameter ```persistent=True``` to the ```tf.GradientTape()``` to allow to compute persistent gradients.\n",
    "- We will need to iterate on the lower layers and the upper layers.\n",
    "\n",
    "The rest of the training loop is similar to the previous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 (1.0 s) - Loss: 1.0863 - Accuracy: 0.6477 - val_loss: 0.6976 - val_accuracy: 0.7665\n",
      "Epoch 2/25 (0.95 s) - Loss: 0.8908 - Accuracy: 0.7012 - val_loss: 0.6237 - val_accuracy: 0.7737\n",
      "Epoch 3/25 (0.94 s) - Loss: 0.8014 - Accuracy: 0.7257 - val_loss: 0.5894 - val_accuracy: 0.7902\n",
      "Epoch 4/25 (0.96 s) - Loss: 0.7604 - Accuracy: 0.7358 - val_loss: 0.5668 - val_accuracy: 0.8007\n",
      "Epoch 5/25 (1.01 s) - Loss: 0.7269 - Accuracy: 0.7481 - val_loss: 0.5315 - val_accuracy: 0.8170\n",
      "Epoch 6/25 (0.97 s) - Loss: 0.6945 - Accuracy: 0.7580 - val_loss: 0.6007 - val_accuracy: 0.7994\n",
      "Epoch 7/25 (0.98 s) - Loss: 0.6692 - Accuracy: 0.7661 - val_loss: 0.5734 - val_accuracy: 0.8066\n",
      "Epoch 8/25 (0.93 s) - Loss: 0.6507 - Accuracy: 0.7720 - val_loss: 0.4858 - val_accuracy: 0.8312\n",
      "Epoch 9/25 (0.99 s) - Loss: 0.6276 - Accuracy: 0.7797 - val_loss: 0.4634 - val_accuracy: 0.8358\n",
      "Epoch 10/25 (1.04 s) - Loss: 0.6091 - Accuracy: 0.7857 - val_loss: 0.4477 - val_accuracy: 0.8430\n",
      "Epoch 11/25 (1.08 s) - Loss: 0.6016 - Accuracy: 0.7888 - val_loss: 0.4749 - val_accuracy: 0.8344\n",
      "Epoch 12/25 (1.1 s) - Loss: 0.5933 - Accuracy: 0.7914 - val_loss: 0.4608 - val_accuracy: 0.8364\n",
      "Epoch 13/25 (1.09 s) - Loss: 0.5794 - Accuracy: 0.7960 - val_loss: 0.4413 - val_accuracy: 0.8457\n",
      "Epoch 14/25 (1.16 s) - Loss: 0.5686 - Accuracy: 0.8009 - val_loss: 0.4420 - val_accuracy: 0.8448\n",
      "Epoch 15/25 (1.03 s) - Loss: 0.5561 - Accuracy: 0.8051 - val_loss: 0.4764 - val_accuracy: 0.8372\n",
      "Epoch 16/25 (1.03 s) - Loss: 0.5486 - Accuracy: 0.8075 - val_loss: 0.4560 - val_accuracy: 0.8417\n",
      "Epoch 17/25 (1.18 s) - Loss: 0.5415 - Accuracy: 0.8090 - val_loss: 0.4477 - val_accuracy: 0.8420\n",
      "Epoch 18/25 (1.18 s) - Loss: 0.5337 - Accuracy: 0.8110 - val_loss: 0.4593 - val_accuracy: 0.8344\n",
      "Epoch 19/25 (1.24 s) - Loss: 0.5295 - Accuracy: 0.8131 - val_loss: 0.4246 - val_accuracy: 0.8512\n",
      "Epoch 20/25 (1.15 s) - Loss: 0.5214 - Accuracy: 0.8158 - val_loss: 0.4242 - val_accuracy: 0.8486\n",
      "Epoch 21/25 (1.19 s) - Loss: 0.5160 - Accuracy: 0.8176 - val_loss: 0.4283 - val_accuracy: 0.8525\n",
      "Epoch 22/25 (1.22 s) - Loss: 0.5118 - Accuracy: 0.8190 - val_loss: 0.4128 - val_accuracy: 0.8560\n",
      "Epoch 23/25 (1.19 s) - Loss: 0.5061 - Accuracy: 0.8208 - val_loss: 0.4709 - val_accuracy: 0.8343\n",
      "Epoch 24/25 (1.17 s) - Loss: 0.5021 - Accuracy: 0.8219 - val_loss: 0.4521 - val_accuracy: 0.8403\n",
      "Epoch 25/25 (1.22 s) - Loss: 0.4953 - Accuracy: 0.8248 - val_loss: 0.4173 - val_accuracy: 0.8540\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    start_time = time.time()    \n",
    "    for step in range(1, n_steps + 1):\n",
    "        x_batch, y_batch = random_sampling(x_train, y_train)\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            y_pred = mnist_model_2(x_batch, training=True)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            model_loss = tf.add_n([main_loss] + mnist_model_2.losses)\n",
    "        for layers, optimizer in ((lower_layers, lower_optimizer), (upper_layers, upper_optimizer)):\n",
    "            gradients = tape.gradient(model_loss, layers.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, layers.trainable_variables))\n",
    "        mean_loss(model_loss)\n",
    "        for metric in metrics_model:\n",
    "            metric(y_batch, y_pred)\n",
    "\n",
    "    y_valid_pred = mnist_model_2(x_valid)\n",
    "    val_loss = np.mean(loss_fn(y_valid, y_valid_pred))\n",
    "    val_accuracy = np.mean(metrics.sparse_categorical_accuracy(tf.constant(y_valid), y_valid_pred))\n",
    "    \n",
    "    end_time = round(time.time() - start_time, 2)\n",
    "    status_bar(epoch, n_epochs, mean_loss, val_loss, end_time, metrics_model, val_accuracy)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6b5f867329b9bf7e45db95e42066174580d403eff48fd9a79312950ee94a58cb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('Books-uCdQc7s-': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
