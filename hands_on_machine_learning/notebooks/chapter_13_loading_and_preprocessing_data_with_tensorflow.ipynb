{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, optimizers, losses, layers, metrics, callbacks\n",
    "from tensorflow.train import BytesList, FloatList, Int64List\n",
    "from tensorflow.train import Features, Feature, Example\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Preprocessing Data With Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 18:07:49.623811: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-01-21 18:07:49.624171: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "x = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.repeat(3).batch(7, drop_remainder=True)\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 18:07:49.697011: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(lambda x: x * 2)\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/w1/7wtz55rs63b9743vzqfb8p880000gp/T/ipykernel_94672/2657488850.py:2: unbatch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.unbatch()`.\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Unbatch the items\n",
    "dataset = dataset.apply(tf.data.experimental.unbatch())\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda x: x < 10)\n",
    "\n",
    "for item in dataset.take(5):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([4 3 7 5 8 0 2], shape=(7,), dtype=int64)\n",
      "tf.Tensor([1 1 0 5 3 9 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([0 4 8 6 2 4 7], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 6 7 6 0 1 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([2 4 6 8 5 3 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([9 7 3 2 1 8 3], shape=(7,), dtype=int64)\n",
      "tf.Tensor([1 7 6 8 4 5 2], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(5)\n",
    "dataset = dataset.shuffle(buffer_size=7, seed=15).batch(7, drop_remainder=True)\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interleaving lines from multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Download the housing data such as chapter 2 \n",
    "    \n",
    "DOWNLOAD_ROOT = 'https://raw.githubusercontent.com/ageron/handson-ml2/master/'\n",
    "HOUSING_PATH = os.path.join('datasets', 'housing')\n",
    "HOUSING_URL = DOWNLOAD_ROOT + 'data/raw/housing/housing.tgz'\n",
    "    \n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    os.makedirs(housing_path, exist_ok=True)\n",
    "    tgz_path = os.path.join(housing_path, 'housing.tgz')\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, 'housing.csv')\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_housing_data()\n",
    "housing_data = load_housing_data()\n",
    "\n",
    "housing_num_features = housing_data.drop(columns=['ocean_proximity'])\n",
    "\n",
    "housing_train, housing_test = train_test_split(housing_num_features, test_size=0.2)\n",
    "housing_train, housing_valid = train_test_split(housing_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4048</th>\n",
       "      <td>-118.53</td>\n",
       "      <td>34.14</td>\n",
       "      <td>28.0</td>\n",
       "      <td>6920.0</td>\n",
       "      <td>906.0</td>\n",
       "      <td>2515.0</td>\n",
       "      <td>860.0</td>\n",
       "      <td>9.2189</td>\n",
       "      <td>500001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13766</th>\n",
       "      <td>-117.08</td>\n",
       "      <td>34.08</td>\n",
       "      <td>34.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0625</td>\n",
       "      <td>500001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-119.61</td>\n",
       "      <td>36.57</td>\n",
       "      <td>42.0</td>\n",
       "      <td>2242.0</td>\n",
       "      <td>521.0</td>\n",
       "      <td>1359.0</td>\n",
       "      <td>483.0</td>\n",
       "      <td>1.5833</td>\n",
       "      <td>65100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2279</th>\n",
       "      <td>-119.78</td>\n",
       "      <td>36.78</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2164.0</td>\n",
       "      <td>456.0</td>\n",
       "      <td>959.0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>2.3293</td>\n",
       "      <td>73400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4183</th>\n",
       "      <td>-118.23</td>\n",
       "      <td>34.13</td>\n",
       "      <td>48.0</td>\n",
       "      <td>737.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>3.5893</td>\n",
       "      <td>212500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "4048     -118.53     34.14                28.0       6920.0           906.0   \n",
       "13766    -117.08     34.08                34.0         45.0            11.0   \n",
       "2442     -119.61     36.57                42.0       2242.0           521.0   \n",
       "2279     -119.78     36.78                31.0       2164.0           456.0   \n",
       "4183     -118.23     34.13                48.0        737.0           166.0   \n",
       "\n",
       "       population  households  median_income  median_house_value  \n",
       "4048       2515.0       860.0         9.2189            500001.0  \n",
       "13766        39.0        14.0         3.0625            500001.0  \n",
       "2442       1359.0       483.0         1.5833             65100.0  \n",
       "2279        959.0       463.0         2.3293             73400.0  \n",
       "4183        462.0       131.0         3.5893            212500.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(dataset, delta=500, type_file='train'):\n",
    "    \n",
    "    dir_path = 'data/raw/housing'\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "    for i in range(0, dataset.shape[0], delta):\n",
    "        \n",
    "        data = dataset.iloc[i : i + delta]\n",
    "        data.to_csv(f'{dir_path}/{type_file}_{int(i / 500)}.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv(housing_train)\n",
    "save_csv(housing_valid, type_file='valid')\n",
    "save_csv(housing_test, type_file='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filepath = 'data/raw/housing/train*.csv'\n",
    "valid_filepath = 'data/raw/housing/valid*.csv'\n",
    "test_filepath = 'data/raw/housing/test*.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and the std of the features of x \n",
    "\n",
    "x_mean = np.mean(housing_num_features.iloc[:, :-1].to_numpy(), axis=0)\n",
    "x_std = np.std(housing_num_features.iloc[:, :-1].to_numpy(), axis=0)\n",
    "n_inputs = 8\n",
    "\n",
    "def preprocess(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    \n",
    "    return (x - x_mean) / x_std, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting Everything Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, repeat=1, n_readers=5, shuffle_buffer_size=10000, batch_size=32):\n",
    "    \n",
    "    dataset = tf.data.Dataset.list_files(filepaths)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath),\n",
    "        cycle_length=n_readers,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).cache()\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat)\n",
    "    \n",
    "    return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Dataset with tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_set = csv_reader_dataset(train_filepath)\n",
    "valid_set = csv_reader_dataset(valid_filepath)\n",
    "test_set = csv_reader_dataset(test_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "      1/Unknown - 0s 368ms/step - loss: 49549185024.0000 - mean_squared_error: 49549185024.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 18:07:50.861758: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    411/Unknown - 3s 6ms/step - loss: 55755128832.0000 - mean_squared_error: 55755128832.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 18:07:53.930780: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413/413 [==============================] - 4s 8ms/step - loss: 55736029184.0000 - mean_squared_error: 55736029184.0000 - val_loss: 56959340544.0000 - val_mean_squared_error: 56959340544.0000\n",
      "Epoch 2/300\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 55736053760.0000 - mean_squared_error: 55736053760.0000 - val_loss: 56959348736.0000 - val_mean_squared_error: 56959348736.0000\n",
      "Epoch 3/300\n",
      "413/413 [==============================] - 3s 7ms/step - loss: 55736025088.0000 - mean_squared_error: 55736025088.0000 - val_loss: 56959348736.0000 - val_mean_squared_error: 56959348736.0000\n",
      "Epoch 4/300\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 55736008704.0000 - mean_squared_error: 55736008704.0000 - val_loss: 56959348736.0000 - val_mean_squared_error: 56959348736.0000\n",
      "Epoch 5/300\n",
      "413/413 [==============================] - 3s 7ms/step - loss: 55735992320.0000 - mean_squared_error: 55735992320.0000 - val_loss: 56959348736.0000 - val_mean_squared_error: 56959348736.0000\n",
      "Epoch 6/300\n",
      "413/413 [==============================] - 3s 7ms/step - loss: 55736049664.0000 - mean_squared_error: 55736049664.0000 - val_loss: 56959348736.0000 - val_mean_squared_error: 56959348736.0000\n",
      "Epoch 7/300\n",
      "413/413 [==============================] - 3s 7ms/step - loss: 55736033280.0000 - mean_squared_error: 55736033280.0000 - val_loss: 56959369216.0000 - val_mean_squared_error: 56959369216.0000\n",
      "Epoch 8/300\n",
      "413/413 [==============================] - 3s 7ms/step - loss: 55736025088.0000 - mean_squared_error: 55736025088.0000 - val_loss: 56959352832.0000 - val_mean_squared_error: 56959352832.0000\n",
      "Epoch 9/300\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 55736049664.0000 - mean_squared_error: 55736049664.0000 - val_loss: 56959348736.0000 - val_mean_squared_error: 56959348736.0000\n",
      "Epoch 10/300\n",
      "413/413 [==============================] - 4s 9ms/step - loss: 55736033280.0000 - mean_squared_error: 55736033280.0000 - val_loss: 56959352832.0000 - val_mean_squared_error: 56959352832.0000\n",
      "Epoch 11/300\n",
      "413/413 [==============================] - 4s 9ms/step - loss: 55736008704.0000 - mean_squared_error: 55736008704.0000 - val_loss: 56959352832.0000 - val_mean_squared_error: 56959352832.0000\n",
      "Epoch 12/300\n",
      "413/413 [==============================] - 4s 9ms/step - loss: 55736020992.0000 - mean_squared_error: 55736020992.0000 - val_loss: 56959344640.0000 - val_mean_squared_error: 56959344640.0000\n",
      "Epoch 13/300\n",
      "413/413 [==============================] - 4s 9ms/step - loss: 55736033280.0000 - mean_squared_error: 55736033280.0000 - val_loss: 56959352832.0000 - val_mean_squared_error: 56959352832.0000\n",
      "Epoch 14/300\n",
      "413/413 [==============================] - 4s 9ms/step - loss: 55736029184.0000 - mean_squared_error: 55736029184.0000 - val_loss: 56959352832.0000 - val_mean_squared_error: 56959352832.0000\n",
      "Epoch 15/300\n",
      "413/413 [==============================] - 4s 9ms/step - loss: 55736004608.0000 - mean_squared_error: 55736004608.0000 - val_loss: 56959344640.0000 - val_mean_squared_error: 56959344640.0000\n",
      "Epoch 16/300\n",
      "413/413 [==============================] - 4s 9ms/step - loss: 55736049664.0000 - mean_squared_error: 55736049664.0000 - val_loss: 56959361024.0000 - val_mean_squared_error: 56959361024.0000\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(30, activation='elu', kernel_initializer='he_normal'),\n",
    "    layers.Dense(30, activation='elu', kernel_initializer='he_normal'),\n",
    "    layers.Dense(1, activation='relu', kernel_initializer='he_normal')\n",
    "])\n",
    "\n",
    "optimizer = optimizers.SGD(learning_rate=0.01, momentum=0.99, nesterov=True)\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mse',\n",
    "              metrics = [metrics.mean_squared_error])\n",
    "\n",
    "early_stopping_cb = callbacks.EarlyStopping(patience=15, restore_best_weights=True)\n",
    "cb_list = [early_stopping_cb]\n",
    "\n",
    "batch_size = 1500\n",
    "\n",
    "history_model = model.fit(train_set,\n",
    "                          validation_data=valid_set,\n",
    "                          epochs=300,\n",
    "                          batch_size=1500,\n",
    "                          callbacks=cb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/129 [==============================] - 1s 5ms/step - loss: 56601210880.0000 - mean_squared_error: 56601210880.0000\n",
      "[56601210880.0, 56601210880.0]\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate(test_set)\n",
    "\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TFRecord Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tfrecord file\n",
    "\n",
    "with tf.io.TFRecordWriter('data/raw/my_data.tfrecord') as f:\n",
    "    f.write(b'This is the first record.')\n",
    "    f.write(b'And this is the second record.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'This is the first record.', shape=(), dtype=string)\n",
      "tf.Tensor(b'And this is the second record.', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "filepaths = ['data/raw/my_data.tfrecord']\n",
    "dataset = tf.data.TFRecordDataset(filepaths, num_parallel_reads=tf.data.AUTOTUNE)\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compressed TFRecord Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'This is the first record.', shape=(), dtype=string)\n",
      "tf.Tensor(b'And this is the second record.', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "options = tf.io.TFRecordOptions(compression_type='GZIP')\n",
    "with tf.io.TFRecordWriter('data/raw/my_data.tfrecord', options) as f:\n",
    "    f.write(b'This is the first record.')\n",
    "    f.write(b'And this is the second record.')\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(filepaths, compression_type='GZIP')\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Protobufs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example of protobuf in tensorflow\n",
    "\n",
    "person_example = Example(\n",
    "    features = Features(\n",
    "        feature = {\n",
    "            'name' : Feature(bytes_list=BytesList(value=[b'Marlon'])),\n",
    "            'id' : Feature(int64_list = Int64List(value=[123])),\n",
    "            'emails' : Feature(bytes_list=BytesList(value=[b'marlon.menendezg@gmail.com',\n",
    "                                                           b'marlon.menendez1506@gmail.com']))\n",
    "}))\n",
    "\n",
    "# Serialize and write a TFRecord file\n",
    "with tf.io.TFRecordWriter('data/raw/contats.tfrecord') as f:\n",
    "    f.write(person_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Parsing Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse example by example\n",
    "\n",
    "feature_description = {\n",
    "    'name' : tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'id' : tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    'emails' : tf.io.VarLenFeature(tf.string),\n",
    "}\n",
    "\n",
    "for serialized_example in tf.data.TFRecordDataset('data/raw/contats.tfrecord'):\n",
    "    parsed_example = tf.io.parse_single_example(serialized_example, feature_description)\n",
    "    \n",
    "\n",
    "# Parse batch by batch \n",
    "\n",
    "dataset = tf.data.TFRecordDataset('data/raw/contats.tfrecord').batch(10)\n",
    "for serialized_examples in dataset:\n",
    "    parsed_examples = tf.io.parse_example(serialized_examples, feature_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Input Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardization(layers.Layer):\n",
    "    \n",
    "    def adapt(self, data_sample):\n",
    "        self.means_ = np.mean(data_sample, axis=0, keepdims=True)\n",
    "        self.stds_ = np.std(data_sample, axis=0, keepdims=True)\n",
    "        self.epsilon_ = 1e-7\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return (inputs - self.means_) / (self.stds_ + self.epsilon_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_layer = Standardization()\n",
    "std_layer.adapt(housing_train.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorical Features Using One-Hot Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ['<1H OCEAN', 'INLAND', 'NEAR OCEAN', 'NEAR BAY', 'ISLAND']\n",
    "indices = tf.range(len(vocab), dtype=tf.int64)\n",
    "table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)\n",
    "num_oov_buckets = 2\n",
    "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([3 5 1 1 0 5 5 1 5 2], shape=(10,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]], shape=(10, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "categories = tf.constant(['NEAR BAY', 'DESERT', 'INLAND', 'INLAND', '<1H OCEAN', 'POLAR', 'FOREST', 'INLAND', 'POLAR', 'NEAR OCEAN'])\n",
    "cat_indices = table.lookup(categories)\n",
    "print(cat_indices)\n",
    "\n",
    "cat_one_hot = tf.one_hot(cat_indices, depth=len(vocab) + num_oov_buckets)\n",
    "print(cat_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 1. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0. 0.]], shape=(10, 7), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 20:17:21.835093: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "text_vec_layer = layers.TextVectorization(output_mode='multi_hot')\n",
    "\n",
    "text_vec_layer.adapt(vocab)\n",
    "tensor = text_vec_layer(categories)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorical Features Using Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(7, 2) dtype=float32, numpy=\n",
      "array([[0.9632548 , 0.29368246],\n",
      "       [0.3882507 , 0.04600167],\n",
      "       [0.26282084, 0.794824  ],\n",
      "       [0.9262692 , 0.6907414 ],\n",
      "       [0.11559474, 0.3930539 ],\n",
      "       [0.70262873, 0.61465704],\n",
      "       [0.8635504 , 0.00665975]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# Manual encoding\n",
    "\n",
    "embedding_dim = 2\n",
    "embed_init = tf.random.uniform([len(vocab) + num_oov_buckets, embedding_dim])\n",
    "embedding_matrix = tf.Variable(embed_init)\n",
    "print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int64, numpy=array([3, 5, 1, 1, 0, 5, 5, 1, 5, 2])>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = tf.constant(['NEAR BAY', 'DESERT', 'INLAND', 'INLAND', '<1H OCEAN', 'POLAR', 'FOREST', 'INLAND', 'POLAR', 'NEAR OCEAN'])\n",
    "cat_indices = table.lookup(categories)\n",
    "cat_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 2), dtype=float32, numpy=\n",
       "array([[0.9262692 , 0.6907414 ],\n",
       "       [0.70262873, 0.61465704],\n",
       "       [0.3882507 , 0.04600167],\n",
       "       [0.3882507 , 0.04600167],\n",
       "       [0.9632548 , 0.29368246],\n",
       "       [0.70262873, 0.61465704],\n",
       "       [0.70262873, 0.61465704],\n",
       "       [0.3882507 , 0.04600167],\n",
       "       [0.70262873, 0.61465704],\n",
       "       [0.26282084, 0.794824  ]], dtype=float32)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.embedding_lookup(embedding_matrix, cat_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 2), dtype=float32, numpy=\n",
       "array([[ 0.04262692,  0.01907415],\n",
       "       [ 0.02026287,  0.01146571],\n",
       "       [-0.01117493, -0.04539983],\n",
       "       [-0.01117493, -0.04539983],\n",
       "       [ 0.04632548, -0.02063175],\n",
       "       [ 0.02026287,  0.01146571],\n",
       "       [ 0.02026287,  0.01146571],\n",
       "       [-0.01117493, -0.04539983],\n",
       "       [ 0.02026287,  0.01146571],\n",
       "       [-0.02371792,  0.0294824 ]], dtype=float32)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding with a layer\n",
    "\n",
    "embedding_layer = layers.Embedding(input_dim=len(vocab) + num_oov_buckets,\n",
    "                                   output_dim=embedding_dim)\n",
    "embedding_layer(cat_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_inputs = layers.Input(shape=[8])\n",
    "categories = layers.Input(shape=[], dtype=tf.string)\n",
    "cat_indices = layers.Lambda(lambda cats: table.lookup(cats))(categories)\n",
    "cat_embed = layers.Embedding(input_dim=6, output_dim=2)(cat_indices)\n",
    "encoded_inputs = layers.concatenate([regular_inputs, cat_embed])\n",
    "outputs = layers.Dense(1)(encoded_inputs)\n",
    "\n",
    "model = models.Model(inputs=[regular_inputs, categories],\n",
    "                     outputs=[outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TensorFlow Datasets (TFDS) Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 21:08:10.626171: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /Users/mmenendezg/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 100%|██████████| 4/4 [00:03<00:00,  1.33 file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to /Users/mmenendezg/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = tfds.load(name='mnist')\n",
    "mnist_train, mnist_test = dataset['train'], dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tfds.load(name='mnist', batch_size=32, as_supervised=True)\n",
    "mnist_train = dataset['train'].prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why would you want to use the Data API?\n",
    "\n",
    "> The Data API allows to load and preprocess large datasets in an efficient way such as loading, repeating, batching, shuffling and prefetching the data. Also enables the multithreading when preprocessing the data. This allows the GPU to be used more efficiently while the CPU is working on the next batch. Furthermore, the Data API allows to read data from different sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What are the benefits of splitting a large dataset into multiple files?\n",
    "\n",
    "> When splitting the dataset into multiple files, we can load from multiple files at the same time and interleave the records. This will shuffle more the data than being shuffled with the memory due to the limitations of the memory. This also breaks some patterns that are created by chance with consecutive instances. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. During training, how can you tell that your input pipeline is the bottleneck? What can you do to fix it?\n",
    "\n",
    "> You can use TensorBoard to visualize profiling data: if the GPU is not fully utilized then your input pipeline is likely to be the bottleneck. You can fix it by making sure it reads and preprocesses the data in multiple threads in parallel, and ensuring it prefetches a few batches. If this is insufficient to get your GPU to 100% usage during training, make sure your preprocessing code is optimized. You can also try saving the dataset into multiple TFRecord files, and if necessary perform some of the preprocessing ahead of time so that it does not need to be done on the fly during training (TF Transform can help with this). If necessary, use a machine with more CPU and RAM, and ensure that the GPU bandwidth is large enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Can you save any binary data to a TFRecord file, or only serialized protocol buffers?\n",
    "\n",
    "> Any binary format can be saved into a TFRecord file. The serialized protocol buffers (protobufs) are preferred due to its efficiency, portability and extensibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Why would you go through the hassle of converting all your data to the Example protobuf format? Why not use your own protobuf definition?\n",
    "\n",
    "> The Example protobuf format has the advantage that TensorFlow provides some operations to parse it (the tf.io.parse*example() functions) without you having to define your own format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. When using TFRecords, when would you want to activate compression? Why not do it systematically?\n",
    "\n",
    "> We would prefer to use compression when we will transfer the data through network or when the storage is very limited. Activating the compression sistematically when no needed may waste CPU processing to decompress the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Data can be preprocessed directly when writing the data files, or within the tf.data pipeline, or in preprocessing layers within your model, or using TF Transform. Can you list a few pros and cons of each option?\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Name a few common techniques you can use to encode categorical features. What about text?\n",
    "\n",
    "> For categorical features one-hot encoding would be one great option when the total categories are just a few. However, when the categories are a lot, embeddings may be a better option. \n",
    "> For text the previous two options are still valid, but there may be another such as TF-IDF (that is the counting of words divided by the total of words), bag of words that is just the counting of the words in a corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions 9 and 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Load the Fashion MNIST dataset (introduced in Chapter 10); split it into a train‐ ing set, a validation set, and a test set; shuffle the training set; and save each dataset to multiple TFRecord files. Each record should be a serialized Example protobuf with two features: the serialized image (use tf.io.serialize_tensor() to serialize each image), and the label.11 Then use tf.data to create an efficient dataset for each set. Finally, use a Keras model to train these datasets, including a preprocessing layer to standardize each input feature. Try to make the input pipeline as efficient as possible, using TensorBoard to visualize profiling data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. In this exercise you will download a dataset, split it, create a tf.data.Dataset to load it and preprocess it efficiently, then build and train a binary classification model containing an Embedding layer:\n",
    "- Download the Large Movie Review Dataset, which contains 50,000 movies reviews from the Internet Movie Database. The data is organized in two direc‐ tories, train and test, each containing a pos subdirectory with 12,500 positive reviews and a neg subdirectory with 12,500 negative reviews. Each review is stored in a separate text file. There are other files and folders (including pre‐ processed bag-of-words), but we will ignore them in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split the test set into a validation set (15,000) and a test set (10,000)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use tf.data to create an efficient dataset for each set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a binary classification model, using a TextVectorization layer to preprocess each review. If the TextVectorization layer is not yet available (or if you like a challenge), try to create your own custom preprocessing layer: you can use the functions in the tf.strings package, for example lower() to make everything lowercase, regex_replace() to replace punctuation with spaces, and split() to split words on spaces. You should use a lookup table to output word indices, which must be prepared in the adapt() method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add an Embedding layer and compute the mean embedding for each review, multiplied by the square root of the number of words (see Chapter 16). This rescaled mean embedding can then be passed to the rest of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train the model and see what accuracy you get. Try to optimize your pipelines to make training as fast as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use TFDS to load the same dataset more easily: tfds.load(\"imdb_reviews\")."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6b5f867329b9bf7e45db95e42066174580d403eff48fd9a79312950ee94a58cb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('Books-uCdQc7s-': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
