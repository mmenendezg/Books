{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation: Geometric Transformations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import (\n",
    "    Sequential,\n",
    "    layers,\n",
    "    losses,\n",
    "    optimizers,\n",
    "    regularizers,\n",
    "    callbacks,\n",
    ")\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "version_ = tf.version.VERSION\n",
    "print(f'Tensorflow version: {version_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU found -> /device:GPU:0\n",
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "if device_name == \"\":\n",
    "    BATCH_SIZE = 32\n",
    "    raise SystemError(\"No GPU found!\")\n",
    "else:\n",
    "    BATCH_SIZE = 512\n",
    "    print(f\"GPU found -> {device_name}\")\n",
    "\n",
    "os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"] = \"COMPRESSED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_path = os.path.join(\"..\", \"..\", \"..\", \"functions\")\n",
    "sys.path.append(functions_path)\n",
    "import learning_rate_functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 448\n",
    "IMG_WIDTH = 448\n",
    "IMG_CHANNELS = 3\n",
    "CLASS_NAMES = 'daisy dandelion rose sunflower tulip'.split()\n",
    "\n",
    "MODEL_URL = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
    "TRAIN_URL = \"gs://practical-ml-vision-book/flowers_tfr/train-*\"\n",
    "VALID_URL = \"gs://practical-ml-vision-book/flowers_tfr/valid-*\"\n",
    "IMAGES_LIST = [\n",
    "    \"gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/dandelion/9818247_e2eac18894.jpg\",\n",
    "    \"gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/dandelion/9853885425_4a82356f1d_m.jpg\",\n",
    "    \"gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/dandelion/98992760_53ed1d26a9.jpg\",\n",
    "    \"gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/dandelion/9939430464_5f5861ebab.jpg\",\n",
    "    \"gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/dandelion/9965757055_ff01b5ee6f_n.jpg\",\n",
    "]\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logdir():\n",
    "    run_id = time.strftime('run_%Y%m%d-%H%M%S')\n",
    "    log_dir = os.path.join('..', '..', 'reports', 'logs', run_id)\n",
    "    return log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Preprocessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def read_from_tfr(self, proto):\n",
    "        features_description = {\n",
    "            \"image\": tf.io.VarLenFeature(tf.float32),\n",
    "            \"shape\": tf.io.VarLenFeature(tf.int64),\n",
    "            \"label\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "            \"label_int\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "        }\n",
    "        record = tf.io.parse_single_example(proto, features_description)\n",
    "        shape = tf.sparse.to_dense(record[\"shape\"])\n",
    "        image = tf.reshape(tf.sparse.to_dense(record[\"image\"]), shape)\n",
    "        label_int = record[\"label_int\"]\n",
    "        return image, label_int\n",
    "\n",
    "    def read_from_jpeg(self, filename):\n",
    "        image = tf.io.read_file(filename)\n",
    "        image = tf.image.decode_jpeg(image, channels=IMG_CHANNELS)\n",
    "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "        return image\n",
    "\n",
    "    def preprocess(self, image):\n",
    "        image = tf.image.resize_with_pad(image, IMG_HEIGHT, IMG_WIDTH)\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessed_dataset(pattern):\n",
    "    preprocessor = _Preprocessor()\n",
    "    dataset = (\n",
    "        tf.data.TFRecordDataset(\n",
    "            [file for file in tf.io.gfile.glob(pattern)], compression_type=\"GZIP\"\n",
    "        )\n",
    "        .map(preprocessor.read_from_tfr, num_parallel_calls=AUTOTUNE)\n",
    "        .map(lambda image, label: (preprocessor.preprocess(image), label), num_parallel_calls=AUTOTUNE)\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def preprocessed_image(filename):\n",
    "    preprocessor = _Preprocessor()\n",
    "    image = preprocessor.read_from_jpeg(filename)\n",
    "    image = preprocessor.preprocess(image)\n",
    "    return image    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning with MobileNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With random cropping and left-right flipping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 150\n",
    "\n",
    "\n",
    "def train_and_evaluate(lrate=0.001, l1=0.0, l2=0.0, num_hidden=16):\n",
    "    with tf.device(\"/device:CPU:0\"):\n",
    "        data_augmentation = Sequential(\n",
    "            [\n",
    "                layers.RandomCrop(\n",
    "                    height=IMG_HEIGHT // 2,\n",
    "                    width=IMG_WIDTH // 2,\n",
    "                    input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),\n",
    "                ),\n",
    "                layers.RandomFlip(mode=\"horizontal\", name=\"random_lr_flip/none\"),\n",
    "                layers.RandomBrightness(factor=0.2, value_range=(0, 1)),\n",
    "                layers.RandomContrast(factor=0.2),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    train_dataset = (\n",
    "        preprocessed_dataset(TRAIN_URL)\n",
    "        .map(\n",
    "            lambda image, label: (data_augmentation(image), label),\n",
    "            num_parallel_calls=AUTOTUNE,\n",
    "        )\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(AUTOTUNE)\n",
    "    )\n",
    "    valid_dataset = (\n",
    "        preprocessed_dataset(VALID_URL)\n",
    "        .map(\n",
    "            lambda image, label: (data_augmentation(image), label),\n",
    "            num_parallel_calls=AUTOTUNE,\n",
    "        )\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    regularizer = regularizers.l1_l2(l1, l2)\n",
    "    layers_ = [\n",
    "        hub.KerasLayer(\n",
    "            MODEL_URL,\n",
    "            trainable=False,\n",
    "            input_shape=(IMG_HEIGHT // 2, IMG_WIDTH // 2, IMG_CHANNELS),\n",
    "            name=\"mobilenet_embedding\",\n",
    "        ),\n",
    "        layers.Dense(\n",
    "            num_hidden,\n",
    "            kernel_regularizer=regularizer,\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            activation=\"elu\",\n",
    "            name=\"dense_hidden\",\n",
    "        ),\n",
    "        layers.Dense(\n",
    "            len(CLASS_NAMES),\n",
    "            kernel_regularizer=regularizer,\n",
    "            activation=\"softmax\",\n",
    "            name=\"flower_probs\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    model = Sequential(layers_, name=\"flower_classification\")\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=lrate),\n",
    "        loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    # Callbacks\n",
    "    exponential_decay_fn = learning_rate_functions.exponential_decay_with_warmup(\n",
    "        lr_start=lrate / 2,\n",
    "        lr_max=lrate,\n",
    "        lr_min=lrate / 10,\n",
    "        lr_rampup_epochs=NUM_EPOCHS // 10,\n",
    "        lr_sustain_epochs=NUM_EPOCHS // 10,\n",
    "        lr_exp_decay=0.25,\n",
    "    )\n",
    "    lr_scheduler = callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "    logdir = get_logdir()\n",
    "    tensorboard_cb = callbacks.TensorBoard(log_dir=logdir)\n",
    "    early_stop_cb = callbacks.EarlyStopping(patience=5, restore_best_weights=False)\n",
    "    checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "        filepath=\"../../flowers_classifier/flower_classifier.h5\", save_best_only=True\n",
    "    )\n",
    "    callbacks_ = [lr_scheduler, tensorboard_cb, early_stop_cb, checkpoint_cb]\n",
    "\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=valid_dataset,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        callbacks=callbacks_,\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 448, 448, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 448, 448, 3), dtype=tf.float32, name='random_crop_input'), name='random_crop_input', description=\"created by layer 'random_crop_input'\"), but it was called on an input with incompatible shape (448, 448, None).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 448, 448, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 448, 448, 3), dtype=tf.float32, name='random_crop_input'), name='random_crop_input', description=\"created by layer 'random_crop_input'\"), but it was called on an input with incompatible shape (448, 448, None).\n",
      "Model: \"flower_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenet_embedding (KerasL  (None, 1280)             2257984   \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " dense_hidden (Dense)        (None, 16)                20496     \n",
      "                                                                 \n",
      " flower_probs (Dense)        (None, 5)                 85        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,278,565\n",
      "Trainable params: 20,581\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "6/6 [==============================] - 190s 31s/step - loss: 2.1564 - accuracy: 0.2089 - val_loss: 1.7724 - val_accuracy: 0.3368 - lr: 5.0000e-04\n",
      "Epoch 2/150\n",
      "6/6 [==============================] - 216s 36s/step - loss: 1.7081 - accuracy: 0.3855 - val_loss: 1.7198 - val_accuracy: 0.4378 - lr: 5.3333e-04\n",
      "Epoch 3/150\n",
      "6/6 [==============================] - 194s 32s/step - loss: 1.3992 - accuracy: 0.5228 - val_loss: 1.2668 - val_accuracy: 0.5078 - lr: 5.6667e-04\n",
      "Epoch 4/150\n",
      "6/6 [==============================] - 179s 29s/step - loss: 1.0996 - accuracy: 0.6070 - val_loss: 1.0621 - val_accuracy: 0.6321 - lr: 6.0000e-04\n",
      "Epoch 5/150\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.9473 - accuracy: 0.6721 - val_loss: 0.8709 - val_accuracy: 0.7124 - lr: 6.3333e-04\n",
      "Epoch 6/150\n",
      "6/6 [==============================] - 182s 30s/step - loss: 0.7940 - accuracy: 0.7389 - val_loss: 0.7871 - val_accuracy: 0.7176 - lr: 6.6667e-04\n",
      "Epoch 7/150\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.6959 - accuracy: 0.7658 - val_loss: 0.7412 - val_accuracy: 0.7306 - lr: 7.0000e-04\n",
      "Epoch 8/150\n",
      "6/6 [==============================] - 191s 31s/step - loss: 0.6358 - accuracy: 0.7747 - val_loss: 0.6941 - val_accuracy: 0.7228 - lr: 7.3333e-04\n",
      "Epoch 9/150\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.5643 - accuracy: 0.8091 - val_loss: 0.6179 - val_accuracy: 0.7642 - lr: 7.6667e-04\n",
      "Epoch 10/150\n",
      "6/6 [==============================] - 179s 30s/step - loss: 0.5395 - accuracy: 0.8115 - val_loss: 0.6001 - val_accuracy: 0.7772 - lr: 8.0000e-04\n",
      "Epoch 11/150\n",
      "6/6 [==============================] - 179s 30s/step - loss: 0.5028 - accuracy: 0.8228 - val_loss: 0.5514 - val_accuracy: 0.7953 - lr: 8.3333e-04\n",
      "Epoch 12/150\n",
      "6/6 [==============================] - 179s 30s/step - loss: 0.4739 - accuracy: 0.8381 - val_loss: 0.5607 - val_accuracy: 0.7876 - lr: 8.6667e-04\n",
      "Epoch 13/150\n",
      "6/6 [==============================] - 178s 30s/step - loss: 0.4520 - accuracy: 0.8446 - val_loss: 0.5289 - val_accuracy: 0.8135 - lr: 9.0000e-04\n",
      "Epoch 14/150\n",
      "6/6 [==============================] - 165s 27s/step - loss: 0.4225 - accuracy: 0.8575 - val_loss: 0.5047 - val_accuracy: 0.8109 - lr: 9.3333e-04\n",
      "Epoch 15/150\n",
      "6/6 [==============================] - 180s 30s/step - loss: 0.4157 - accuracy: 0.8613 - val_loss: 0.4759 - val_accuracy: 0.8135 - lr: 9.6667e-04\n",
      "Epoch 16/150\n",
      "6/6 [==============================] - 179s 29s/step - loss: 0.3977 - accuracy: 0.8592 - val_loss: 0.4958 - val_accuracy: 0.8161 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "6/6 [==============================] - 178s 30s/step - loss: 0.4014 - accuracy: 0.8664 - val_loss: 0.4653 - val_accuracy: 0.8187 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "6/6 [==============================] - 190s 31s/step - loss: 0.3676 - accuracy: 0.8657 - val_loss: 0.4268 - val_accuracy: 0.8394 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "6/6 [==============================] - 176s 29s/step - loss: 0.3921 - accuracy: 0.8643 - val_loss: 0.4486 - val_accuracy: 0.8316 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "6/6 [==============================] - 182s 31s/step - loss: 0.3720 - accuracy: 0.8688 - val_loss: 0.4629 - val_accuracy: 0.8238 - lr: 0.0010\n",
      "Epoch 21/150\n",
      "6/6 [==============================] - 176s 29s/step - loss: 0.3627 - accuracy: 0.8739 - val_loss: 0.4327 - val_accuracy: 0.8368 - lr: 0.0010\n",
      "Epoch 22/150\n",
      "6/6 [==============================] - 182s 31s/step - loss: 0.3433 - accuracy: 0.8776 - val_loss: 0.4535 - val_accuracy: 0.8420 - lr: 0.0010\n",
      "Epoch 23/150\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.3538 - accuracy: 0.8773 - val_loss: 0.4287 - val_accuracy: 0.8472 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "model = train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 51664), started 7:31:42 ago. (Use '!kill 51664' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4236b8e8ec60a169\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4236b8e8ec60a169\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=../../reports/logs\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b20fb8559386147dc77cba9f7518a8c175fc982b8c10bebe842520e16b3ac5d9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
